\documentclass[a4paper, 10pt]{article}

\input{../Subfiles/Packages.tex}
\input{../Subfiles/Operators.tex}
\input{../Subfiles/Languages.tex}

\title{Horn Minimization}
\author{Obiedkov Sergei, Vilmin Simon}

\begin{document}
	
\maketitle
	
\section{Abstract}

The topic is implication theories, or Horn, minimization, used in database applications for instance. More precisely the aim is to provide a review of existing algorithms for performing minimization and implement them to see how do
they behave under practical test cases.

We study several algorithms and review them within the context of closure
systems. On top of providing explanations on their operation and complexity analysis, we implemented those algorithms using C++. With randomly generated systems and real data, we provide ideas on which closure operator matches the best each algorithm and which algorithm or steps are likely to be used in practice. Those results being valid within the context of our tests, suggest further research and experiments to lead in future work.
	
\section{Introduction}

The question of minimization has been discussed and developed through various 
frameworks, and several computer scientists communities. Notice that in order 
not to make this synthesis too long, we will stay within the context of 
minimization and will not trace the field of implication theories in general. 
For a survey of this domain anyway, the reader should refer to 
\cite{wild_joy_2017}. Also, note that minimality in general terms is not 
unique. Indeed, one can define several types of minimality among implication 
systems. For instance, not only we can define minimality with respect to the 
number of implications within a system (which is our interest) but also with 
respect to the number of attributes in each implications. The former one is 
called canonical in relational database field, and hyperarc 
minimum within the graph context. Especially in the graph-theoretic and 
boolean logic settings, one can derive more types of minimality. For general 
introduction to boolean logic notations, we invite the reader to see 
\cite{cori_mathematical_2000}. In terms of propositional logic, implications 
are represented through Horn formulae. Interestingly, the minimization problem 
we are going to consider is the only one being polynomial time solvable. Other 
problems are proved to be NP-Complete or NP-Hard. For more discussion on other 
minimality definitions and their computational complexity, the reader should 
refer to \cite{boros_strong_2017, ausiello_directed_2017, 
	ganter_conceptual_2016, ausiello_minimal_1986, wild_joy_2017, 
	boros_horn_1998, hammer_optimal_1993}. In subsequent explanations, we will refer to minimization with respect to the number of implications.

To the best of our knowledge, the two first fields in which algorithms and 
properties of minimality arose are Formal Concept Analysis (FCA) (see 
\cite{ganter_formal_1999, 
	ganter_two_2010} for an introduction) and Database Theory (DB) (see 
\cite{maier_theory_1983}). Both sides were developed independently in the early 
80's. For the first domain, characterization of minimality goes to Duquenne and 
Guigues \cite{guigues_familles_1986}, in which they describe the so-called 
canonical basis (also called Duquenne-Guigues basis after its 
authors, abbreviated DG all along this report) relying on the notion of pseudo-closed sets. For the database part, study of implications is made by Maier through functional dependencies (\cite{maier_theory_1983, maier_minimum_1980}). The polynomial time algorithm he gives for minimization heavily relies on a fast subroutine discovered by Beeri and Bernstein in \cite{beeri_computational_1979}, 1979.

From then on, knowledge increased over years and spread out over domains. 
Another algorithm based on a minimality theorem is given by Shock in 1986 
(\cite{shock_computing_1986}). Unfortunately, as we shall see and as already 
discussed by Wild in \cite{wild_computations_1995} the algorithm may not be
correct in general, even though the underlying theorem is. During the same 
period, Ausiello et al. brought the problem to graph-theoretic ground, and 
provided new structure known as FD-Graph and algorithm to represent 
and work on implication systems in \cite{ausiello_directed_2017, 
	ausiello_graph_1983, ausiello_minimal_1986}. This approach has been seen in 
graph theory as an extension of the transitive closure in graphs 
(\cite{aho_transitive_2006}), but no consideration equivalent to minimization 
task seems to have been taken beforehand, as far as we know. Still in the 1980 
decade, Ganter expressed the canonical basis formalized by Duquenne and Guigues 
in his paper related to algorithms in FCA, \cite{ganter_two_2010} through 
closure systems, pseudo-closed and quasi-closed sets. Also, the works of Maier and Duquenne-Guigues have been used in the lattice-theoretic context by Day in 
\cite{day_lattice_1992} to derive an algorithm based on congruence relations. 
For in-depth knowledge of implication system within lattice terminology, we can 
see \cite{davey_introduction_2002} as an introduction and 
\cite{bertet_lattices_2016} for a survey. Next, Wild  (\cite{wild_implicational_1989, wild_theory_1994, wild_computations_1995}) 
linked within this set-theoretic framework both the relational databases, 
formal concept analysis and lattice-theoretic approach. In relating those 
fields, he describes an algorithm for minimizing a basis, similar to algorithms 
of Day and, somehow, Shock (resp. \cite{day_lattice_1992},  
\cite{shock_computing_1986}). This framework is the one we will use for our 
study, and can be found in more recent work by Ganter \& Obiedkov in 
\cite{ganter_conceptual_2016}. Later, Duquenne proposed some 
variations of Day's work with another algorithm in 
\cite{duquenne_variations_2007}. More recently, Bor\`os et al. by 
working in a boolean logic framework, exhibited a theorem on the size of
canonical basis \cite{boros_exclusive_2010, boros_strong_2017}. They also gave
a general theoretic approach that algorithm should do one way or another on
reduction purpose. Out of these papers, Berczi \& al. derived a new 
minimization procedure based on hypergraphs in \cite{berczi_directed_2017}. 
Furthermore, an algorithm for computing the canonical basis starting from any 
system is given in \cite{ganter_conceptual_2016}. This last algorithm was our starting point for this study.

Even though the work we are going to cite is not designed to answer this 
question of minimization, it must also be exposed as the algorithm is 
intimately related to DG basis and can be used for base reduction. The paper
of Angluin et al. in query learning, see \cite{angluin_learning_1992}, provides
an algorithm for learning a Horn representation of an unknown initial formula. 
It has been shown later by Ari√†s and Alcazar (\cite{arias_canonical_2009}) that
the output of Angluin algorithm was always the Duquennes-Guigues basis.

In this paper our aim is to review various algorithms dedicated to the minimization task. After description of our notations and recall of the main definitions, we present different algorithms. Eventually, we exhibit some experiments on those algorithms.

\section{Notations}

For our study let us define notions. $\Sg$ will be an attribute set. An implication is an ordered pair $(A, B), A, B \subseteq \Sg$ denoted $A \imp B$.
$A$ is the premise, $B$ the conclusion of the implication. A set of implications
$\I$ over $\Sg$ is an implication system/theory. A subset $M$ of $\Sg$ is a model of $A \imp B$, denoted $M \models A \imp B$ if $B \subseteq M \lor A \nsubseteq M$. $M$ is a model of an implication system $\I$ if it is a model for
all implications of $\I$. In fact, models of $\I$ define a closure system $\Sg^{\I}$. 

\begin{definition}[Closure operator] Let $\Sg$ be a set and $\phi : 
	2^{\Sg} \imp 2^{\Sg}$ an application on the power set of $\Sg$. $\phi$ is
	a closure operator if $\forall X, Y \subseteq \Sg$:
	\begin{itemize}
		\item[(i)] $X \subseteq \phi(X)$ (extensive),
		\item[(ii)] $X \subseteq Y \imp \phi(X) \subseteq \phi(Y)$
		(monotone),
		\item[(iii)] $\phi(X) = \phi(\phi(X))$ (idempotent).
	\end{itemize}
	$X \subseteq \Sg$ is called \belemp{closed} if $X = \phi(X)$.
\end{definition}

\begin{definition}[Closure system] Let $\Sg$ be a set, and $\Sg^{\phi}
	\subseteq 2^{\Sg}$. $\Sg^{\phi}$ is called a closure system if:
	\begin{itemize}
		\item[(i)] $\Sg \in \Sg^{\phi}$,
		\item[(ii)] if $\cal{S} \subseteq \Sg^{\phi}$, then $\bigcap \cal{S} 
		\in \Sg^{\phi}$ \quad (closed under intersection).
	\end{itemize}
	
\end{definition}

With this closure system comes a closure operator $\I(\cdot)$ associating to any $X \subseteq \Sg$, the smallest model of $\Sg^{\I}$ containing $X$:

 \[ \I(X) = \bigcap \{A \in \Sg^{\I},\  X \subseteq A \} \]
 
$A \imp B$ semantically follows from $\I$, $\I \models A \imp B$ if all models of $\I$ are models of $A \imp B$. More generally, if we have two systems $\I_1, \I_2$, $\I_1 \models \I_2$ if all implications of $\I_2$ follow from $\I_1$. Checking entailment may seem to be expensive in terms of set operations. Fortunately, a proposition based on closure operators ease the way.

\begin{proposition} $\I \models A \imp B$ if and only if $B \subseteq \I(A)$.
	\end{proposition}
	
Two systems $\I_1$, $\I_2$ are equivalent if they define the same closure system. More precisely, they are equivalent if $\I_1 \models \I_2$ and $\I_2 \models \I_1$. Given some $\I$ over $\Sg$ one can find \textit{"useless"} implications, or redundant implications. An implication $A \imp B \in \I$ is redundant if $\I - \{ A \imp B \} \models A \imp B$. For the remainder of the paper, we may denote by $\I^{-}$ the system $\I - \{ A \imp B \}$. It should be clear when used. 

\begin{definition}[Minimum base] $\I$ is a minimum base if there is no equivalent system $\I'$ with fewer implications than $\I$.
	
\end{definition}

A well known minimum base is the canonical or Duquennes-Guigues (\cite{guigues_familles_1986}). It relies on pseudo-closed sets.

\begin{definition}[Pseudo-closed set] Given $\I$ over $\Sg$, we say
	that $P \subseteq \Sg$ is \belemp{pseudo-closed} if:
	\begin{itemize}
		\item[(i)] $P \neq \I(P)$,
		\item[(ii)] $Q \subset P$ and $Q$ pseudo-closed implies $\I(Q) 
		\subseteq P$.
	\end{itemize}
\end{definition}

\begin{definition}[Duquenne-Guigues basis] The base $\I$ defined by
	
	\[ \I = \{ P \imp \I(P) \; | \; P \text{ is pseudo-closed in $\I$ } \} \]
	
\noindent is called the Duquenne-Guigues or canonical basis. It is minimum.
	
\end{definition}

So far we discussed several notions: implications, pseudo-closed set, 
quasi-closed set, canonical basis and so forth. Most of them rely heavily on
computing the closure of sets with respect to $\I$. Hence, to have practical 
efficiency, we must be able to compute closures as fast as possible. 
Fortunately, several algorithms can be found. Among them, there is a na√Øve
procedure based on the operation $\circ$:

\[ X^{\circ} = 
X \cup \bigcup \{ B \; | \; A \imp B \in \I, \; A \subseteq X \} \]

Repetition of this operation up to saturation leads to \textsc{Closure}. Furthermore the algorithm by Beeri and Bernstein in \cite{beeri_computational_1979} called \textsc{LinClosure} addresses this question. \textsc{LinClosure} as previously mentioned has been widely used, notably in \cite{maier_theory_1983, maier_minimum_1980, ganter_conceptual_2016, shock_computing_1986, day_lattice_1992}. Before describing those procedures, let us introduce our complexity notations:
\begin{itemize}
	\item[-] $|\Sg|$ will denote the size of the attribute set $\Sg$,
	\item[-] $|\B|$ will be the number of implications in $\I$ ($\B$ stands
	for body),
	\item[-] $|\I|$ is the number of symbols used to represent $\I$.
\end{itemize}
Recall that $O$ is the asymptotically worst case complexity (in time or space). For instance, in the worst case, $|\I| = |\B|\times|\Sg|$, thus $|\I| = O(|\B|\times|\Sg|)$. \textsc{Closure} and \textsc{LinClosure} are algorithms \ref{alg:closure}, \ref{alg:linclosure} (resp.). 

\vspace{1.2em}

\begin{algorithm}[ht]
	\KwIn{A base $\I$, $X \subseteq \Sg$}
	\KwOut{The closure $\I(X)$ of $X$ under $\I$}
	
	\BlankLine
	\BlankLine
	
	$closed := \bot$ \;
	$\I(X) := X$ \;
	\While{$\lnot closed$}{
		$closed := \top$ \;
		\ForEach{$A \imp B \in \I$}{
			\If{$A \subseteq \I(X)$}{
				$\I(X) := \I(X) \cup B$ \;
				$\I := \I - \{A \imp B \}$ \;
				$closed := \bot$ \;
			}
			
		}
	}
	
	\BlankLine
	
	return $\I(X)$\;
	
	\caption{\textsc{Closure}}
	\label{alg:closure}
\end{algorithm}


As we already mentioned, the algorithm \textsc{Closure} relies on the $\circ$ 
operation. The principle is to re-roll over the set of implications $\I$ to see 
whether there exists an implication $A \imp B$ in $\I$ such that $\I(X) 
\not\models A \imp B$ up to stability. Asymptotically, we will need $O(|\B|^2 
\times |\Sg|)$ if we remove only one implication per loop. the $|\Sg|$ cost
comes from the set union. $\bot$ stands for \textit{false} and $\top$ for \textit{true}.

\begin{algorithm}
	\KwIn{A base $\I$, $X \subseteq \Sg$}
	\KwOut{The closure $\I(X)$ of $X$ under $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$count[A \imp B] := |A|$ \;
		
		\If{$|A| = 0$}{
			$X := X \cup B$ \;
		}
		
		\ForEach{$a \in A$}{
			$list[a] = list[a] \cup  \{ A \imp B \}$ \;
		}
	}
	
	\BlankLine
	
	$update := X$ \;
	
	\BlankLine
	
	\While{$update \neq \emptyset$}{
		choose $m \in update$ \;
		$update := update - \{m\}$ \;
		
		\ForEach{$A \imp B \in list[m]$}{
			$count[A \imp B] := count[A \imp B] - 1$ \;
			\If{$count[A \imp B] = 0$}{
				$add := B - X$ \;
				$X := X \cup add$ \;
				$update := update \cup add$ \;
			}
			
		}
	}
	
	return $X$ \;
	
	\caption{\textsc{LinClosure}}
	\label{alg:linclosure}
\end{algorithm}

\textsc{LinClosure} has $O(|\I|)$ time complexity. The main idea is to use
counters. Starting from $X$, if we reach for a given $A \imp B$ as many elements
as $|A|$, then $A \subseteq \I(X)$ and we must also add $B$. Because the closure
in itself is not the main point of our topic, we will not study 
\textsc{LinClosure} in depth. Furthermore, there exists other
algorithm for computing closure given by Wild in \cite{wild_computations_1995}. It is derived from \textsc{LinClosure}, but we did not consider it because it brings no improvement in complexity. For more complete theoretical and practical comparisons of closure algorithms, we redirect the reader to 
\cite{bazhanov_optimizations_2014}. In this paper, \textsc{LinClosure} is 
shown maybe not to be the most efficient algorithm in practice when used in 
other algorithms, especially when compared with \textsc{Closure}. Anyway, 
because of its theoretical complexity and use in all algorithms we will review, 
we will still consider \textsc{LinClosure}.


	
\section{Algorithms for minimization}

We are going to focus on 5 algorithms. To begin with, let us talk about an algorithm issued by A. Day in \cite{day_lattice_1992} and by Wild somehow in
\cite{wild_implicational_1989}. It can be found as we will write it in \cite{ganter_conceptual_2016}. The principle is to perform right-saturation
first so has to have right-closed implications. Next, it computes left-saturation to find pseudo-closed sets and remove redundant implications.
See algorithm \ref{alg:MinCover}.

\begin{algorithm}
	\KwIn{$\I$: an implication system}
	\KwOut{the canonical base of $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I := \I - \{ A \imp B \}$ \;
		$B  := \I(A \cup B)$ \;
		$\I := \I \cup \{ A \imp B \}$ \;
	}
	
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I := \I - \{ A \imp B \}$ \;
		$A  := \I(A)$ \;
		\If{$A \neq B$}{
			$\I := \I \cup \{ A \imp B \}$ \;	
		}
		
	}
	
	\caption{\textsc{MinCover}}
	\label{alg:MinCover}	
\end{algorithm}

Given the linear complexity $O(|\I|)$ of \textsc{LinClosure} being the theoretically fastest closure algorithm, both loops of \textsc{MinCover}
require at most $O(|\B||\I|)$ operations. Hence $O(|\B||\I|)$ is the complexity of the all algorithm. In the paper of Day, this algorithm is discussed in terms
of congruences within complete join-semilattices, being much more algebraical.

The second algorithm we have has been discussed by Duquenne in \cite{duquenne_variations_2007} as variations of Day algorithms. In particular, one can find in \textsc{DuquenneMinimization} (see algorithm \ref{alg:Duquenne-min}) an alternative to \textsc{MinCover}.

\begin{algorithm}
	\KwIn{$\I$ a theory to minimize}
	\KwOut{$\I_c$ the DQ-basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I = \I - \{ A \imp B \}$ \;
		$A := \I(A)$ \;
		
		\If{$B \not\subseteq A$}{
			$B = B \cup A$ \;
			$\I := \I \cup \{ A \imp B \}$ \;	
		}
		
	}
	
	\BlankLine
	
	\textsc{LecticOrder}($\I$) \;
	$\I_c := \emptyset$ \;
	
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		\ForEach{$\alpha \imp \beta \in \I_c$}{
			\If{$\alpha \subset A \land \beta \nsubseteq A$}{
				$\I = \I - \{ A \imp B \}$ \;
				\textbf{goto next $A \imp B \in \I$} \;
				
			}
			
		}
		
		$B = \I(B)$ \;
		$\I_c := \I_c \cup \{ A \imp B \}$ \;
	}
	
	\BlankLine
	
	return $\I_c$ \;
	
	\caption{\textsc{DuquenneMinimization}}
	\label{alg:Duquenne-min}
\end{algorithm}

In this algorithm, we first perform left-saturation as much as redundancy elimination. Doing those steps not only allows for having quasi-closed, and in particular all possible pseudo-closed sets, as premises of implications but also
to get rid of several useless ones. Next, ordering of implications in a $\subseteq$-compatible way on premises ease the last step. It consists in iteratively build the canonical base $\I_c$ as an output. Because we have quasi-closed premises under lectic order, checking for pseudo-closedness for any premise of the input system only requires a run over the base we are building. If a premise is to be considered pseudo-closed, its corresponding implication is right-closed and added to the resulting system ($\I_c$).
Regarding the complexity, still thinking of \textsc{LinClosure}, the first loop,
quite similar to the second one of \textsc{MinClosure} requires $O(|\B||\I|)$ operations. Because lectic order is total, one can use fast sorting procedure to
perform the second step: $O(|\I|\log(|\B|))$. Finally, the building loop may
require no more than $O(|\B||\I|)$ operations since closure computations occur out of the nested loop. Consequently, the whole algorithm runs in $O(|\B||\I|)$ as \textsc{MinCover} and ends up on the Duquenne-Guigues basis too.

Next, we are interested in an algorithm quite different since coming out of Database theory. It has been proposed by Maier in \cite{maier_minimum_1980, maier_theory_1983}. Wild discussed it and its connection with closure spaces framework in \cite{wild_implicational_1989}. In our case, we keep on focusing on implications notations, see \textsc{MaierMinimization}. Here, apart from prior redundancy elimination as previously studied, we are to split implications into equivalence classes according to the closure of their premises. Then, using \textit{direct determination}, equivalence classes will be
reduced. In details, an implication $A \imp B$ will be removed when one can find
an implication $C \imp D$ such that $A \imp B, \ C \imp D \in E_{\I}(A)$ and $\I - E_{\I}[A] \models A \imp C$. In this case, $A$ directly determines $C$ and
$A \imp B$ can be removed if we replace $C \imp D$ by $C \imp B \cup D$ to preserve the closure system defined by $\I$. The algorithm ends up on a minimum
cover different from the canonical basis. Indeed, observe that we do not alter 
premises of implications, hence they may not be pseudo-closed.

\begin{algorithm}
	\KwIn{$\I$ : a theory to minimize}
	\KwOut{$\I$ minimized}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		\If{$\I - \{ A \imp B \} \models A \imp B$}{
			remove $A \imp B$ from $\I$ \;
		}
		
	}
	
	\BlankLine
	
	$E_{\I} := $ \textsc{EquivClasses}($\I$) \;
	
	\BlankLine
	
	\ForEach{$E_{\I}(X) \in E_{\I}$}{
		\ForEach{$A \imp B \in E_{\I}(X)$}{
			\If{$\exists C \imp D \in E_{\I}(X)$ s.t $A \ddv C$}{
				remove $A \imp B$ from $\I$ \;
				replace $C \imp D$ by $C \imp D \cup B$ \;	
			}
		}
		
	}
	
	\caption{\textsc{MaierMinimization}}
	\label{alg:Maier-Min}
\end{algorithm}

Let us focus on the complexity of this algorithm. Redundancy elimination can be done in $O(|\B||\I|)$ operations. To determine equivalence classes, the idea provided by Maier is to alter \textsc{LinClosure}. For each implication $A \imp B$, the closure operator permits to get all other premises reachable from $A$. When this $O(|\B||\I|)$ operation is done, we have a $|\B| \times |\B|$ matrix. With this structure, determining equivalence classes requires no more than a run
over it hence $O(|\B|^2)$ operations. Finally, finding direct determination is again an alteration of \textsc{LinClosure}. For each implication $A \imp B$, it is sufficient to stop closure computation when we reach a premise of $E_{\I}(A)$. Not omitting subsequent set union, the whole loop needs $O(|\B||\I|)$ operations to terminate. The whole complexity of the algorithm is then $O(|\B||\I|)$.

More recently, an algorithm has been issued by Berczi et al. in \cite{berczi_directed_2017}.Contrary to the previous algorithms, in this case 
we are likely to build the canonical base instead of minimizing the input system. We Refer to algorithm \ref{alg:Berczi-min}. Originally it is logic/hypergraph based, but we give it in terms of implications and closures. 
Starting from an empty system $\I_c$, and up to equivalence with the input $\I$, we iteratively take the next minimal pseudo-closed set $P$ of $\I$ (inclusion-wise) and add the implication $P \imp \I(P)$ to $\I_c$. This way,
the algorithm terminates on the Duquenne-Guigues base.



\begin{algorithm}
	\KwIn{$\I$: an implication theory}
	\KwOut{$\I_c$: the DG basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	
	\While{$\exists B \in \B(\I)$ s.t $\I_c(B) \neq \I(B)$}{
		$P := \min\{\I_c(B), \ B \in \B(\I) \text{ and } \I_c(B) \neq \I(B)\}$ 
		\;
		$\I_c := \I_c \cup \{P \imp \I(P) \}$ \;
		
	}
	
	return $\I_c$ \;
	
	\caption{\textsc{BercziMinimization}}
	\label{alg:Berczi-min}
\end{algorithm}

Here however, we must face a higher complexity than previously exposed algorithm. The outter loop runs up to equivalence, but since we add an implication to $\I_c$ per iteration, we must end after at most $|\B|$ steps. Finding the next minimum pseudo-closed sets require closure computations under 
both $\I$ and $\I_c$ for all premises of $\I$, hence $O(|\B||\I|)$ operations. Therefore, the algorithm has $O(|\B|^2|\I)$ time complexity. 

Finally, let us focus on an algorithm derived from Angluin query-learning based approach (see \cite{angluin_learning_1992, arias_canonical_2009}). For a quick recap,  the idea is we formulate queries to an oracle knowing the basis we are trying to learn. The oracle is assumed to provide an answer to our query in constant time. Depending on the query, it might also provide information on the object we are looking for. For Angluin algorithm, we need 2 types of queries. Say we want to learn a basis $\I$ over $\Sg$:
\begin{enumerate}
	\item membership query: is $M \subseteq \Sg$ a model of $\I$? The
	oracle may answer \textit{"yes"}, or \textit{"no"}.
	\item equivalence query: is a basis $\I'$ equivalent to $\I$? Again
	the answers are \textit{"yes"}, or \textit{"no"}. In the second case, the oracle provides a counterexample either positive or negative:
	\begin{itemize}
		\item[(i)] positive: a model $M$ of $\I$ which is not a
		model of $\I'$,
		\item[(ii)] negative: a non-model $M$ of $\I$ being a model
		of $\I'$. 
	\end{itemize}
\end{enumerate}
\noindent To clarify, the terms negative/positive are related to the base $\I$
we want to learn. \textsc{AngluinAlgorithm} (\ref{alg:Angluin}) is the algorithm presented by Angluin, Frazer and Pitts in \cite{angluin_learning_1992} as \textsc{Horn1}. 
Initially, it is based on learning logical representation of implication theories: Horn clauses. This learning algorithm has been shown first to terminate on a minimum representation of the basis we want to learn (\cite{angluin_learning_1992}) and more than that, to end up on the canonical
one by Arias et al. \cite{arias_canonical_2009}. It uses two operations allowing to reduce implications:
\begin{itemize}
	\item[-] $\textit{refine}(A \imp B, M)$: produces $M \imp \Sg$ if $B = \Sg$, $M \imp B \cup A - M$ otherwise,
	\item[-] $\textit{reduce}(A \imp B, M)$: produces $A \imp M - A$ if $B = \Sg$, $A \imp B \cap M$ otherwise.
\end{itemize}
\noindent The main idea is to ask the oracle whether the theory we are building ($\I_c$) is equivalent to $\I$ until it answers \textit{"yes"}. If it says \textit{"no"} then it provides an example on which $\I_c$ and $\I$ differ. If
the example is a model of $\I$, then we track implications in $\I_c$ falsified
by this example and correct them. If the example however is not a model of $\I$
we look for the first possible smaller example out of the one we got. The main
idea is to say that if we correct a smaller example, we are likely to correct
a larger one too. If we do not find any smaller example to correct, we add 
an implication in $\I_c$ addressing the problem. In practice, we may not be given the power of an oracle. Therefore, one can derive from \textsc{AngluinAlgorithm} the algorithm \textsc{AFPMinimization} (\ref{alg:AFP}).

\begin{algorithm}[H]
	\KwIn{$\I$ a theory to learn and an \textit{Oracle} with \textit{membership, equivalence} queries}
	\KwOut{$\I_c$ the canonical representation of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c = \emptyset$ \;
	\While{not equivalence($\I_c$)}{
		$M$ is the counterexample \;
		\If{$M$ is positive}{
			\ForEach{$A \imp B \in \I_c$ such that $M \not\models A \imp B$}{
				replace $A \imp B$ by $reduce(A \imp B, M)$ \;	
			}
			
		} \Else {
			\ForEach{$A \imp B \in \I_c$ such that $A \cap M \subset A$}{
				membership($M \cap A$) \;	
			}
			
			\If{Oracle replied "no" for at least one $A \imp B$}{
				Take the first such $A \imp B$ in $\I_c$ \;	
				replace $A \imp B$ by $refine(A \imp B, A \cap M)$ \;
				
			} \Else {
				add $M \imp \Sg$ to $\I_c$ \;
				
			}
			
		}
		
	}
	return $\I_c$ \;
	
	\caption{\textsc{AngluinAlgorithm}}
	\label{alg:Angluin}
\end{algorithm}



\vspace{1.2em}


\begin{algorithm}
	\KwIn{some theory $\I$ over $\Sg$}
	\KwOut{$\I_c$ the Duquenne-Guigues basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	Stack $\mathcal{S}$ \;
	
	\BlankLine
	
	\ForAll{$A \imp B \in \I$}{
		$\mathcal{S} := \left[ A \right]$ \;
		\Repeat{$\mathcal{S} = \emptyset$}{
			$X := \I_c(\text{pop}(\mathcal{S}))$ \;
			
			
			\If{$X \neq \I(X)$}{
				$found := \bot$ \;
				
				\ForAll{$\alpha \imp \beta \in \I_c$}{
					$C := \alpha \cap X$ \;
					\If{$C \neq \alpha$}{
						$D := \I(C)$ \;
						
						\If{$C \neq D$}{
							$found := \top$ \;
							change $\alpha \imp \beta$ by $C \imp D$ in $\I_c$\;
							push($X \cup D$, $\mathcal{S}$) \;
							
							\If{$\beta \neq D$}{
								push($\alpha$, $\mathcal{S}$)\;
							}
							\textbf{exit for}
						}
					}
				}
				
				\BlankLine
				
				\If{$found = \bot$}{
					$\I_c := \I_c \cup \{X \imp \I(X)\}$ \;	
				}
			}
		}
		
	}
	
	return $\I_c$ \;
	
	
	\caption{\textsc{AFPMinimization}}
	\label{alg:AFP}
\end{algorithm}

In this version, we use a stack to keep track of possible generators of negative counter-example. Whenever we find one, we apply the same operations as in \textsc{AFP}. Note that we do not use positive counter-example since we only
consider right-closed implications. Unfortunately we are not yet able to prove the algorithm nor give a precise complexity. Assuming the stack does not store more than $|\B|$ examples, one may find at most $O(|\B|^3|\I|)$ time complexity.

Note that \textsc{BercziMinimization} can be interpreted in terms of query learning too. In fact, the algorithm is ruled by an equivalence query. At each step we consider the next minimal negative counter-example being pseudo-closed. Because we base the algorithm on premises of $\I$, we conclude that premises of
the input system $\I$ can generate a sufficient set of negative counter examples when taken as in \textsc{BercziMinimization}. From this point of view, it gets closer to Angluin algorithm.

\section{Experiments}

\bibliographystyle{acm}
\bibliography{Biblio.bib}
\end{document}