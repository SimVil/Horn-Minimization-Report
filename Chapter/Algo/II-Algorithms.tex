\chapter{Minimization algorithms for implication theories}

In the first chapter, we settled the context of our study. We introduced our 
subject of interest and defined the theoretical ground it is built on. In this
chapter, we discuss several algorithms and their complexity. Our aim with this
review is to provide understanding of the algorithms, study their complexity.
We also describe them to prepare their implementation in the next chapter.  Our first section within this part will be dedicated to give a general outline of our review. Next, we will effectively dive into in-depth studies of minimization procedure. 


\section{Overview of the study}

Here, we draw the main lines of our subsequent explanations. First and foremost, we shall try as much as possible to express all algorithms in our framework of closure systems and orders. Nevertheless, to draw parallels
and understand the translation from one framework to another we may proceed to 
some travels in other terminologies such as query learning or directed graphs.

\vspace{1.2em}

The study will be divided into three parts. First, we will study algorithms 
coming from the FCA (Formal Concept Analysis) community. This includes 
algorithms \textsc{MinCover} from \cite{bazhanov_optimizations_2014, day_lattice_1992, wild_implicational_1989} and \textsc{DuquenneMinimization}
being a variation of \textsc{MinCover} (more precisely, of the algorithm
provided by Day in a lattice-theoretic framework). This second procedure can
be found in \cite{duquenne_variations_2007}. Even though \textsc{ShockMinimization} is an algorithm from FCA community, we may include
discussion about it in this part because of the theorem it is based on. Secondly, we will dive into DB (Database) and graphs domains by working on
\textsc{MaierMinimization} (see \cite{maier_theory_1983, maier_minimum_1980}) and FD-Graphs, an extension of minimization to graphs provided by Ausiello and al; in \cite{ausiello_graph_1983, ausiello_minimal_1986, ausiello_directed_2017}. Eventually we get into algorithms coming out of 
boolean logic and query learning communities with \textsc{BercziMinimization}
and \textsc{AFPMinimization} (see \cite{berczi_directed_2017} and \cite{angluin_learning_1992, arias_canonical_2009} resp.).

\vspace{1.2em}

Regarding the order in which we will study the algorithms, there is no particular choice but our proximity with the domains. Because \textsc{MinCover} 
is the algorithm we are starting from, it seemed logical for us to explain
it first. Furthermore, we try as much as possible to provide hands made elements
of proof so that the study would be self-sufficient. Still, we may find in all
the papers we cite more technical definitions and proofs extending 
the knowledge we summarize here.




% ---------------------------------------------------------------------------- %
% ==== Section: MinCover, Shock, Day, Duquenne 
% ---------------------------------------------------------------------------- %

\section{Algorithms on closure systems}

In this section we will be interested in algorithms arising from Formal Concept
Analysis. More precisely we will rely on the work provided by Ganter, Day, Wild,
Duquenne-Guigues in \cite{ganter_formal_1999, ganter_two_2010, day_lattice_1992, wild_implicational_1989, wild_computations_1995, guigues_familles_1986, duquenne_variations_2007}. In this framework we will heavily rely on pseudo-closed and quasi-closed sets.


% ==== Minimal Cover ==== %

\subsection{Minimal Cover}

The minimization procedure we will describe in this section, soberly called
\textsc{MinCover} is the starting point for this master thesis. It can be found
in \cite{ganter_conceptual_2016}. As we shall explain, it has roots in
Day lattice-based algorithm \cite{day_lattice_1992}, and more surprisingly, 
\textit{"unknown"} ancestor in Shock algorithm \cite{shock_computing_1986}. 

\vspace{1.2em}

The principle is to perform \belemp{right-saturation}, and then \belemp{body 
redundancy} elimination. In fact, not only this is the general idea recently 
issued in \cite{boros_strong_2017}, but it is also the main theorem of Shock in 
\cite{shock_computing_1986} and the part of a theorem by Wild 
(\cite{wild_implicational_1989, wild_theory_1994}). This procedure has the 
advantage to be somehow intuitive. Indeed, right-saturation means replacing the 
conclusion of an implication by the closure of its premise:

\begin{center} $A \imp B$ becomes $A \imp \I(A)$ (of course $B \subseteq 
	\I(A)$)
\end{center}

\noindent hence, it means that we associate to $A$, all the information we can 
reach starting from $A$. Then, we perform body redundancy elimination. That is, 
for each right-closed implication, we check whether the amount of knowledge 
represented by $\I(A)$ depends necessarily on $A$. In other words, we remove
$A \imp \I(A)$ from $\I$, and if starting from $A$ we still get the same amount 
of information ($\I^{-}(A) = \I(A)$), then $A$ is not required to get $\I(A)$: $A \imp \I(A)$ can be removed. The second loop can also be named \belemp{left-saturation} since we are maximizing a premise up to its implication. As we will see, because implications are right-closed, this is left-saturation is pseudo-closing  or closing a premise. Now that the principle is explained in words, let us introduce the pseudo-code (see 
algorithm \ref{alg:MinCover}).

\vspace{1.2em}

\begin{algorithm}
	\KwIn{$\I$: an implication base}
	\KwOut{the canonical base of $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I := \I - \{ A \imp B \}$ \;
		$B  := \I(A \cup B)$ \;
		$\I := \I \cup \{ A \imp B \}$ \;
	}
	
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I := \I - \{ A \imp B \}$ \;
		$A  := \I(A)$ \;
		\If{$A \neq B$}{
			$\I := \I \cup \{ A \imp B \}$ \;	
		}
		
	}
	
	\caption{\textsc{MinCover}}
	\label{alg:MinCover}	
\end{algorithm}

\textsc{MinCover} ends up on the canonical basis. Assuming that 
closure are computed with \textsc{LinClosure}, the overhaul complexity of
the algorithm is $O(|\B||\I|)$. To see correctness of the algorithm, observe
that the resulting $\I_c$ is equivalent to $\I$ at the end of the algorithm.
Indeed, at the end of the first loop, we replaced $B$ in every implications $A 
\imp B$ of $\I$ by $\I(A)$. But by proposition 1, $\I \models A \imp B$ if and 
only if $B \subseteq \I(A)$. This is in particular the case for $B = \I(A)$.
In the second loop we remove an implication only if it is redundant, thus
the resulting $\I_c$ is indeed equivalent to $\I$. The main question is 
minimality of $\I_c$. Recall that the DQ-basis, being minimal is based on 
pseudo-closed sets. Hence, if we can show that we keep an implication in the 
second loop only if the premise $\I^{-}(A)$ is pseudo-closed, we are done. This
is the purpose of next \textit{"hand-made"} proposition:

\begin{proposition} Let $\I$ be a \belemp{right-closed} implication theory. 
	Denote $\I^{-}(A) := (\I - \{ A \imp \I(A) \})(A)$, the following holds for 
	all $A \imp \I(A) \in \I$:
	\begin{itemize}
		\item[(i)] if $\I(A) = \I^{-}(A)$, $A \imp \I(A)$ is redundant in $\I$,
		\item[(ii)] if $\I(A) \neq \I^{-}(A)$, $\I^{-}(A)$ is pseudo-closed.
	\end{itemize}
	
\end{proposition}

\begin{proof} \textit{(i)} is trivial by definition. For \textit{(ii)}, let us
show that $\I^{-}(A)$ is quasi-closed, and then minimal among quasi-closed 
sets in its equivalence class. Suppose $\I^{-}(A)$ is not quasi-closed, then 
there must exist $B \subseteq \I^{-}(A)$ such that $\I(B) \nsubseteq \I^{-}(A)$ 
and $\I(B) \neq \I(\I^{-}(A)) = \I(A)$. Because $B \subseteq \I^{-}(A)$, either 
$\I(B) \subset \I(A)$ or $\I(B) = \I(A)$. If we are in the equality case, 
we are done. So let $B \subseteq \I^{-}(A)$ and $\I(B) \subset \I(A)$. By 
definition of $\I^{-}$, if there exists such $B$, either it is closed in 
$\I$ and we are done, or there exist implications $C_i \imp \I(C_i)$ such that 
$C_i \subseteq B$, $\I(C_i) \nsubseteq B$ with $\bigcup \I(C_i) = \I(B)$. But 
for all such implications, $C_i \subseteq \I^{-}(A)$, and by construction of 
$\I^{-}(A)$, $\I(B) = \bigcup \I(C_i) \subseteq \I^{-}(A)$. Hence, 
$\I^{-}(A)$ is indeed quasi-closed. Now, let us show that it is minimal among 
quasi-closed sets in its equivalence class. If $A$ is closed in $\I^{-}$, the 
result is direct, because for all $C \imp \I(C)$ in $\I$, either $C \nsubseteq 
A$ or $(C \subseteq A) \ \land \ (\I(C) \subseteq \I^{-}(A) = A)$. Assume the 
presence of some $B$ such that $A \subseteq B \subseteq \I^{-}(A)$ with $B$ 
being quasi-closed. Note that if $A$ is not closed under $\I^{-}$, it
cannot be quasi-closed. If $B \imp \I(B) = \I(A)$ is in $\I$, then 
$\I^{-}(A) = \I(A)$ and we have a contradiction. If $B \imp \I(B) \not\in \I$, 
then we have $\I^{-}(A) = B$ because $B$ contains $A$ and will be closed under 
$\I^{-}$, which concludes the proof.
\end{proof}

\vspace{1.2em}

In fact, the second loop of \textsc{MinCover} is performing somehow the following operation: $\I^{-}(A) := A \cup \bigcup \{\I(B) \ | \ B \subset A\}$
related to \midemp{proper implications} we shall encounter in the third chapter.
Still, This proposition is sufficient for the algorithm to correctly end up on the canonical basis. Interestingly, the main idea of \textsc{MinCover} is similar to the theorem 2.1 of Shock in \cite{shock_computing_1986}, but the algorithm in practice is much closer from the procedure given by Day in section 6 of \cite{day_lattice_1992}. Before moving to their work, let us settle down an example of trace for \textsc{MinCover}.

\vspace{1.2em}

\paragraph{Example} Let us discuss the following example:
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item[-] $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, abcd $\imp$ ef} \} 
\end{itemize}
We will present a trace of \textsc{MinCover} by a sequence of vectors 
representing $\I$ after modifications:

\[
\begin{pmatrix}
ab  \imp  cde  \\
cd  \imp   f    \\
c  \imp  a     \\
d  \imp  b     \\
abcd \imp ef \\
\end{pmatrix}
\imp
\begin{pmatrix}
ab \imp \text{\midemp{abcdef}}   \\
cd \imp \text{\midemp{abcdef}}   \\
c \imp \text{\midemp{ac}}        \\
d \imp \text{\midemp{bd}}        \\
abcd \imp \text{\midemp{abcdef}} \\
\end{pmatrix}
\imp
\begin{pmatrix}
ab \imp abcdef \\
\text{\aliemp{abcdef $\imp$ abcdef}} \\
c \imp ac \\
d \imp bd \\
\text{\aliemp{abcdef $\imp$ abcdef}} \\
\end{pmatrix}
\imp
\begin{pmatrix}
ab \imp abcdef \\
c \imp ac \\
d \imp bd \\
\end{pmatrix}
\]

The first vector is the initial basis. Then we perform right-saturation. The
third vector differs a bit from true execution of \textsc{MinCover}, but it
illustrates replacement of $A$ by $\I^{-}(A)$ in $A \imp \I(A)$. As we can see,
two implications have the same premises and conclusion: they are useless and
hence removed in the resulting $\I$, being the last vector.

\vspace{1.2em}

Now that things should be a bit clearer, let us discuss the two other 
algorithms previously cited. Remark that we will not explain the procedure
given by Wild in \cite{wild_implicational_1989, wild_theory_1994} because it
is strictly \textsc{MinCover}:
\begin{enumerate}
	\item right-close all implications of $\I$,
	\item find a minimal non-redundant subfamily of implications in $\I$
	right-closed, i.e redundancy elimination. 
\end{enumerate}
\noindent Hence, the procedure given by Shock is presented in algorithm 
\ref{alg:Shock}.

\begin{algorithm}
	\KwIn{$\I$: a theory to minimize}
	\KwOut{a minimum cover for $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I := \I - \{ A \imp B \}$ \;
		\If{$B \nsubseteq \I(A)$}{
			$\I := \I \cup \{A \imp \I(B) \}$ \;	
		}
		
	}
	\caption{\textsc{ShockMinimization}}
	\label{alg:Shock}
\end{algorithm}

This routine, co-issued with the theorem we discussed previously is 
quite different from \textsc{MinCover}. Even though the conditional statement 
$B \nsubseteq \I(A)$ is equivalent to $\I(A) \neq \I^{-}(A)$ and 
replacing $A \imp B$ by $A \imp \I^{-}(B)$ is about right-closing $A \imp B$, 
the resulting basis of this algorithm may not be minimal in
general:
\begin{itemize}
	\item[-] if $\I = \{ \emptyset \imp a, \ a \imp b \}$ (in this order), 
	\textsc{ShockMinimization} will produce $\emptyset \imp ab$ which is right,
	\item[-] if $\I = \{ a \imp b, \ \emptyset \imp b \}$, the result will
	be $\{ a \imp ab, \ \emptyset \imp ab \}$ being redundant.
\end{itemize} 
In fact, this error has already been pointed out in 1995 by Wild in 
\cite{wild_computations_1995}. However, the implications given by Shock may have quasi-closed premises and present similarities with the algorithm for generating quasi-closed sets \textsc{FindCrucialGenerators} of Day (\cite{day_lattice_1992}). In Wild work, we can also find another proof for the theorem of Shock, jointly with minimality of DG-basis (theorem 5 of \cite{wild_theory_1994, wild_computations_1995}). Let us discuss now the algorithm proposed by A. Day in 1992 (see pseudo-code \ref{alg:Day}: \textsc{DayMinimization}). We express it through our framework of closure systems and implications. Since it would only complicate our explanations, we do not integrate definitions of lattice theory. However, for the reader with few background in lattice theory, we can mention Day's overhaul framework in some sentences. At the best of our understanding, the main idea is to focus on the partial order $(2^{\Sg}, \subseteq)$ being a complete join-semilattice (or just complete lattice when we consider finite $\Sg$) where the closure operator $\I(\cdot)$ is in fact a $\bigvee$-morphism from $(2^{\Sg}, \subseteq)$ to $(\Sg^{\I}, \subseteq)$. In $(2^{\Sg}, \subseteq)$, the join operation ($\lor$) is set union. Consequently, the equivalence classes of $\I$ define a congruence on the powerset of $\Sg$ being the kernel of $\I(\cdot)$. The approach developed by A. Day is then to build congruences (hence closure operators) through ordered pairs (or quotients) of the same equivalence class, representing implications.

\begin{algorithm}
	\KwIn{$\I$: a theory to minimize}
	\KwOut{canonical basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I := \I - \{ A \imp B \}$ \;
		$A := \I(A)$ \;
		$B := \I(A \cup B)$ \;
		\If{$A \neq B$}{
			$\I := \I \cup \{ A \imp B \}$ \;	
		}
		
	}
	
	\caption{\textsc{DayMinimization}}
	\label{alg:Day}
\end{algorithm}

\noindent Here, equivalence with \textsc{MinCover} is clear. The only difference
is the order in which operations are performed. Even though the 2 algorithms 
rely on the same computation, it is worth noting a particular case where the
algorithm by Day may fail. Consider a system not being reduced, e.g:

\[ \I =  \{ b \imp ac, \ c \imp a, \  c \imp b \} \]

\noindent we have:
\begin{itemize}
	\item[-] \textsc{MinCover} output: $\{ b \imp ac, \ c \imp abc \}$,
	\item[-] \textsc{DayMinimization} output: $ \{ b \imp ac, \ ac \imp abc, \  c \imp abc \}$
\end{itemize}
In general, performing right-closure before redundancy elimination, as in 
\textsc{MinCover} avoids this problem. 

\vspace{1.2em}

In this section we reviewed the algorithm acting as our starting point, by 
studying its complexity and principle. We also linked it to research we made
and other algorithms we found. As exposed, \textsc{MinCover} summarizes and 
corrects all material we covered here, and hence justifies not to implement
all of them. The next section defines a slightly different algorithm, being a
variation of the work from \cite{day_lattice_1992}.

\subsection{Duquenne algorithm}

Later, also based on Day lattice theoretic work and its own approach, Duquenne
proposed a variation of \textsc{MinCover} based on first computing quasi-closed
sets and then using the recursive characterization of pseudo-closed sets to iteratively build the Duquenne-Guigues basis. Recall that pseudo-closed sets
are particular open sets that can be defined by two means:
\begin{itemize}
	\item[-] a set $P$ is pseudo-closed if it is larger than all the closure of any	sub pseudo-closed set,
	\item[-] $P$ is pseudo-closed if it is quasi-closed and minimal among quasi-closed sets of $\left[ P \right]_{\I}$.
\end{itemize}
We will call this procedure \textsc{DuquenneMinimization} (see algorithm \ref{alg:Duquenne-min}). In fact, It uses the algorithm 2 from \cite{day_lattice_1992} to compute quasi-closed sets from premises of $\I$,
this is the first loop. All pseudo-closed sets are included in the resulting $\I$ after this first step. Then, we use \belemp{lectic ordering} to have a $\subseteq$-compatible way to process implications before building $\I_c$.


\begin{algorithm}
	\KwIn{$\I$ a theory to minimize}
	\KwOut{$\I_c$ the DQ-basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		$\I = \I - \{ A \imp B \}$ \;
		$A := \I(A)$ \;
		
		\If{$B \not\subseteq A$}{
			$B = B \cup A$ \;
			$\I := \I \cup \{ A \imp B \}$ \;	
		}

	}

	\BlankLine
	
	\textsc{LecticOrder}($\I$) \;
	$\I_c := \emptyset$ \;
	
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		\ForEach{$\alpha \imp \beta \in \I_c$}{
			\If{$\alpha \subset A \land \beta \nsubseteq A$}{
				$\I = \I - \{ A \imp B \}$ \;
				\textbf{goto next $A \imp B \in \I$} \;
		
			}
	
		}
	
		$B = \I(B)$ \;
		$\I_c := \I_c \cup \{ A \imp B \}$ \;
	}

	\BlankLine
	
	return $\I_c$ \;
	
	\caption{\textsc{DuquenneMinimization}}
	\label{alg:Duquenne-min}
\end{algorithm}

Before proving the algorithm, let us define lectic ordering $\leq_{\Sg}$. First, we must assume that $\Sg$ can be assigned a total order $\leq$. For the recall, an order is total if for all pairs $(x, \ y)$ of $\Sg$, $x \leq y$ or $y \leq x$. Hence, provided $\Sg$ is a chain, we can define $\leq_{\Sg}$ on
$2^{\Sg}$ as follows: $\forall A, B \subseteq \Sg$ $A \leq_{\Sg} B$ if the
smallest element in which $A$ and $B$ differ belongs to $B$. We say that $A$ is \belemp{lectically smaller than} B. Note that $\leq_{\Sg}$ is \belemp{$\subseteq$-compatible}, that is $A \subseteq B \imp A \leq_{\Sg} B$.
The opposite direction however does not hold since $\subseteq$ is a partial 
ordering, while $\leq_{\Sg}$ is total. 

\paragraph{Example} Consider $\Sg = \{ a, b, c \}$ with $a < b < c$. In
this setting, $b \leq_{\Sg} a$, $b \leq_{\Sg} ab$ and $b \leq_{\Sg} bc$ for instance. To observe $\subseteq$-compatibility, we can refer to figure \ref{fig:II-LecticSubset} illustrating the two orderings side-by-side. 

\begin{figure}[ht]
	\input{Pictures/II/Lectic.tex}
\end{figure}

\vspace{1.2em}

The advantage of lectic ordering is to allow for easier checking of pseudo-closeness recursive property: if we test sets in lectic order,
we are sure not to avoid any pseudo-closed subset of a given set when
looking at previously considered ones. Furthermore, because this is 
a total order, we can use fast logarithmic sorting procedure to order 
efficiently sets. Next, let us give some elements of proof for this 
algorithm.


\begin{proposition} The following statements hold for all $A \imp B \in \I$, $\I^{-} = \I - \{ A \imp B \}$:
	\begin{itemize}
		\item[(i)] $\I^{-}(A) = \I(A)$, then $A \imp B$ is redundant,
		\item[(ii)] $\I^{-}(A) \neq \I(A)$, then $\I^{-}(A)$ is quasi-closed in $\I$.
	\end{itemize}
\end{proposition}

\begin{proof} \textit{(i)}. $\I^{-}(A) = \I(A)$ is equivalent to $B \subseteq \I^{-}(A)$, that is $\I - \{ A \imp B \} \models A \imp B$. \textit{(ii)}. Assume $\I^{-}(A) \neq \I(A)$ and $\I^{-}(A)$ not quasi-closed. Then we must be
able to find an implication $\alpha \imp \beta$ such that $\alpha \subseteq \I^{-}(A)$ and $ \I(\alpha) \nsubseteq \I^{-}(A)$. We have also $\I(\alpha) \subset \I(A)$. $\I^{-}$ is a closure operator, hence $\alpha \subseteq \I^{-}(A) \imp \I^{-}(\alpha) \subseteq \I^{-}(A)$. $\I^{-}(\alpha) \subseteq \I^{-}(A)$ and $ \I(\alpha) \nsubseteq \I^{-}(A)$ leads to $\I^{-}(\alpha) 
\subset \I(\alpha)$. Because of $\I^{-}$ computations, this leads to $A \subseteq \I(\alpha)$, hence by monotonicity of $\I$, $\I(A) \subseteq \I(\alpha)$ contradicting $\I(\alpha) \subset \I(A)$.

\end{proof}

Next, we will assume a lemma given in \cite{duquenne_variations_2007} from where
the algorithm comes:

\begin{lemma} For lists $\I$, $\mathcal{H}$ of implications, with $\mathcal{H} \subseteq \I$, the following statements are equivalent:
\begin{itemize}
	\item[(i)] $\I \equiv \mathcal{H}$,
	\item[(ii)] $\I$ has the same canonical basis as $\mathcal{H}$,
	\item[(iii)] for every pseudo-closed set $P$ of $\I$, there is at least
	one $A \imp B \in \mathcal{H}$ for which $A \subseteq P \subset \I(P) = \mathcal{H}(A)$.
\end{itemize}
\end{lemma}

Because $\I^{-}$ is a closure operator, if $\I^{-}(A)$ is not closed under $\I$,
it is then the smallest quasi-closed set containing $A$. Furthermore, both $A$
and $\I^{-}(A)$ belong to $\left[ A \right]_{\I}$. With lemma 1 where $\mathcal{H} = \I$, one has that every pseudo-closed sets of $\I$ must be a premise of $\I$ after the first loop.

\begin{proposition} At the end of \textsc{DuquenneMinimization} $\I_c$ is the
DG-basis of $\I$.
\end{proposition}

\begin{proof} We proved that after the first step, all pseudo-closed sets
	appeared as premise of $\I$. Because we use a $\subseteq$-compatible ordering on premises of $\I$ with lectic order, the nested loop on $\I_c$
	checks the recursive property of being pseudo-closed. Hence we only add
	implications $P \imp \I(P)$ where $P$ is $\I$-pseudo-closed to $\I_c$.
\end{proof}

\paragraph{Example} In order to clarify the algorithm, let us execute it on a
small example. Because we may have a lazy imagination, consider the example we used for \textsc{MinCover}:
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item[-] $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, 
		abcd $\imp$ ef} \} 
\end{itemize}
We proceed by steps:
\begin{enumerate}
	\item \midemp{left-saturation}. For all implications $A \imp B$ of $\I$,
	we compare $\I(A)$ and $\I^{-}(A)$ where $\I^{-}$ is $\I$ from which we removed $A \imp B$. We should keep in mind that $\I^{-}$ is different 
	for every $A \imp B$ then, not only because previous implications are altered or remove, but also since at each step we delete a different implication from $\I$. Let us present results of this step through table \ref{tab:duq-red}.
	
\begin{table}[ht]
	\begin{tabular}{| c || p{2.7cm} | c | p{2.5cm} | c | c | p{2.7cm} | }
	\hline \rowcolor{clouds}
	$\B(\I)$ & $\I$ before & $\I(\cdot)$ & $\I^{-}$ & $\I^{-}(\cdot)$ & imp ? & $\I$ after \\ \hline
	
	$ab$ & $ab \smimp cde, cd \smimp f,$ \newline $c \smimp a, d \smimp b,$ \newline  $abcd \smimp ef$ & $abcdef$ & $cd \smimp f, c \smimp a,$ \newline $d \smimp b,$ \newline $abcd \smimp ef$ & $ab$ & $ab \imp abcde$ & $ab \smimp abcde,$ \newline $cd \smimp f, c \smimp a,$ \newline $d \smimp b, abcd \smimp ef$ \\	\hline
	 
	$cd$ & $ab \smimp abcde,$ \newline $cd \smimp f, c \smimp a,$ \newline $d \smimp b, abcd \smimp ef$ &	$abcdef$ & $ab \smimp abcde,$ \newline $c \smimp a, d \smimp b,$ \newline $abcd \smimp ef$ & $abcdef$ & \aliemp{removed} & $ab \smimp abcde, c \smimp a,$ \newline $d \smimp b, abcd \smimp ef$ \\ \hline
	
	$c$ & $ab \smimp abcde, c \smimp a,$ \newline $d \smimp b, abcd \smimp ef$ & $ca$ & $ab \smimp abcde$ \newline $d \smimp b, abcd \smimp ef$ & $c$ & 
	$c \imp ca$ & $ab \smimp abcde,$ \newline $c \smimp ca, d \smimp b,$ \newline $abcd \smimp ef$ \\ \hline
	
	$d$ & $ab \smimp abcde,$ \newline $c \smimp ca, d \smimp b,$ \newline $abcd \smimp ef$ & $db$ & $ab \smimp abcde,$ \newline $c \smimp ca,$ \newline $abcd \smimp ef$ & $d$ & $d \imp db$ & $ab \smimp abcde,$ \newline $c \smimp ca, d \smimp db,$ \newline $abcd \smimp ef$ \\ \hline
	
	$abcd$ & $ab \smimp abcde,$ \newline $c \smimp ca, d \smimp db,$ \newline $abcd \smimp ef$ & $abcdef$ & $ab \smimp abcde,$ \newline $c \smimp ca, d \smimp db$ & $abcde$ & $abcde \imp abcdef$ & $ab \smimp abcde,$ \newline $c \smimp ca, d \smimp db,$ \newline $abcde \smimp abcdef$ \\ \hline
		
	\end{tabular}

\caption{First step of \textsc{DuquenneMinimization}}
\label{tab:duq-red}
\end{table}
Observe that at the second step, when we consider $cd$, because $\I(cd) = \I^{-}(cd)$, the implication $cd \imp f$ is removed from $\I$. The basis we get
after this step is: $ab \imp abcde, \ c \imp ca, \ d \imp db,$  $abcde \imp abcdef$.

\item \midemp{lectic ordering} Here we will not follow the sorting procedure, but instead, we will illustrate how does lectic ordering work in practice, and how to compute it at hands easily. Say we have the following \belemp{total} order in elements of $\Sg$: $a < b < c < d < e < f$. We can imagine represent
a subset of $\Sg$ by a binary string of size $|\Sg|$ where the least significant bit (on the right) corresponds to $f$, and the most significant bit
(on the left) matches $a$. The binary string associated to some $A \subset \Sg$ will have ones in place of elements it contains. For instance, the subset 
$acef$ will have $101011$ as binary string:

\[\begin{matrix}
	$a$ & $b$ & $c$ & $d$ & $e$ & $f$ \\
	1 & 0 & 1 & 0 & 1 & 1 \\
\end{matrix}\]

From this point of view, lectic ordering is just binary enumeration. A premise $A_1$ will be lower than $A_2$ under lectic order if the binary string
associated to $A_1$ comes before the word related to $A_2$  when enumerating naturally binary numbers. In our case, we have:
\begin{itemize}
	\item[-] $ab$: $110000$,
	\item[-] $c$: $001000$,
	\item[-] $d$: $000100$,
	\item[-] $abcde$: $111110$.
\end{itemize}
Ordering those premises by their order of appearance under binary counting, we
get $\I$ shuffled as follows: $ d \imp db, \ c \imp ca, \ ab \imp abcde, \ abcde \imp abcdef$.

\item \midemp{getting pseudo-closed implications} For each implication of $\I$, we want to check whether its premise is pseudo-closed or not. To do this, we will build iteratively our resulting basis $\I_c$, containing left-pseudo-closed and right-closed implications (see table \ref{tab:duq-PC}).

\begin{table}[ht]
	\centering
	\begin{tabular}{| c | p{3cm} | c | p{3cm} |}
		\hline \rowcolor{clouds}
		implication & input $\I_c$ & pseudo-closed ? & output $\I_c$ \\ \hline
		$d \smimp db$ & $\emptyset$ & $\lor$ & $d \smimp db$ \\ \hline
		$c \smimp ca$ & $d \smimp db$ & $\lor$ & $d \smimp db, c \smimp ca$ \\ \hline
		$ab \smimp abcde$ & $d \smimp db, c \smimp ca$ & $\lor$ & 
		$d \smimp db, c \smimp ca$ \newline $ab \smimp abcdef$ \\ \hline
		$abcde \smimp abcdef$ & $d \smimp db, c \smimp ca$ \newline $ab \smimp abcdef$ & $\times$: \aliemp{ab $\smimp$ abcdef} & $d \smimp db, c \smimp ca$ \newline $ab \smimp abcdef$ \\ \hline
	\end{tabular}
			
\caption{\textsc{DuquenneMinimization} third step}
\label{tab:duq-PC}
\end{table}

As one can see, when we add an implication from $\I$ to $\I_c$ we perform 
right-closing. This is necessary to check pseudo-closeness. Consequently for
the last step, because $ab \imp \I(abcde) = abcdef \in \I_c$, $abcde$ is not
pseudo-closed: $ab \subseteq abcde$ and $\I(ab) = abcdef \nsubseteq abcde$. Thus we do not add it to $\I_c$. At the end of the algorithm we have $\I_c = 
d \imp db, \ c \imp ca, \ ab \imp abcdef$.
\end{enumerate}

Regarding the complexity of the algorithm, one may note that the first loop has
the same complexity as the second step of \textsc{MinCover}, that is $O(|\B||\I|)$ provided we use \textsc{LinClosure} for lowering theoretical complexity of closure computations. Observe then that the lectic order is a total order, hence we can use a logarithmic sorting function as quick-sort for
\textsc{LecticOrder}, resulting in $O(|\Sg||\B|\log_2(|\B|))$ due to $O(|\Sg|)$
comparison of two sets. Eventually, for each $A \imp B$, the nested for each loop may require $O(|\I|)$ since we are performing set operations on at most as much implications as $|\B|$. Then we may perform a closure under $\I$, being
$O(|\I|)$ also. Therefore, the overhaul loop should require $O(|\B||\I|)$ time.
From the three steps we have, we can conclude that the whole algorithm has complexity $O(|\B||\I|)$ as \textsc{MinCover}. However, the first loop of 
\textsc{DuquenneMinimization} act as a redundancy elimination which helps to
reduce the cost of closure computations in the whole algorithm. This could be
an improvement of \textsc{MinCover} to test in practical implementation.

\vspace{1.2em}

In this section we were interested in studying algorithms built on the work of
Wild, Day, Duquenne-Guigues and Ganter mainly relying on left and right-saturation of implications to produce the canonical basis. In the next section we will be involved in studying algorithms based on the work of Maier
and later, Ausiello.


% ---------------------------------------------------------------------------- %
% ==== Section: DB algorithms
% ---------------------------------------------------------------------------- %


\section{Algorithms based on Maier's database approach}

Here, we deal with algorithms based on Functional Dependencies (FD's). In database theory, functional dependencies are also implications between sets
of attributes. The difference lies in the value of those attributes. Up to know we have been considering \textit{"binary"} attributes and implications: either
we have $x$ or we do not have it. In DB, an attribute can be multi-valued (imagine an attribute \textit{"age"} for our plant example). Therefore, the 
notion of implication is stricter: if we are given two attributes $x$ and $y$,
$x \imp y$ will be a valid functional dependency if the \belemp{value} (not the presence) of $x$ determines the value of $y$. For more precision, see \cite{maier_theory_1983}. Actually, when it comes at minimization this semantic difference does not matter. Therefore, the algorithms we are about to study suits also our framework. For this reason we will explain algorithms within our
usual context.

\subsection{First algorithm: Maier's algorithm on FDs}

Here we will consider one of the first algorithm given for minimization 
purpose. It has been proposed by Maier in \cite{maier_theory_1983, 
maier_minimum_1980} and relies notably on the algorithm \textsc{LinClosure} issued in \cite{beeri_computational_1979}. For understandability we will explain this algorithm through implication theories framework while drawing parallel with Maier's notation and definitions. As a soft introduction, we will develop an interesting and simple example in Maier's algorithm context.


\paragraph{Example} Let $\Sg$ and $\I$ be as follows (in fact, same example as 
in previous section):
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item[-] $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, abcd $\imp$ ef} \} 
\end{itemize}
\noindent Let us try to minimize it \textit{"with hands"}. First, we see \textit{abcd $\imp$ ef} to be redundant. Indeed, if we remove it from $\I$, we still have \textit{ab $\imp$ cde} and \textit{cd $\imp$ f}, thus $\I^{-} := \I - \{ abcd \imp ef \} \models abcd \imp ef$. $\I^{-}$ is not redundant any more. 
Nevertheless, we can still remove an implication. Indeed, not only we can reach 
$ab$ from $cd$, but also $cd$ from $ab$. Consequently, we could remove $cd \imp 
f$ from $\I^{-}$ while adding to the head of $ab \imp cde$ the element
$f$ (the head of $cd \imp f$) to avoid loss of informations. Hence, we 
would end up with 
\begin{center}
	$\I =$ \{ \textit{c $\imp$ a, d $\imp$ b, ab $\imp$ cdef} \}
\end{center}

\vspace{1.2em}

Those steps of redundancy elimination and equivalence manipulation are the core 
manipulations of Maier algorithm. For the recall, Maier worked with functional
dependencies, but this makes no difference when it comes as implications. 

\paragraph{Redundancy elimination} As mentioned in the first chapter, given 
$\I$, $A \imp B \in \I$ is redundant if $\I^{-} := \I - \{ A \imp B \} \models A \imp B$ or equivalently if $B \subseteq \I^{-}(A)$. Thus get rid of 
redundancy elimination can be done in the following procedure:

\begin{algorithm}
	\KwIn{$\I$: an implication theory}
	\KwOut{$\I$ without redundant implications}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		\If{$\I - \{ A \imp B \} \models A \imp B$}{
			remove $A \imp B$ from $\I$ \;
		}
		
	}
	
	\caption{\textsc{RedundancyElimination}}
	\label{alg:Maier-RE}	
\end{algorithm}

\noindent Checking for redundancy is done with \textsc{LinClosure}. Because this
is done for all implications of $\I$, complexity of redundancy elimination is
$O(|\B| \times |\I|)$.

\vspace{1.2em}

\paragraph{Equivalence classes} As briefly described previously, we can identify
equivalence classes within $\Sg^{\I}$. For $X \subseteq \Sg$, we can set up
$\left[ X \right]_{\I} = \{ Y \subseteq \Sg \ | \ \I(Y) = \I(X) \}$. Recall 
that this is the definition of an \belemp{equivalence class} we presented in the first chapter. More than this, we can limit those equivalence classes to premises of $\I$, i.e for some $A \subseteq \Sg$ let
\begin{itemize}
	\item[(i)] $E_{\I}(A) = \{ X \imp Y \in \I \ | \ X \in \left[ A 
	\right]_{\I} \}$ 
	\item[(ii)] $e_{\I}(A) =\{ X \ | \ X \in \B(\I) \cap \left[ A \right]_{\I} 
	\}$
\end{itemize}
\noindent Plus, we say that $A$ \belemp{directly determines} $B$, denoted $A 
\ddv B$, if $\I - E_{\I}(A) \models A \imp B$. Now, the minimization process
in \cite{maier_theory_1983, maier_minimum_1980} is the following: 

\begin{proposition} Let $\I$ be an irredundant theory. If $A \imp B, \ C \imp D 
\in \I$ (distinct) are such that $C \equiv_{\I} A$ and $A \ddv C$, then we can remove $A \imp B$ from $\I$ and replace $C \imp D$ by $C \imp D \cup B$ without 
altering $\Sg^{\I}$.
\end{proposition}

\begin{proof} Suppose we removed $A \imp B$ and modified $C \imp D$ to $C \imp D \cup B$. Put $\I^{-}$ as the system we obtained. The main point is to show that we still have $\I^{-} \models A \imp B$. Recall $A \ddv C$, thus:

\begin{align*}
	(\I - E_{\I}(A) \models A \imp C) & \; \imp \I - A \imp B \models A \imp 
	C \\
	& \; \imp \I^{-} \models A \imp C \\ 
\end{align*}

\noindent Because we changed $C \imp D$ to $C \imp D \cup B$, we have then

\[ (\I^{-} \models A \imp C) \land (\I^{-} \models C \imp D \cup B) \imp
	(\I^{-} \models A \imp B) \]

\noindent by transitivity. Note that equivalence of $A$ and $C$ is 
preserved, because in $\I$, by transitivity $C \imp A \imp B$. Removing $A \imp 
B$ but moving $B$ to $C \imp D \cup B$ preserves $C \imp B$. Also, taking 
equivalent premise is important in order not to alter the closure system 
$\Sg^{\I}$. If $A$ and $C$ were not equivalent, we may have changed the system 
by adding $B$ to the closure of $C$ even though $\I \not\models C \imp B$.
\end{proof}

\vspace{1.2em}

This is worth noting we gave a \textit{"light"} definition of direct determination more relying on a property proved by Maier than the strict original definition. Also, Wild in \cite{wild_implicational_1989, wild_theory_1994} drawn a parallel between quasi-closeness property and equivalence classes $E_{\I}(\cdot)$. Indeed, a set $A$ will be quasi-closed if and only if $\I(A) = \I^{-}_{E_{\I}(A)}(A)$. Finally, the last proposition gives us an algorithmic test for minimization: given an equivalence class $E_{\I}(X)$, one can run across all its implications and successively remove useless ones. Actually it says that if an non-redundant basis contains direct determinations, it is not minimal. Hence contrapositive yields a condition for minimality ensuring correctness and end of the operation.

\vspace{1.2em}

The main question is: how to get the equivalence classes efficiently? It turns
out this can be done using a modified version of \textsc{LinClosure}. It is 
sufficient to embed in the function a vector of implied premises. That is, 
for a given premise $X$, we provide to \textsc{LinClosure} a bit-vector 
$implied$ of size $|\B|$. Within the procedure, whenever we reach $count\left[A \imp B \right] = 0$ for some $A \imp B \in \I$, then $A$ is implied by $X$ under $\I$. Hence we set $implied\left[ A \imp B\right]$ to 1. Doing this operation for all implications in $\I$ provide a matrix $M$ of size $|\B| \times |\B|$. Then, to compute equivalence classes, a travel over the matrix is enough. Two implications $A \imp B$ and $C \imp D$ of $\I$ belong to the same equivalence class if 

\[ M\left[A \imp B, \ C \imp D \right] = 1 \quad \text{and} \quad 
M\left[C \imp D, \ A \imp B \right] = 1 \]

\noindent Building the matrix requires $|\B|$ executions of \textsc{LinClosure}
in any case, thus has $O(|\B||\I|)$ time complexity. Running across $M$ is 
of course $O(|\B|^2)$. Hence the whole operation can be done in $O(|\B||\I|)$,
since $|\B||\I| = |\B|^2|\Sg| > |\B|^2$. We will not rewrite 
\textsc{LinClosure} altered since the modification is about one line in the 
algorithm and quite simple to understand as is. We will not write the run over 
$M$ for conciseness, since principle seems sufficient for understanding. All 
those steps will be summarized as \textsc{EquivClasses}($\I$) in subsequent 
algorithm. Finally, the whole Maier minimization process is given in algorithm
\ref{alg:Maier-Min}.

\vspace{1.2em}

\begin{algorithm}
	\KwIn{$\I$ : a theory to minimize}
	\KwOut{$\I$ minimized}
	
	\BlankLine
	\BlankLine
	
	\ForEach{$A \imp B \in \I$}{
		\If{$\I - \{ A \imp B \} \models A \imp B$}{
			remove $A \imp B$ from $\I$ \;
		}
		
	}
	
	\BlankLine
	
	$E_{\I} := $ \textsc{EquivClasses}($\I$) \;
	
	\BlankLine
	
	\ForEach{$E_{\I}(X) \in E_{\I}$}{
		\ForEach{$A \imp B \in E_{\I}(X)$}{
			\If{$\exists C \imp D \in E_{\I}(X)$ s.t $A \ddv C$}{
				remove $A \imp B$ from $\I$ \;
				replace $C \imp D$ by $C \imp D \cup B$ \;	
			}
		}
		
	}
	
	\caption{\textsc{MaierMinimization}}
	\label{alg:Maier-Min}
\end{algorithm}

A question we could have is about complexity of removing direct determinations.
In fact, we can use again a modified version of \textsc{LinClosure} to find
direct determination. For each implications $A \imp B$, we would have to 
provide \textsc{LinClosure} with a vector of implications with premises 
equivalent to $A$. The first one we reach (i.e the first one for which the 
counter goes to 0) is necessarily an example of direct determination. Moreover,
note that equivalence classes in $E_{\I}$ defines a partition of $\I$. That is,
we will have to compute at most $|\B|$ closures to get rid off direct 
determinations. So, the last step of the algorithm requires $O(|\B||\I|)$,
which is consequently the complexity of \textsc{MaierMinimization} by 
previous explanations. It is important to mention that even though the base
we obtain is minimal, it is not the canonical basis, as we shall see in the
next example, acting as a trace.

\vspace{1.2em}

\paragraph{Example} Let us use the example we presented when studying the first algorithm, but this time, applying explicitly Maier's algorithm on it. As a reminder, we had:
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item[-] $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, abcd $\imp$ ef} \} 
\end{itemize}
\noindent We will proceed by steps.
\begin{enumerate}
	\item \midemp{redundancy elimination}: in this step, we compute the closure
	of each premise to see whether there exists $A \imp B \in \I$ such that 
	$B \subseteq \I^{-}(A)$. It turns out that the only one for which
	this happens is $abcd \imp ef$. After this step, we have:
	
	\begin{center}
		$\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b} \} 
	\end{center}
	
	\item \midemp{getting equivalence classes}: here is a more interesting step.
	First, we have to compute the matrix $M$ (see table \ref{tab:Maier-Mat}).
	The table does not represent the way computations are done, but is still of
	interest. On the left-hand side, we described the closure of each premise
	of $\I$. On the right-hand side, we gave the matrix $M$. We can see that
	an element $M(i, j)$ of $M$ equals 1 if the closure of $i$ contains the
	premise of $j$.
	
	\begin{table}[ht]
		\centering
		\subfloat[closures of $\B(\I)$]{
			\begin{tabular}{| c | c |}
				\hline \rowcolor{clouds}
				$\B(\I)$ & $\I(\cdot)$ \\ \hline
				$ab$ & $abcdef$ \\ \hline
				$cd$ & $abcdef$ \\ \hline
				$c$  & $ac$ \\ \hline
				$d$  & $bd$ \\ \hline
			\end{tabular}
		}\quad
		\subfloat[matrix $M$]{
			\begin{tabular}{c | c c c c}
				& $ab \imp cde$ & $cd \imp f$ & $c \imp a$ & $d \imp b$ \\ 
				\hline
				$ab \imp cde$ & 1 & 1 & 1 & 1 \\
				$cd \imp f$   & 1 & 1 & 1 & 1 \\
				$c \imp a$    & 0 & 0 & 1 & 0 \\
				$d \imp b$    & 0 & 0 & 0 & 1 \\    
			\end{tabular} 
			
		}
		\caption{Computing matrix $M$ of implied premises}
		\label{tab:Maier-Mat}
	\end{table}
	
	\noindent Then, we need to derive out of $M$ the different equivalence 
	classes. For all pairs of implications $(i, j)$, if $M(i, j) = M(j, i) = 1$,
	then they belong to the same equivalence class. In our case, we will 
	partition $\I$ in 3 classes:
	\begin{itemize}
		\item[-] $E_{\I}(ab) = \{ ab \imp cde, \ cd \imp f \}$ (= $E_{\I}(cd)$),
		\item[-] $E_{\I}(c) = \{ c \imp a \}$,
		\item[-] $E_{\I}(d) = \{ d \imp b \}$
	\end{itemize}
	\item \midemp{removing direct determination}: last step. We have to look in
	all equivalence classes for distinct implications with direct determination.
	Because $E_{\I}(c)$ and $E_{\I}(d)$ are of size 1, they cannot be reduced.
	However, $E_{\I}(ab)$ is more interesting. We do not have $ab \ddv cd$. 
	Indeed, the only way to reach $cd$ from $ab$ is to use $ab \imp cde$, that 
	is, an element of $E_{\I}(ab)$. Nevertheless, $cd \ddv ab$ because if we 
	restrict ourselves to $\I - E_{\I}(ab) = \{ c \imp a, \ d \imp b \}$, $cd 
	\imp ab$ holds. Consequently, we can apply our modifications: we remove 
	$cd \imp f$ from $\I$, and $ab \imp cde$ becomes $ab \imp cdef$.
\end{enumerate}
After applying this algorithm, we end up with a minimal $\I$ being:

\[ \I = \{ c \imp a, \ d \imp b, \ ab \imp cdef \} \]

\vspace{1.2em}

In this section we provided a theoretical study of the algorithm 
proposed by Maier in \cite{maier_theory_1983, maier_minimum_1980} for finding a 
minimal cover of a basis $\I$. Based on his results, we stated that the  
asymptotic complexity of this algorithm was $O(|\B||\I|)$. In the next section, 
we will develop another procedure coming from the graph theory community.

\subsection{Graph-theoretic approach to Maier's algorithm}

This section is dedicated to a minimization algorithm relying on graphs. 
It has been set up by Ausiello and al. in \cite{ausiello_directed_2017, 
ausiello_graph_1983, ausiello_minimal_1986}. Starting from a directed 
hypergraph representation of functional dependencies, it builds a special kind 
of directed graph, called \belemp{FD-Graph} with which it reduces the initial 
hypergraph. In order, we are going to define what is a FD-graph, provide the 
general idea for the algorithm as explained in \cite{ausiello_minimal_1986} and 
then go into further details and more precise algorithms for such computations 
as exposed in \cite{ausiello_graph_1983}.

\subsubsection{FD-Graphs and minimum covers}

As we already mentioned, the graph framework developed by Ausiello and al. in
\cite{ausiello_graph_1983, ausiello_minimal_1986} comes from the work of Maier
in database theory over functional dependencies (see \cite{maier_theory_1983}).
Moreover we already discussed the closeness of FD and implications in our 
context, hence we can still consider the algorithms we are about to study from
an implication point of view. This leads to no alteration. Furthermore, the
hypergraph representation of some theory $\I$ is no more than worth mentioning
for us, since it just presents an attractive graphical description of $\I$.
Because the structure presented by Ausiello is a particular kind of directed
graph, let us try to keep explanations as simple as possible and stick to this
one. It might be first interesting to recall what are graphs (undirected and 
directed) as an introduction.

\begin{definition}[\midemp{graph}] A \belemp{graph} $G = (V, E)$ is a pair
of sets where $V$ is a set of \belemp{nodes} or \belemp{vertices} and $E$ is
a set of unordered pair called \belemp{edges} or \belemp{arcs} from $V^2$.
\end{definition}

\begin{definition}[\midemp{directed graph}] A graph $G = (V, E)$ where $E$ is
a set of \belemp{ordered} pairs from $V^2$ is called a \belemp{directed graph}.	
\end{definition}

\paragraph{Example} Let us illustrate those notions with examples. First, let us
imagine a graph (not directed) $G_1 = (V_1, E_1)$ where $V_1 = \{
a, b, c, d, e\}$ is a set of cities and
$E_1$ would be railways between them, as $(b, c)$ and $(a, d)$ for example. 
Because railways are bidirectional, if we can go from one city to another, then 
the other way around is valid too. One possible \textit{"map"} is represented 
on the left side of figure \ref{fig:II-graph}.

\begin{figure}[ht]
	\input{Pictures/II/Graphs.tex}
\end{figure}

For an example of directed graph, recall our \textit{"like"} binary relation from chapter 1. The associated graph is $G_2 = (V_2, E_2)$ where $V_2 = $ 
\{\textit{Narcisse, Neige, Jezabel, Seraphin}\} and for instance, 
(\textit{Seraphin, Jezabel}) is an edge of $E_2$ while (\textit{Jezabel, 
Seraphin}) is not. See right-hand side of figure \ref{fig:II-graph} for an 
illustration.

\vspace{1.2em}

Now that the notion of graph may be clearer, let us introduce a particular
kind of directed graph issued in \cite{ausiello_graph_1983, 
ausiello_minimal_1986} as an improvement of the structure proposed in 
\cite{ausiello_graph_1980}. It deserves to represent implication theories within
a framework simpler than hypergraphs. It has been recently re-issued in 
\cite{ausiello_directed_2017} being a survey.

\begin{definition}[\midemp{FD-Graph}] Given a theory $\I$ over some $\Sg$, 
the directed graph $G_{\I} = (V, \  E)$ such that:
\begin{itemize}
	\item[-] $V = V_0 \cup V_1$ is the set of nodes where:
	\begin{itemize}
		\item[$\bullet$] $V_0 = \Sg$ is the set of \belemp{simple} nodes (a node
		per attribute in $\Sg$),
		\item[$\bullet$] $V_1 = \{X | X \in \body{\I} \}$ is the set of 
		\belemp{compound} nodes (a node per distinct body in $\I$),
	\end{itemize}
	
	\item[-] $E = E_0 \cup E_1$ is the set of arcs where:
	\begin{itemize}
		\item[$\bullet$] $E_0$ is the set of \belemp{full} arcs. We have a full arc $(X, i)$ in	$E_0$ if $(X, i)$ is an hyperarc of $\I$,
		\item[$\bullet$] $E_1$ the set of \belemp{dotted} arcs. For each compound node $X$ of $V^1$, we have a dotted arc $(X, \ i)$ to every attributes $i$ of $X$,
	\end{itemize}
	
\end{itemize}
\noindent is the \belemp{Functionnal Dependency Graph} or \belemp{FD-Graph} 
associated to $\I$.
\end{definition}


Again, the definition may be quite confusing. $\B(\I)$ is still the set of premises of $\I$. Therefore, let us pause our explanations with some toy examples, presented in figure \ref{fig:FD-Graph-1}. 
From those graphs, we can give \textit{"handy"} way
to build an FD-graph out of some theory $\I$:
\begin{itemize}
	\item[-] every single attribute of $\Sg$ is a node, as every premise of $\I$,
	\item[-] for each $A \imp B$ of $\I$ we draw a \belemp{full} arc from the node
	$A$ to \textit{every attribute} of $B$,
	\item[-] for each compound node $A$, we draw a \belemp{dotted} arc from
	$A$ to \textit{all of its attribute}.
\end{itemize}
\noindent This is indeed what we formally defined previously. Furthermore, for
this algorithm, we consider a basis $\I$ over an attribute set $\Sg$, such that:
\begin{itemize}
	\item[-] there is no $A \imp B$, $A' \imp B'$ in $\I$ such that $A = A'$ when
	$B \neq B'$,
	\item[-] for all $A \imp B$ of $\I$, $A \cap B = \emptyset$
\end{itemize}
\noindent A theory in this form is said \belemp{reduced}.

\begin{figure}[ht]
	\input{Pictures/Graph.1-FD-Ex.tex}
\end{figure}

\vspace{1.2em}

The next definition is about describing a graph-theoretic way to combine 
implications to derive new ones. This notion is essential for all the following 
material and is called \belemp{FD-paths}.

\begin{definition}[\midemp{FD-Path}] Given an FD-Graph $G_{\I} = (V, E)$, an
\belemp{FD-Path} $\langle i, \ j \rangle$ is a minimal subgraph 
$\bar{G}_{\I} = (\bar{V}, \bar{E})$ of $G_{\I}$ such that $i, j \in \bar{V}$ 
and either $(i, j) \in \bar{E}$ or one of the following holds:
\begin{itemize}
	\item[-] $j$ is a simple node and there exists $k \in \bar{V}$ such that 
	$(k, j) \in \bar{E}$ and there exists a FD-Path $\langle i, \ k \rangle$ 
	included in $\bar{G}$, 
	 
	\item[-] $j = \bigcup_{k = 1}^n j_k$ is a compound node and there exists 
	FD-paths $\langle i, \ j_k \rangle$ included in $\bar{G}$, for all $k = 
	1, \ \dots, \ n$.
\end{itemize}
	
\end{definition}

\noindent Informally, an FD-path from a node $i$ to $j$ describes the 
implications we use to derive $i \imp j$. Intuitively, 
directed paths are FD-paths. But there is also one case in which
we can go \textit{"backward"} in the graph. For better understanding, see examples of figure \ref{fig:FD-Graph-3} based on the theory described in figure 
\ref{fig:FD-Graph-2}. To be more precise, $\I = \{ ab \imp f, \ af \imp g,
\ a \imp c, \ b \imp d, \ cd \imp e, \ c \imp h, \ cd \imp e \}$.

\begin{figure}[ht]
	\input{Pictures/Graph.1-FD-Ex-2.tex}
\end{figure}

There are either \belemp{dotted} or \belemp{full} paths. A path $\langle i, j
\rangle$ is dotted if all arcs leaving $i$ are dotted, it is full otherwise.

\begin{figure}[ht]
	\input{Pictures/Graph.1-FD-Ex-3.tex}
\end{figure}

\vspace{1.2em}

Having explained FD-Graphs, we will now move to explanations of the algorithm
developed by Ausiello and al. The procedure finds from a given basis 
its minimal representation in our terms (see alorithm \ref{alg:Ausiello}).

\vspace{1.2em}

\begin{algorithm}[H]
	\KwIn{$\I$ an implication basis}
	\KwOut{$\I_c$ a minimal cover for $\I$}
	
	\BlankLine
	\BlankLine
	
	Find the \belemp{FD-Graph} of $\I$ \;
	Remove \belemp{redundant} nodes \;
	Remove \belemp{superfluous} nodes \;
	Remove \belemp{redundant} arc \;
	Derive $\I_c$ from the new graph \;
	
	\caption{\textsc{AusielloMinimization} (Overview, 1983)}
	\label{alg:Ausiello}
\end{algorithm}

\vspace{1.2em}

As we will see in detailed explanations, those steps are equivalent to Maier's
procedure. In fact, the last part, removing redundant arcs, goes beyond the 
scope of our needs since it deserves to reduce sizes of premises and 
conclusion, not the number of implications. This has also been studied in 
Maier's work, but for out of scope reason we did not reviewed it. For the same
argument here, we will not focus on it either. To help the reader see where we 
are heading, one should keep in mind that the two other steps of removing 
redundant nodes and superfluous nodes will be equivalent to removing redundant 
implication and direct determination respectively. However, we must first
dive into the closure of an FD-Graph (parallel to closure of implications) to
be able to perform removal steps.

\subsubsection{Closure of an FD-Graph}

The closure is based on the following data structures:
\begin{itemize}
	\item[-] $V_0$: set of \belemp{simple} nodes,
	\item[-] $V_1$: set of \belemp{compound} nodes,
	\item[-] $D_i$ ($\forall i \in V$): nodes from \belemp{incoming dotted} arcs
	$\{j \in V \ | \ (j, \  i) \text{ is a dotted arc} \}$,
	\item[-] $L_{i}^0$ ($\forall i \in V$): nodes from \belemp{outgoing full} arcs
	$\{j \in V \ | \ (i, \  j) \text{ is a full arc} \}$,
	\item[-] $L_{i}^1$ ($\forall i \in V$): nodes from \belemp{outgoing dotted} 	
	arcs $\{j \in V \ | \ (i, \  j) \text{ is a dotted arc} \}$,
	\item[-] $L_{i}^{0+}, L_{i}^{1+}$ ($\forall i \in V$): the respective closures
	of $L_i^0, L_i^1$,
	\item[-] $q_m$ ($\forall m \in V^1$): counter of nodes in $m$ belonging to 
	$L_i^{0+} \cup L_i^{1+}$ for some $i \in V$.
\end{itemize}

\noindent To make understanding easier, we first give pseudo-code closer from
principle than algorithms. From a general point of view, to determine the 
closure of a FD-graph, we must compute the closure of all its nodes. The 
closure of a node is described by its full and dotted outgoing arcs. Because we
put a priority on dotted possibilities, they will be computed before. Principle
are given in algorithmic/pseudo-code form so that identification between steps
of procedures and ideas of principle are easier to see.

\vspace{1.2em}

First, we introduce the procedure \textsc{NodeClosure} which computes the 
closure of a node with respect to a type of arc. In other words, to compute the 
full closure of a node, we must first apply \textsc{NodeClosure} to its dotted 
arcs, then to its full arcs. The principle and algorithm for 
\textsc{Nodeclosure} are procedures \ref{alg:FD-NodeClosure-Principle}, 
\ref{alg:FD-NodeClosure}. 

\vspace{1.2em}

\begin{algorithm}
	\KwIn{
		$L_i$: set of nodes for which there exists dotted (resp. full) arcs 
		$(i, j)$}
	\KwOut{$L_i^+$: the dotted (resp. full) closure of $i$}
	
	\BlankLine
	\BlankLine
	
	Initialize a list of nodes to treat $S_i$ to $L_i$ \;
	\While{there is a node $j$ to treat in $S_i$}{
		remove $j$ from $S_i$ \;
		\If{$j$ is simple node}{
			\ForAll{compound node $m$ \belemp{except $i$}, $j$ appears in}{
				increase $q_m$ by 1 \;
				\If {$q_m$ = number of outgoing \belemp{dotted} arcs from $m$}{
					$m$ is reachable from $i$ by \aliemp{union} \;
					$m$ must be treated, add it to $S_i$ \;
				}
			}
			
		}
		
		add $j$ to the closure $L_i^+$ \;
		
		\ForAll{nodes $k$ such that there is an arc $(j, \ k)$}{
			\If{$k$ is not yet in the closure $L_i^+$ or in the \belemp{dotted}
				closure $L_i^{1+}$ of $i$}{
				$k$ is reachable from $i$ by \aliemp{transitivity} \;
				$k$ must be treated, add it to $S_i$ \;
			}
		}
		
		return $L_i^{+}$ \;
	}
	
	
	\caption{\textsc{NodeClosure} (Principle)}
	\label{alg:FD-NodeClosure-Principle}
\end{algorithm}

We would like to provide some observations on top of their description. Namely 
on the \aliemp{union} step and $q_m$ counters. Say $i \imp m$ where $m$ is a 
compound node is a valid implication in a FD-graph. Furthermore say $m = 
\bigcup_i m_i$ where $m_i$'s are simple nodes. The union step models the fact 
that if we have $i \imp m_i$ for all $m_i$ in $m$, then we must have $i \imp m$ 
also. The counter $q_m$ ensures that we indeed reached all $m_i$'s in $m$. 
Also, the algorithm has access to all the structures we described above (nodes, 
sets of arcs, and so forth). Parameters are thus lists we are going to modify 
somewhat. The \textsc{NodeClosure} algorithm runs in time $O(|\I|)$. The first 
nested loop runs in at most $O(|\Sg| \times |\B|) = O(|\I|)$ because 
$S_i$ contains at most $|\Sg|$ elements, and the block referring to the 
\textit{union} rule runs over compound nodes, that is bodies of $\I$. For the 
second loop (transitivity) note that we can at most consider all the edges of 
the FD-graph. In fact, the cost of transitivity operation for all $j$ is 
$O(\sum_{j = 1}^n |L_j^0 \cup L_j^1 |)$. But by definition, those sets are 
disjoints, and therefore we cannot treat more than $| E |$ arcs (the total 
number of arcs in $G$), that is $|\I|$. It is important to note that we can
reach this complexity only if we consider the closure of a node to be a matrix in which accessing an element is $O(1)$. Hence, if we consider the FD-Graph to be an adjacency list so as to ensure $|G_{\I}| = O(|\I|)$ as mentioned in \cite{ausiello_graph_1983, ausiello_minimal_1986}, the closure is represented 
through an adjacency matrix and adding element to the closure may be understood as \textit{"setting the value to 1 in the vector corresponding to the closure of i"}. The size of the graph representing the closure is then $O((|\B| + |\Sg|)^2)$ since every node appears in the closure.

\begin{algorithm}
	\KwIn{
		$L_i$: set of nodes for which there exists dotted (resp. full) arcs 
		$(i, j)$}
	\KwOut{$S_i^+$: the dotted (resp. full) closure of $i$}
	
	\BlankLine
	\BlankLine	
	
	$S_i := L_i$ \;
	$S_i^{+} := \emptyset$ \;
	
	\While{$S_i \neq \emptyset$}{
		select $j$ from $S_i$ \;
		\If{$j \in V^0$}{
			\ForAll{$m \in D_j - \{ i \}$}{
				$q_m := q_m + 1$ \;
				\If{$q_m = |L_m^1|$}{
					$S_i := S_i \cup \{ m \}$ \;
				}
				
			}
		}
		
		$S_i^+ := S_i^+ \cup \{ j \}$ \;
		
		\ForAll{$k \in L_j^0 \cup L_j^1$}{
			\If{$k \not\in S_i^+ \cup L_i^{1+} \cup \{ i\}$}{
				$S_i := S_i \cup \{ k \}$ \;
			}
		}
	}
	return $S_i^+$ \;
	\caption{\textsc{NodeClosure}}
	\label{alg:FD-NodeClosure}
\end{algorithm}

\vspace{1.2em}

Next, we present the principle and pseudo-code for the closure of an FD-graph
\ref{alg:FD-Closure-Principle}, \ref{alg:FD-Closure}. Mostly, the principle is 
the idea we described previously. There is just one observation to make about 
setting a counter $q_m$ to 1. This variable helps to see whether we can use 
union rule as we saw in procedure \textsc{NodeClosure}
(\ref{alg:FD-NodeClosure-Principle}, \ref{alg:FD-NodeClosure}). We initialize it
in case $i$ is indeed part of some compound node so that we do not omit to count
it when dealing with $S_i$ (because $S_i$ does not contain $i$). In terms of
complexity, we are running \textsc{NodeClosure} on all nodes having outgoing
edges, that is $|\B|$ nodes (if a compound node is represented, it must
have at least one outgoing full arc). Since \textsc{NodeClosure} operates in
$O(|\I|)$, the whole closure algorithm must run in $O(|\B| \times |\I|)$.

\begin{algorithm}
	\KwIn{$V_0$, $V_1$ and $\forall i \in V$ $D_i$, $L_i^0$, $L_i^1$}
	\KwOut{$\forall i \in V$ $L_i^{0+}$, $L_i^{1+}$}
	
	\BlankLine
	\BlankLine
	
	\ForAll{node $i$ in $V$ with outgoing arcs}{
		
		\If{$i$ is an attribute of a compound node $m$}{
			set a counter $q_m$ to $1$ \;	
		}
		
		initialize the closure of $i$ to $\emptyset$ \;
		
		\If{$i$ is a compound node}{
			determine \belemp{dotted} arcs in the closure of $i$ \;
		}
		
		determine \belemp{full} arcs in the closure of $i$ \;
		
	}
	
	\caption{\textsc{GraphClosure} (Principle)}
	\label{alg:FD-Closure-Principle}
\end{algorithm}

\begin{algorithm}
	\KwIn{$V_0$, $V_1$ and $\forall i \in V$ $D_i$, $L_i^0$, $L_i^1$}
	\KwOut{$\forall i \in V$ $L_i^{0+}$, $L_i^{1+}$}
	
	\BlankLine
	\BlankLine
	
	\ForAll{$i \in V$ with $L_i^0  \cup L_i^1 \neq \emptyset$}{
		
		\ForAll{$m \in V^1$}{
			\uIf{$m \in D_i$}{
				$q_m := 1$ \;
				
			} \Else {
				$q_m := 0$ \;
				
			}
		}
		
		$L_i^{1+} := \emptyset$ \;
		$L_i^{0+} := \emptyset$ \;
		
		\If{$i \in V^1$}{
			$L_i^{1+} := $ \textsc{NodeClosure}($L_i^{1}$) \;	
		}
		
		$L_i^{0+} := $ \textsc{NodeClosure}($L_i^{0} - L_i^{1+}$) \;
		
	} 
	
	\caption{\textsc{GraphClosure}}
	\label{alg:FD-Closure}
\end{algorithm}

\vspace{1.2em}

Now that algorithms for computing the closure of a FD-graph have been set, we
can move to the minimization part.

\subsubsection{Removing redundant nodes}

The first step is about removing redundant implications. In terms of FD-graphs, 
we remove redundant nodes. A compound node (only) $i$ is said \belemp{redundant} if for each full arc $(i, j)$ leaving $i$
there exists a dotted path $\fdpath{i}{j}$. We give an example in the figure
\ref{fig:FD-Graph-4}.

\begin{figure}[ht]
\input{Pictures/II/RedExample.tex}
\end{figure}

In this example, the associated basis is $\I = { ab \imp cd \, ; \, a \imp c \, ; \, b \imp d}$. Indeed, in this case, $ab \imp cd$ is 
redundant because $\I - {ab \imp cd} \models ab \imp cd$. So removing a redundant node is removing exactly one implication in $\I$ since $\I$ is reduced. It is quite direct to see equivalence between redundancy of a node and
redundancy of the implication having this node as a premise. We give a 
proposition anyway to make everything clear.

\begin{proposition} An implication $A \imp B$ is redundant in $\I$ if and only if $A$ is a redundant node in the FD-graph $G_{\I}$ associated to $\I$.
\end{proposition}

\begin{proof} Assume $A \imp B$ is redundant in $\I$. Then $A \imp B$ still holds in $\I^{-} := \I - {A \imp B}$. The FD-graph $G_{\I^{-}}$ associated to 
$\I^{-}$ is in fact $G_{\I}$ where we got rid of node $A$ (being compound) and
of all its outgoing arcs, dotted and full. If $\I^{-} \models A \imp B$ we must be able to find implications $X_i \imp Y_i$, $X_i \subseteq A$ such that $\bigcup_{i} X_i \imp B$. In particular we could add a compound node $\bigcup_i X_i$ to $G_{\I^{-}}$ with only dotted arcs to its attribute so that
we would have only a dotted FD-path from $\bigcup_i X_i$ to $B$, hence from $A$
to $B$.

\vspace{1.2em}

Suppose $A$ is redundant node in $G_{\I}$. It has full outgoing arcs, and is
compound hence corresponds to a premise $A$ of $\I$. Because it is redundant, we
can remove all of its full outgoing arcs without any loss of information. Say
$A \imp B$ is the implication represented by the node $A$ and its full outgoing
arcs. Removing all is reducing $A \imp B$ to $A \imp \emptyset$ that is an
implication we can remove. Because there is still FD-path from $A$ to $B$ in this set up we have $A \imp B$ still holding in $\I$ where $A \imp B$ has been
replaced by $A \imp \emptyset$ equivalent to $\I - \{ A \imp B \}$.

\end{proof}

To remove redundant nodes, Ausiello and al. observed that a redundant node will
only have dotted arcs in the closure of a FD-Graph. Hence its minimization procedure for some $\I$ and associated $G_{\I}$ suggests to determine the closure of $G_{\I}$ and then to remove all redundant nodes by checking it. However, let us consider the following case:

\[ \I = \{ ab \imp d, \ bc \imp d, a \imp c, c \imp a \} \]

with the associated FD-Graph presented in figure \ref{fig:FD-Red-CountEx}. On the right-hand side of the figure we gave the closure of the FD-graph. We can
observe that two nodes are redundant, namely $ab$ and $bc$. Indeed, we have:
\begin{itemize}
	\item[-] $\I - \{ab \imp d \} \models ab \imp d$
	\item[-] $\I - \{bc \imp d \} \models bc \imp d$
\end{itemize}
\noindent Nevertheless, this does not mean we can remove the two of them. Indeed
those two implications are somehow \textit{"mutually redundant"}: if we remove
one, the other is not redundant anymore. For instance, consider removing $ab \imp d$ from $\I$. Then, $\I = \{ bc \imp d, a \imp c, c \imp a \}$. In this
case $\I - \{bc \imp d \} \not\models bc \imp d$ because even though $c \imp a$,
the lack of $ac \imp d$ prevent redundancy of $bc \imp d$. Therefore, the idea
proposed by Ausiello as we understood it would result in $\I = \{ a \imp c, \ c \imp a \}$ after redundancy elimination, being not correct. In Maier's term, this would be equivalent to first marking all redundant implications and then
removing all of them.

\begin{figure}[ht]
	\input{Pictures/II/RedCountEx.tex}
\end{figure}

\vspace{1.2em}

FD-Graphs suffer from another drawback: representation of non-closed empty set.
To the best of our knowledge, this has not been discussed. While Maier's algorithm is flexible towards open empty set, FD-graphs may not, unless we missed informations. How to represent the basis $\I = \{ \emptyset \imp ab, a \imp b\}$? We thought of two possible choices:
\begin{itemize}
	\item[(i)] consider $\emptyset$ as a compound node without dotted arcs,
	hence like a simple node
	\item[(ii)] when $\emptyset$ is present, consider all simple nodes as 
	compound, and add a dotted arc for all of them into $\emptyset$
\end{itemize}
Let us investigate those two representations. As one may have noticed, $\I$
is redundant and we should only keep $\emptyset \imp ab$. The two ideas
are represented in figure \ref{fig:FD-empty}.

\begin{figure}[ht]
	\input{Pictures/II/RedEmptyEx.tex}	
\end{figure}

\vspace{1.2em}

In the first representation, we will not remove any implication, since there
is no dotted arcs anywhere. On the right-hand side, we would remove simple
nodes, namely $a$, since we have indeed dotted FD-path from $a$ to $b$.
However, we would reduce our attribute set and consequently we would keep only
$\emptyset \imp b$ in $\I$. Therefore, none of those ideas is satisfying. In fact, one possible solution is to add a new element $\sigma$ to $\Sg$ acting
as the empty set. Then, for all premises $A$ of $\I$ we add $\sigma$ as a new
element of $A$. This however brings a solution in pre-processing our implication
theory, it does not solve the problem in a graph theoretic manner.

\vspace{1.2em}

That said, we could argue on two points. First, maybe we should doubt of our
interpretation of the algorithm. Second, let us consider we understood it well,
then a possible correction would be to compute both dotted and full closure for
all compound nodes of $\I$ to see whether they are redundant or not. If it is the case, we update the graph and its closure (among nodes already computed!) by removing the node. This would be strictly the same operation as Maier's redundancy elimination, plus the cost in time and memory of bidirectional translation from basis to FD-Graph. On top of that, one should consider the cost of removing only one node and all of its outgoing arcs within a graph. While this could be done quite easily in an adjacency matrix representation, the adjacency lists choice (which seems to be the one assumed somehow in Ausiello's work) would be more time consuming, namely $O(\I)$. 

\vspace{1.2em}

We shall discuss it again later on, but as for now, it seemed to us that  Ausiello algorithm requiring bidirectional translation, maybe misleading operations, or equal processing as in the Maier case was not worth implementing.


\subsubsection{Removing superfluous nodes}

From now on, assume we are given a nonredundant FD-Graph. Let us remove so-called superfluous nodes. A node $i$ is \belemp{superfluous} if there is an equivalent node $j$ and a dotted path from $i$ to $j$. Two nodes $i, j$ are \belemp{equivalent} if there are FD-paths $\fdpath{i}{j}$ and $\fdpath{j}{i}$.
It comes at no surprise that nodes are equivalent exactly when they have the
same closure in $\I$. From a theoretical point of view, the minimization algorithm suggests the following operation:
\begin{itemize}
	\item[-] find a superfluous node $i$, and an equivalent node $j$ with a dotted
	path from $i$ to $j$
	\item[-] for each full arc $ik$, we add a full arc $jk$
	\item[-] then we remove the node $i$ and all of its outgoing arcs from the 
	graph
	\item[-] repeat until no more superfluous nodes exist
\end{itemize}
\noindent An example of this procedure is given in the figure
\ref{fig:FD-Graph-5}. In this example $\I = {ab \imp e \, ; \, a \imp c
	\, ; \, b \imp d \, ; \, cd \imp ab}$. The node $ab$ is superfluous. Since 
our
basis are reduced, note that removing a superfluous node is removing exactly 
one implication in $\I$. In this case, the resulting $\I$ will be

\[ \I = {a \imp c, b \imp d, cd \imp abe} \] 


\begin{figure}[ht]
\input{Pictures/II/SupExample.tex}
\end{figure}


\noindent Now we may rewrite this operation in our terms. Let $A \imp B$ and 
for instance $C \imp D$ be part of $\I$ to be general. Then $A$ is superfluous
body if

\[ \I \models A \imp C, C \imp A \land \exists X \subset A \; s.t \;
\I \models X \imp C \]

\noindent In this case, we apply the following operations
\begin{itemize}
	\item[-] $C \imp D$ becomes $C \imp (D \cup B)$
	\item[-] we remove $A \imp B$ from $\I$
\end{itemize}

\noindent In order to prove correctness of this operation, we will show that
a node $A$ is superfluous exactly when $A$ directly determines some equivalent
$B$ in Maier's terms. Because in both case we are doing same replacement/deletion operation, if two statements are equivalent then the Ausiello algorithm is correct as Maier's one is.

\begin{proposition}\label{prop:maier.equiv_sup_sub} Let $A \imp C$ be the implication of $\I$ with $A$ as body. The following properties are equivalent:
\begin{itemize}
	\item[(i)] A node $A$ in a FD-graph is superfluous with respect to $B$,
	\item[(ii)] $A \equiv B$ and $\I - \{A \imp C \} \models A \imp B$.
\end{itemize}
\end{proposition}

\begin{proof} (i) $\imp$ (ii). If $A$ is superfluous, we have a dotted FD-Path
$\langle A, \ B \rangle$. Since it is dotted, let us remove this node $A$ 
and its outgoing arcs. Actually, none of the nodes pointed by dotted arcs of $A$
have been removed, thus we can still find the nodes $a_i$ (attributes of 
$a$) used in the dotted path $\langle A, \ B \rangle$ such that $\bigcup_i a_i 
\models B$. Because $\bigcup_i a_i \subseteq A$, we end up with $\I - \{A 
\imp C \} \models A \imp B$.

\vspace{1.2em}

(ii) $\imp$ (i). In the FD-Graph associated to $\I - \{A \imp C \}$, the 
node $A$ is not present. But still, $A \imp B$ holds. This means that we must be
able to find a list of proper subsets $A_i$ of $A$ (possibly single 
attributes) such that $\bigcup_{i} A_i \models B$. Adding $A \imp C$ will add the node $A$ and in particular dotted arcs from $A$ to each attributes of $\bigcup_{i} A_i \subseteq A$. Thus, we will have a dotted path from $A$ to $\bigcup_{i} A_i$ and consequently, to $B$. $A$ is indeed superfluous. Because $B$ is equivalent to $A$ by assumption, this property is preserved when adding a node.
	
\end{proof}


\begin{proposition} \label{prop:maier.equiv_ssup_dd}
the following statements are equivalent, for $A, B$ bodies of $\I$:
\begin{itemize}
	\item[(i)] $A \ddv B$ and $B \equiv A$,
	\item[(ii)] the node $A$ is superfluous with respect to $B$, and there 
	exists 
	a dotted FD-path from $A$ to $B$ not using any outgoing full arcs of
	nodes equivalent to $A$.
\end{itemize}
	
\end{proposition}


\begin{proof} (i) $\imp$ (ii). Using proposition
\ref{prop:maier.equiv_sup_sub} and the equivalence between $A \ddv B$
and $\I - E_{\I}(A) \models A \imp B$ (from Maier's terminology), showing that there is a direct determination starting from $A$ implies $A$ is a superfluous node in the FD-Graph is straightforward. If $\I - E_{\I}(A) \subseteq \I - \{A \imp C \} \models A \imp B$ so does $\I - \{A \imp C \}$. This holds in particular if $B \subseteq A$. Moreover, notice that using an outgoing full arc from a node $D$ equivalent to $A$ is exactly using an implication with left hand side equivalent to $A$. Therefore, if there is not dotted FD-path from $A$ to $B$ not using those arcs, we would contradict direct determination.

\vspace{1.2em}

(ii) $\imp$ (i). Suppose $A$ is superfluous and there exists a dotted FD-path from $A$ to $B$ not using any outgoing full arcs from nodes equivalent to $A$. Those full arcs represent exactly the implications contained in $E_{\I}(A)$. Since we don't use them, the path still holds in $\I - E_{\I}(A)$ (we would remove compound nodes without outgoing full arcs of course, but this would only make the path stops to attributes instead of compound node). Having this path in $\I - E_{\I}(A)$ means that $\I - E_{\I}(A) \models A \imp B$.
	
\end{proof}

With propositions 3, 6 and above remarks on operations done in FD-graphs, we
can conclude that \textsc{AusielloMinimization} performs from a graph-theoretic point of view the operations made by \textsc{MaierMinimization} in implications
framework. Hence both are correct and the resulting FD-graph represents a minimal basis in our terms. 

\vspace{1.2em}

In fact, the manipulations we described previously to remove superfluous nodes do not represent the true algorithm given by Ausiello and al for this purpose.
To be more precise, the papers \cite{ausiello_graph_1983, ausiello_minimal_1986} suggests to perform the computations of algorithm \ref{alg:Ausiello-Superfluous}.

\vspace{1.2em}

\begin{algorithm}
	\KwIn{$G_{\I}$: the FD-Graph of some non-redundant basis $\I$}
	\KwOut{$G_{\I}$: the associated minimum FD-Graph}
	
	\BlankLine
	\BlankLine
	
	\ForAll{$i \in V^1$}{
		find an equivalent node $j$ \;
		\If{$j$ exists}{
			$L_j^{0+} := L_j^{0+} \cup (L_i^{0+} \cap L_j^{1+})$ \;
			$L_j^{1+} := L_j^{1+} - (L_i^{0+} \cap L_j^{1+})$ \;
		}
		
		remove $i$ from the closure \;
		add $(i, j)$ to a list $L$ \; 
	}
	
	remove superfluous nodes \;
	move arcs to their final destination \;	
	
	\caption{\textsc{SuperfluousnessElimination}}
	\label{alg:Ausiello-Superfluous}
\end{algorithm}

For all compound nodes $i$, we check in its dotted closure $L_{i}^{1+}$ whether 
we can find an equivalent node $j$. This can be done in $O(|\B| + |\Sg|)$. If we found such $j$, then $i$ is superfluous and we move to the full closure of $j$ all nodes $k$ such that $(i, k)$ is full and $(j, k)$ dotted. This corresponds to the union operation we depicted earlier. When we remove an implication with a superfluous premise, we append to the consequence of the \textit{"target"} implication, the consequence of the superfluous one. This is also $O(|\B| + |\Sg|)$. Then, removing $i$ from the closure does also require a run over all nodes of the closure: $O((|\B| + |\Sg|))$. Therefore, the first part of this algorithm is $O(|\B|(|\B| + |\Sg|))$. The second part is about removing nodes and moving arcs. If run over $L$ from the end, we may be able to find the final destination of full arcs with only one run. Then, we can run over the graph, and for each arc, whether it points out to a superfluous node and we remove it, the node itself is redundant and the arc is full and we move it to its destination, or we do nothing. All these operations are $O(1)$ and the overhaul operation requires to visit all the edges, resulting in $O(|\I|)$ complexity. Therefore, the cost of removing superfluous node provided we have the closure of the FD-Graph we work on seems to be $O(|\B|(|\B| + |\Sg|))$, in
accordance with \cite{ausiello_graph_1983, ausiello_minimal_1986} being better than the removal of direct determination in Maier's algorithm.

\vspace{1.2em}

What we can conclude of the procedure issued by Ausiello is that it performs
the same work as Maier's algorithm but using advantage of the structure of
FD-Graph and its distinction between dotted/full arcs to enhance removing superfluous nodes / direct determination. However, as aforementioned, we did not implement this graph-theoretic approach. The main reason is some difficulties in clearly understanding the ideas of Ausiello, notably for the
redundancy step, empty set representation, or the data structures one should use to match theoretical complexity. Furthermore, as we exposed, the operations performed are somehow very similar to the work done by Maier even though FD-Graphs use their dotted-full arcs to slightly improve computations. Nevertheless, the whole algorithm may still require as much theoretical time as Maier since most of the properties of \textsc{AusielloMinimization} use the closure of an FD-Graph needing $O(|\B||\Sg|)$ to be computed. On top of that, to get from a basis to a graph, we may need (linear) translation time, as much as a large amount of memory (especially for the closure of a graph). Eventually, this algorithm could be considered as a weak point of our study since for all this reason taking time to investigate, we chose to focus on other algorithms. Plus, because of the similarities with \textsc{MaierMinimization}, limiting ourselves to the Maier's algorithm permits to have a first insight on the efficiency of core operations. Nevertheless, as all this section shows, we still provided some theoretical comparison to exhibit the heart of \textsc{AusielloMinimization} being a translation from a framework to another. Hence, with those explanations we leave the door open to a future implementation. The interest of truly implementing this graph algorithm will depend on the efficiency of \textsc{MaierMinimization} in practice.

\vspace{1.2em}

In this section we studied \textsc{MaierMinimization} and its graph-theoretic version \textsc{AusielloMinimization}, two algorithms designed for functional 
dependencies minimization. It remains now to focus on algorithms coming from
logical and query learning communities.


\section{Propositional logic based approach}

The last domain we will explore is boolean logic. Actually, boolean logic is just a background as previous domains and we will focus to the expression of the algorithms within our framework. The main studies we will focus on can be found in \cite{boros_strong_2017, berczi_directed_2017, angluin_learning_1992, arias_canonical_2009}.  Anyway, as an informal introduction, let us see the 
parallel between logic and our set-theoretic representation. This introduction is absolutely not mandatory for the understanding of next material, but it can be interesting for the reader with some knowledge in propositional logic.


\subsection{From sets to boolean logic}

We will need the usual logical notations $\lnot, \lor, \land, \imp$ (negation, disjunction, conjunction, implication for the recall) we defined in the first chapter. The reader can read \cite{cori_mathematical_2000} to have in-depth explanations. For a moment, let $\Sg$ be a set of \belemp{boolean variables}: variables being either \textit{true} ($\top$, $1$) or \textit{false} ($\bot$, $0$). From this
point of view, an implication is in fact a so-called \belemp{clause}. A clause
is a disjunction of literals. For instance:
\begin{itemize}
	\item[-] $(x_1 \lor x_2 \lor \lnot x_3)$ is a clause,
	\item[-] $((x_1 \lor x_2) \land x_3)$ is not (in our context).
\end{itemize}
\noindent And in particular, a clause in which there is \belemp{at most} one
positive literal is called a \belemp{Horn clause}. When there is exactly one
positive variable, it is a \belemp{definite} such clause. In our context, we only have definite Horn clauses:
\begin{itemize}
	\item[-] $(x_1 \lor \lnot x_2 \lor \lnot x_3)$ is a definite Horn clause,
	\item[-] $(\lnot x_1 \lor \lnot x_2)$ is a Horn clause not definite.
\end{itemize}
\noindent Because $x_1 \imp x_2$ is logically equivalent to $x_2 \lor \lnot x_1$
and thanks to de Morgan's law on negation (the negation of a disjunction is the conjunction of the negations and vice-versa), we may rewrite a clause $(x_1 \lor \lnot x_2 \lor \lnot x_3)$ as an implication $(x_2 \land x_3) \imp x_1$. 
Now, we can associate several clauses in a conjunction:

\[ ((x_2 \land x_3) \imp x_1) \land ((x_2 \land x_3 \land x_4) \imp x_5) 
\land (x_2 \land x_3) \imp x_4) \]

\noindent Such a conjunction is called a \belemp{Horn formula}. When written
under its clause form, it is also a formula under \belemp{Conjunctive Normal Form} (CNF): a conjunction of disjunctive clauses. To get back up on our feet
with sets, we only lack one step. Given a subset $A$ of an attribute set $\Sg$, we can use the \belemp{characteristic vector} $\psi_A$ of $A$. It is a binary vector of size $|\Sg|$ such that the $i$-th bit of $\psi_A$ is a boolean variable saying whether the $i$-th element of $\Sg$ is in $A$ or not. This supposes $\Sg$ to be totally ordered, which is never an issue. For instance,
let $\Sg = \{ a, b, c, d, e\}$ be an attribute set (not boolean variables),
and $A = abd$, then $\psi_A = 11010$. Now, we can associate propositional variables $x_a, x_b, x_c, x_d, x_e$ to $a, b, c, d, e$ respectively. Finally,
let us take an implication say $ab \imp de$. This attribute implication can be
transformed into logical one:

	\[ x_a \land x_b \imp x_d \land x_e = (x_a \land x_b \imp x_d)
	\land (x_a \land x_b \imp x_e) \]

\noindent We use conjunction because $ab$ contains $a$ \textit{and} $b$. This
very short paragraph concludes our quick parallel with logical background, we will now go back to our usual framework to study algorithms. With this procedure explained, it remains know to focus on query learning and Angluin algorithm.

\subsection{Iterative building of the canonical basis}

Here, we will study an algorithm proposed by Berczi and al. in 
\cite{berczi_directed_2017} following a paper by Boros and al. 
(\cite{boros_strong_2017}). Readers having a glance at the paper previously 
cited will see different notations and framework between our study and the one 
performed by the authors. This is because they use a graph-theoretic ground 
equivalent to ours to express logical process, but as we previously said, in order to stay in a somehow coherent set up all along this report, we will discuss in terms of implications and so forth.

\vspace{1.2em}

The main idea of the algorithm we should keep in mind, is to build iteratively
the DG basis. To describe briefly the procedure in words, having an initial 
basis $\I$: we initialize $\I_c = \emptyset$ and then at each step of the 
algorithm we add a new implication $A \imp \I(A)$ in $\I_c$ such that $A$ is 
pseudo-closed in $\I$. By construction then, we will terminate and end up with 
the DQ basis. Now that the process is defined, let us expose the procedure
and discuss it (see algorithm \ref{alg:Berczi-min})

\begin{algorithm}
	\KwIn{$\I$: an implication theory}
	\KwOut{$\I_c$: the DG-basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	
	\While{$\exists B \in \B(\I)$ s.t $\I_c(B) \neq \I(B)$}{
		$P := \min\{\I_c(B), \ B \in \B(\I) \text{ and } \I_c(B) \neq \I(B)\}$ 
		\;
		$\I_c := \I_c \cup \{P \imp \I(P) \}$ \;
		
	}
	
	return $\I_c$ \;
	
	\caption{\textsc{BercziMinimization}}
	\label{alg:Berczi-min}
\end{algorithm}

\noindent In \cite{berczi_directed_2017, boros_strong_2017}, pseudo-closure
is not explicitly considered. Instead, an implication of the form $P \imp \I(P)$ where $P$ is pseudo-closed, is called \belemp{left-right-saturated}. In fact, left-saturation stands for quasi-closure somehow. To stay close to our definition of the canonical basis, we provide an proposition for the correctness of this algorithm, based on pseudo-closed sets:

\begin{proposition} In algorithm \ref{alg:Berczi-min}, we add an implication
	$P \imp \I(P)$ only if $P$ is pseudo-closed.
	
\end{proposition}

\begin{proof} Let us prove this proposition by induction. 
	
	\paragraph{Initial Case} The initial case is the first implication we add
	to $\I_c$. Because $\I_c$ is empty, for all $B \in \B(\I)$, $\I_c(B) = B$.
	Thus, we add to $I_c$ an implication $B \imp \I(B)$ where $B$ is minimal
	inclusion-wise among bodies of $\I$. Recalling our definition of 
	pseudo-closure, $B$ is compelled to be pseudo-closed then. Note that in 
	fact,
	this argument will hold for all minimal bodies inclusion-wise. Hence, the 
	proposition is true for the initial case.
	
	\vspace{0.5em}
	
	\paragraph{Induction} Suppose we added only implications with pseudo-closed 
	sets as premises in $\I_c$. We will show that in the next implication $P 
	\imp 
	\I(P)$ we add, $P$ is pseudo-closed. Take $P$ as mentioned in the algorithm.
	First observe that taking the minimal non-closed sets of $\I$ closed in 
	$\I_c$
	generated by bodies of $\I$ is sufficient to have the minimal such sets in 
	general. Indeed, let $X$ be a closed set of $\I_c$ not closed in $\I$. Then,
	because bodies are minimal in non-closed sets of $\I$, there must exist 
	implications $\alpha \imp \beta$ in $\I$ such that $\alpha \subseteq X$.
	In particular, we must have an implication $\alpha_i \imp \beta_i$ such 
	that 
	$\alpha_i \subseteq X$ and $\beta_i \nsubseteq X$, because $X$ is not 
	closed. 
	Now by construction of the algorithm, we have the following for all 
	implications $A \imp B$ of $\I$: either $\I_c(A) \imp \I(A)$ belongs to 
	$\I_c$, 
	either it does not (here $\I_c(A)$ is the closure of $A$ before adding 
	$\I_c(A) \imp \I(A)$ to $\I_c$). Because $\beta_i \nsubseteq X = 
	\I_c(X)$ we can conclude that $\I_c(\alpha_i) \imp \I(\alpha_i) \not\in 
	\I_c$.
	Thus $X$ will not be minimal $\I_c$-closed $\I$-non-closed unless it is the 
	closure of some body of $\I$.
	Because we are sure to take a minimal $\I_c$-closed $\I$-non-closed set at 
	each step, we are sure to have all possible pseudo-closed sets $P_i \subset 
	P$
	when considering $P$. Furthermore, since we take $P$ to be the minimal close
	set of $\I_c$, $\I(P_i) \subseteq P$ for all $P_i$. Hence $P$ is indeed 
	pseudo-closed, which confirms the induction hypothesis and the property in
	general.
	
\end{proof}

This statement saying that if we add an implication, then its premise is 
pseudo-closed is enough to justify termination of the algorithm on DG
basis. The outer while loop will be executed at most $|\B|$ times since at
each step, we take out another body of $\I$. Computing and finding the next
pseudo-closed set in this case is done in $O(|\B||\I|)$ operations (an 
execution of \textsc{LinClosure} for each implication of $\I$), thus resulting
in an $O(|\B|^2|\I|)$ asymptotic complexity for the whole algorithm. Even though
simple in its form, it is much more time consuming than previous studied 
algorithms. Let us study an example.

\paragraph{Example} To be coherent, let us take again our perpetual example:
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item[-] $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, abcd $\imp$ ef} \} 
\end{itemize}
Let us illustrate the algorithm through a graphical trace (see figure 
\ref{fig:berczi-trace}). In this figure, we represented the 4 steps of 
\textsc{BercziMinimization} over $\I$ as follows: on the left-hand side of each 
step, one can find the closures of premises of $\I$ under $\I_c$, denoted 
$\I_c(\B(\I))$, ordered by inclusion ($\subseteq$). On the right-hand side, we 
have the closures of premises of $\I$ under $\I$, that is $\I(\B(\I))$, again 
ordered by inclusion.

\vspace{1.2em}

In fact, Berczi procedure is a matter of comparing those two orderings. At each
step, we should consider all premises $B$ of $\I$ such that $\I(B)$ is not an
element of $(\I_c(\B(\I)), \subseteq)$. Among those premises, we take one with
a minimal $\I_c$-closure. Then, adding $\I_c(B) \imp \I(B)$ to $\I_c$ ensures
in next steps, we will not have to consider elements of $\downarrow \I(B)$ in
$(\I(\B(\I)), \subseteq)$. In details (a point refers to a step in the figure):
\begin{itemize}
	\item[(a)] $\I_c = \emptyset$, so for all premises $B$ of $\I$, $\I_c(B) = 
	B$.	Hence we take $c$ as a premise with minimal $\I_c$-closure, and append 
	$c \imp ac$ to $\I_c$.
	\item[(b)] $\I_c = \{ c \imp ac \}$. $d$ is a premise of $\I$ being closed 
	in $\I_c$, hence minimal. Consequently, we add $d \imp bd$ to $\I_c$.
	\item[(c)] $\I_c = \{ c \imp ac, d \imp bd \}$, the closures of $c$ and 
	$d$, are the same in $\I_c$ and in $\I$. It remains then $ab$, $cd$ and 
	$abcd$. In $\I_c$, we have:
	\begin{itemize}
		\item $\I_c(ab) = ab$,
		\item $\I_c(cd) = abcd$,
		\item $\I_c(abcd) = abcd$,
	\end{itemize}
	thus the minimal one is $\I_c(ab) = ab$ and we add $ab \imp abcdef$ to 
	$\I_c$,
	\item[(d)] $\I_c = \{ c \imp ac, d \imp bd, ab \imp abcdef 
	\}$, for all $B \in \B(\I)$, $\I_c(B) = \I(B)$, the two orderings
	are identical (or \belemp{isomorphic}), $\I_c$ is equivalent to $\I$ and
	canonical whence minimal.
\end{itemize}


\begin{figure}[ht]
	\input{Pictures/II-Berczi-Trace.tex}
\end{figure}

As we said, this algorithm is much less 
efficient in theory than \textsc{MinCover} or even than \textsc{MaierMinimization}. The problem may come from the need to re-compute the closure of bodies in $\I$ under $\I_c$ at each step to find a possible minimum (see chapter 3).

\subsection{Angluin algorithm and AFP: Query Learning based approach}

Here, the method for building a minimal base is slightly different. We use 
so-called \belemp{query learning}. The idea is we formulate \belemp{queries}
to an \belemp{oracle} knowing the basis we are trying to learn. The oracle 
is assumed to provide an answer to our query in constant time. Depending on 
the query, it might also provide informations on the object we are looking for.
For the Angluin algorithm, we need 2 types of queries. Say we want to learn
a basis $\I$ over $\Sg$:
\begin{enumerate}
	\item \belemp{membership} query: is $M \subseteq \Sg$ a model of $\I$? The
	oracle may answer \textit{"yes"}, or \textit{"no"}.
	\item \belemp{equivalence} query: is a basis $\I'$ equivalent to $\I$? Again
	the answers are \textit{"yes"}, or \textit{"no"}. In the second case, the oracle provides a	\belemp{counterexample} either positive or negative:
	\begin{itemize}
		\item[(i)] \belemp{positive}: a model $M$ of $\I$ which is not a
		model of $\I'$,
		\item[(ii)] \belemp{negative}: a non-model $M$ of $\I$ being a model
		of $\I'$. 
	\end{itemize}
\end{enumerate}
\noindent To clarify, the terms negative/positive are related to the base $\I$
we want to learn. \textsc{AngluinAlgorithm} (\ref{alg:Angluin}) is the algorithm presented by Angluin, Frazer and Pitts in \cite{angluin_learning_1992} as \textsc{Horn1}. 
Initially, it is based on learning logical representation of implication theories: Horn clauses. This learning algorithm has been shown first to terminate on a minimal representation of the basis we want to learn (\cite{angluin_learning_1992}) and more than that, to end up on the canonical
basis by Arias and al. \cite{arias_canonical_2009}. It uses two operations allowing to reduce implications:
\begin{itemize}
	\item[-] $\textit{refine}(A \imp B, M)$: produces $M \imp \Sg$ if $B = \Sg$, $M \imp B \cup A - M$ otherwise,
	\item[-] $\textit{reduce}(A \imp B, M)$: produces $A \imp M - A$ if $B = \Sg$, $A \imp B \cap M$ otherwise.
\end{itemize}
\noindent The main idea is to ask the oracle whether the basis we are building ($\I_c$) is equivalent to $\I$ until it answers \textit{"yes"}. If it says \textit{"no"} then it provides an example on which $\I_c$ and $\I$ differ. If
the example is a model of $\I$, then we track implications in $\I_c$ falsified
by this example and correct them. If the example however is not a model of $\I$
we look for the first possible smaller example out of the one we got. The main
idea is to say that if we correct a smaller example, we are likely to correct
a larger one too. If we do not find any smaller example to correct, we add 
an implication in $\I_c$ addressing the problem. In practice, this procedure is likely to be somehow random because of the oracle. To get rid of non-determinism, one can derive from \textsc{AngluinAlgorithm} the algorithm \textsc{AFPMinimization} (\ref{alg:AFP}).

\vspace{1.2em}

\begin{algorithm}[H]
	\KwIn{$\I$ a theory to learn and an \textit{Oracle} with \textit{membership, equivalence} queries}
	\KwOut{$\I_c$ the canonical representation of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c = \emptyset$ \;
	\While{not equivalence($\I_c$)}{
		$M$ is the counterexample \;
		\If{$M$ is positive}{
			\ForEach{$A \imp B \in \I_c$ such that $M \not\models A \imp B$}{
				replace $A \imp B$ by $reduce(A \imp B, M)$ \;	
			}
			
		} \Else {
			\ForEach{$A \imp B \in \I_c$ such that $A \cap M \subset A$}{
				membership($M \cap A$) \;	
			}
			
			\If{Oracle replied "no" for at least one $A \imp B$}{
				Take the first such $A \imp B$ in $\I_c$ \;	
				replace $A \imp B$ by $refine(A \imp B, A \cap M)$ \;
				
			} \Else {
				add $M \imp \Sg$ to $\I_c$ \;
				
			}
			
		}
		
	}
	return $\I_c$ \;
	
	\caption{\textsc{AngluinAlgorithm}}
	\label{alg:Angluin}
\end{algorithm}



\vspace{1.2em}


\begin{algorithm}
	\KwIn{some theory $\I$ over $\Sg$}
	\KwOut{$\I_c$ the Duquenne-Guigues basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	Stack $\mathcal{S}$ \;
	
	\BlankLine
	
	\ForAll{$A \imp B \in \I$}{
		$\mathcal{S} := \left[ A \right]$ \;
		\Repeat{$\mathcal{S} = \emptyset$}{
			$X := \I_c(\text{pop}(\mathcal{S}))$ \;
			
			
			\If{$X \neq \I(X)$}{
				$found := \bot$ \;
				
				\ForAll{$\alpha \imp \beta \in \I_c$}{
					$C := \alpha \cap X$ \;
					\If{$C \neq \alpha$}{
						$D := \I(C)$ \;
						
						\If{$C \neq D$}{
							$found := \top$ \;
							change $\alpha \imp \beta$ by $C \imp D$ in $\I_c$\;
							push($X \cup D$, $\mathcal{S}$) \;
							
							\If{$\beta \neq D$}{
								push($\alpha$, $\mathcal{S}$)\;
							}
							\textbf{exit for}
						}
					}
				}
			
				\BlankLine
			
				\If{$found = \bot$}{
					$\I_c := \I_c \cup \{X \imp \I(X)\}$ \;	
				}
			}
		}

	}

	return $\I_c$ \;
	
	
	\caption{\textsc{AFPMinimization}}
	\label{alg:AFP}
\end{algorithm}

\noindent In this function, questions to and answers from the oracle are replaced by a stack and closure operations. Indeed, membership queries can
be done by computing closures under $\I$, $\I_c$. Regarding the stopping 
criterion equivalence query, observe that premises of $\I$ are a sufficient set of negative counter-example to find $\I_c$: when a premise is no more a negative counter-example, it means that it is neither closed in $\I$ nor in 
$\I_c$. Furthermore, since implications of $\I_c$ are $\I$-right-closed, every
premise $A$ of $\I$ has the same closure in $\I$ and $\I_c$. Hence, when there
is no premise left being negative counter-example, $I \equiv \I_c$. Note that this approach does not require positive counter-examples. The nested for all loop over $\I_c$ is similar to the operations we make in case of negative counter-example in \textsc{AngluinAlgorithm}: whenever we find a smaller counter-example out of $X$ and an implication of $\I_c$ we refine this implication and move to the next possible example. If we do not find any implication in $\I_c$ to refine (i.e the oracle replied "no" for all implications in $\I_c$ in other Angluin's algorithm), then we add $X \imp \I(X)$ to $\I_c$. 

\vspace{1.2em}

Up to know, we are not sure of the correctness of the algorithm, since we do
not know whether an implication can be refined twice, hence if the stack can
grow more than twice the number of implications in $\I$ (asymptotically). Still, if an implication is refined more than one time, the elements stacked shrink in size. Hence we are likely to have a loop requiring in the worst case $O(|\I|)$ to terminate. Because we are doubtful about this assumption, we do not integrate it in complexity estimation. This leads to an approximation of the overhaul time complexity. Indeed, even though we do not know the number of time one could iterate over the repeat loop, we still have nested
iterations over implications of $\I$ and $\I_c$. Furthermore, because the run
over implications of $\I_c$ contains computations of elements in $\I$, the 
overhaul algorithm may be cubic in the number of premises in $\I$ (that is $O(|\B|^2|\I|)$). Therefore, this would be the worst time complexity.

\vspace{1.2em}

Even though we do not have a proof for this algorithm, we can leave for further
research some ideas helping in approaching the correctness of \textsc{AFP}. Nevertheless, because they are somehow \textit{"abstract"} ideas, they may suffer from lack of precisions, or omit details:
\begin{itemize}
	\item[-] a set $A$ in $\mathcal{S}$ is always comprised between a premise
	of $\I$ and its closure $\I(A)$ in terms of inclusion,
	\item[-] the principle of \textsc{AFPMinimization} has echoes in \textsc{BercziMinimization}. In fact, one could rewrite the procedure 
	given by Berczi as follows: while we can find a premise in $\I$ generating a $\I_c$-closed $\I$-non-closed set (that is, a negative counter-example),
	take the minimal such example and add its full implication to $\I_c$. The minimality argument permits to limit the algorithm to appending full implications. Hence here is a possible idea for further work: it is likely that because in \textsc{AFP} we do not use minimality, we may need to update several implications. This processing relies on the main loop iterating over all implications and not being an equivalence query (as opposed to \textsc{BercziMinimization} and \textsc{AngluinAlgorithm}).
	\item[-] one should be interested in describing an invariant for the
	procedure. For instance, we may observe that at each step the remaining
	set of premises of $\I$ are a sufficient set of counter-examples for $\I_c$ with respect to $\I$. More than this, this may also be the case for $\mathcal{S} \cup \B_i$ where $B_i$ denotes the remaining set of premises of $\I$ at step $i$. If a premise is a counter example, it is likely to
	be pushed in $\mathcal{S}$ so that $mathcal{S} \cup \B_i$ does not change
	overhaul. Knowing that an element of the stack cannot be larger than its closure, and that if an implication can be refine more than one time, then 
	in the worst case all elements of the stack may shrink in size because $|\Sg|$ is finite. Therefore, termination of the repeat-until loop should be guaranteed, allowing for reducing the size of sufficient set of premises by 1 at each step of the main loop.
\end{itemize}
\noindent In spite of the lack of proof or counter-example, we can still provide a trace to see how does \textsc{AFP} work for real.

\paragraph{Example} Let us observe a trace with our toy example:
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item[-] $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, abcd $\imp$ ef} \} 
\end{itemize}
\noindent We will proceed by implications in $\I$:
\begin{enumerate}
	\item $ab \imp cde$, $\I_c = \emptyset$. We want to build a negative example with $ab$. The smallest closed set of $\I_c$ containing $ab$ is
	$\I_c(ab) = ab$. $ab$ is not closed in $\I$, hence it is itself a counter-example. Because $\I_c$ is empty, we can only add $ab \imp abcdef$ to $\I_c$.
	
	\item $cd \imp f$, $\I_c = \{ ab \imp abcdef \}$. Again, $cd$ is a negative
	counter example to $\I$: it is a model of $\I_c$, but not of $\I$. Furthermore, the only smaller example we build is $ab \cap cd = \emptyset$ 
	not being a negative one. Hence, we add $cd \imp abcdef$ to $\I_c$.
	
	\item $c \imp a$, $\I_c = \{ ab \imp abcdef, cd \imp abcdef \}$. $c$ is a
	negative example: $\I_c(c) = c \neq \I(c)$. For all implications of $\I_c$,
	we try to compute a new example:
	\begin{itemize}
		\item[-] $ab \cap c = \emptyset$, which is not a negative counter example,
		\item[-] $cd \cap c = c$. It is a negative example.
	\end{itemize}
	For $c$, we replace $cd \imp abcdef$ by $c \imp ca$ to correct $\I_c$. We push $ca$ in the stack since it may generate an counter-example. Furthermore, because we replaced $cd \imp abcdef$ by $c \imp ca$ and $abcdef \neq ca$, the closure of $cd$ under $\I_c$ is no more its closure under $\I$, it may also produce a counter-example, we push it into the stack.
	
	The next attribute set in the stack is $cd$. The corresponding example is
	$\I_c(cd) = acd$ not closed in $\I$:
	\begin{itemize}
		\item[-]$acd \cap ab = a$: $a$ is closed both under $\I$ and $\I_c$,
		\item[-]$acd \cap c = c$: $c$ has the same closure under $\I$ and $\I_c$.
	\end{itemize}
	we found no counter example, we add $acd \imp abcdef$ to $\I_c$.
	
	The last element of the stack is $ca$. It is closed under $\I$ hence it cannot be a counter example.
	
	\item $d \imp b$, $\I_c = \{ ab \imp abcdef, c \imp ca, acd \imp abcdef \}$.
	$\I_c(d) = d \neq \I(d)$ is a negative counter example:
	\begin{itemize}
		\item[-] $ab \cap d = \emptyset$
		\item[-] $c \cap d = \emptyset$
		\item[-] $acd \cap d = d$, being a negative counter example
	\end{itemize}
	For $d$, we replace $acd \imp abcdef$ by $d \imp bd$. Then we push it then stack $bd$ and because $bd \neq abcdef$, we also push $acd$.
	
	The next element in the stack is $acd$ for which the closure under $\I_c$
	is the same as the closure under $\I$: $abcdef$. It is not a counter example.
	
	The last element before the stack goes empty is $bd$ being a model of $\I$.
	
	\item $abcd \imp ef$, $\I_c = \{ ab \imp abcdef, c \imp ca, d \imp bd \}$.
	$\I_c(abcd) = abcdef = \I(abcd)$. It is not a counter example.
\end{enumerate}
\noindent The resulting basis $\I_c =  \{ ab \imp abcdef, c \imp ca, d \imp bd \}$ is indeed the DG basis of $\I$.

\vspace{1.2em}

Unlike \textsc{AusielloMinimization}, we implemented \textsc{AFPMinimization}
even though we cannot prove its correctness. The choice comes out of the following reasons. Actually, \textsc{AFPMinimization} has a principle very
different from all other algorithms, which is not the case of \textsc{AusielloAlgorithm} being somehow an optimization (at best) of \textsc{MaierMinimization}. Furthermore, because \textsc{AFPMinimization} can be expressed within closure systems,
it does not require memory overhead and can be implemented quickly, especially in our case where it was one of the last algorithms to be studied. Since we do not have a concrete idea of its complexity, implementing \textsc{AFPMinimization} allows to have a glimpse of its efficiency, as much as
an idea of the tractability of \textsc{AngluinAlgorithm} when we get rid of 
the randomness of an oracle.

\vspace{1.2em}

We discussed here algorithms coming from logical and query learning. Interestingly those two procedures have another approach to minimization: they try to build a minimal basis against an input theory. Unfortunately, it seems
like this approach may not be more efficient than minimizing
the input base itself, at least from a theoretical point of view. Before concluding this chapter, we will quickly discuss some expectations we can 
derive from our study.


\section{Theoretical expectations and conclusion}

All along previous sections, we discussed in detail several minimization
algorithms. To summarize, we studied:
\begin{itemize}
	\item[-] \textsc{MinCover} with complexity $O(|\B||\I|)$,
	\item[-] \textsc{DuquenneMinimization} with complexity $O(|\B||\I|)$,
	\item[-] \textsc{MaierMinimization}, $O(|\B||\I|)$,
	\item[-] \textsc{BercziMinimization}, $O(|\B|^2|\I|)$,
	\item[-] \textsc{AFPMinimization}, with a complexity supposed to be greater than $|\B|^2|\I|$.
\end{itemize}
\noindent Note that we did not truly listed all the algorithm we have been
talking about more than the algorithms we will indeed study in practical terms
in the next chapter. As one can observe, at least 3 of the algorithms have 
the same complexity even though being quite different. It turns out anyway that
among the first three algorithms, \textsc{MinCover, DuquenneMinimization} are
quite similar. Indeed they both rely on using quasi-closure and right-closure. Those steps first issued in works by Day in \cite{day_lattice_1992}, Duquenne-Guigues in \cite{duquenne_variations_2007, guigues_familles_1986}, Wild in \cite{wild_theory_1994, wild_computations_1995, wild_implicational_1989} have been recently discussed
by Boros and al in \cite{boros_strong_2017}. On the other hand \textsc{MaierMinimization} and \textsc{DuquenneMinimization} share redundancy elimination as a first step, being the last part of \textsc{MinCover}. The most striking difference of \textsc{MaierMinimization} with the two previous algorithms (or in fact all other algorithms but \textsc{AusielloMinimization} being based on \textsc{MaierMinimization}) is that the resulting implication base may not be the Duquenne-Guigues basis. Furthermore, where the two first functions need at most two closure computations per implication, we may need three for Maier's algorithm. This could suppose that \textsc{MaierMinimization} is worst in practice than \textsc{DuquenneMinimization} and \textsc{MinCover}. However, this argument is opposed to redundancy elimination of Duquenne's and Maier's functions. Removing several implications lighten subsequent loops and closure computations. Because \textsc{MinCover} does not perform redundancy elimination beforehand, there is also a chance that it is more expensive than the two others procedures, at least with non-minimal basis. Still, it uses redundancy elimination even though an implication being redundant in \textsc{MinCover} may not be considered as redundant in Maier's terms (because of right-closure in the latter one). Nevertheless, we still have a common point within those three first procedures: we are minimizing a given basis. Quite the opposite, \textsc{BercziMinimization} and \textsc{AFPMinimization} are building a canonical representation of an input basis. For \textsc{DuquenneMinimization} however, even if we are indeed minimizing $\I$ by removing its redundant implications, we are also building our result. Speaking of theoretical complexity, it seems like this building approach is not the most efficient. Still, we may keep in mind that we based our study of complexity on \textsc{LinClosure} which may not be the fastest closure algorithm in practice (see \cite{bazhanov_optimizations_2014}). 


\paragraph{Conclusion} In this chapter, we studied several algorithms for 
minimization task coming from various communities. Starting from algorithms
heavily using quasi-closure of FCA domain (\textsc{MinCover}, \textsc{DuquenneMinimization}), moving to Maier's database approach and Ausiello graphical representation with which we encountered some difficulties. Eventually we studied algorithms from boolean logic and query learning with \textsc{BercziMinimization} and \textsc{AFPMinimization}. After having studied their operations we will in the next chapter implement and compare them under practical computations.



