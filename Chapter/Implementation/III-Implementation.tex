\chapter{Implementation}



\section{Test set up}

\subsection{Tools}

Here, we understand by tools the general implementation ground we rely on:
the languages, some classes, file formatting and so forth. Let us begin by
a very large point of view. We use two languages: C++ (11) for implementing
the algorithms and testing. For data visualization we use python with libraries \lil{matplotlib}, \lil{numpy}, \lil{csv}. To be more precise about C++, 
we use MinGW-64 compiler and \lil{-O3} optimization flag. Our tests are timed
with release builds. We recorded CPU-time, not wall-clock time which consider also the time spent outside the program. CPU is an Intel I5, 2.4 GHz.

\vspace{1.2em}

We used the code from \url{https://github.com/yazevnul/fcai}, used in 
\cite{bazhanov_optimizations_2014} for experience on \textsc{LinClosure}. It has been made under FCA context. In this framework, sets are represented as \lil{boost::dynamic_bitset} instances (named \lil{BitSet}). Because \lil{boost}
was already present in the project and has various built-in tools, we used it
for timing and recording purposes with \lil{boost::timer} and \lil{boost::accumulators} respectively.

\vspace{1.2em}

On top of implementation of the algorithms we wrote few tools to ease I/O and
testing. Even though we will not go into details, let us introduce quickly
main use of those tools, especially the file format we chose. Because we may need to load or save some implication theories, we have to think of a file format able to represent such basis easily. For this aim, we made \lil{.imp}
files, structured as follows:
\begin{itemize}
	\item[-] the first line contains the number of attributes and the number of
	implications, separated by a space,
	\item[-] because of the \lil{Bitset} representation, an implication is 
	written as lists of indices, premise and conclusion, separated by ">". Indices are also space separated.
\end{itemize}
For instance, consider our running example from the previous chapter:
$\Sg = \{ a, \ b, \  c, \  d, \  e, \ f \}$ and $\I = \{ ab \imp cde, \ cd \imp f, \ c \imp a, \ d \imp b, \  abcd \imp  ef \}$. Considering $a$ indexed as 0 and $f$ indexed as 5, the corresponding \lil{.imp} file will be:

\begin{lstlisting}[language = inline, style = Light]
6 5
0 1 > 2 3 4
2 3 > 5
2 > 0
3 > 1
0 1 2 3 > 4 5
\end{lstlisting}

\noindent Using functions \lil{ReadFile} and \lil{WriteFile} in the namespace \lil{ImplicationTools}, one can easily read or write implication basis:

\begin{lstlisting}[language = CoreCpp, style = Light]
theory L, Lc;  // alias for std::vector<FCA::ImplicationInd>
ImplicationTools::ReadFile("input_file.imp", L);
Minimize(L, Lc);  // result of some minimization into Lc
ImplicationTools::WriteFile("output_file.imp", Lc);
\end{lstlisting}

\noindent Note that the meaning of attributes here is somehow lost. So far, this is not an issue and it may be fixed by keeping a trace of correspondence between indices and attributes names. To test minimization, we use a class
\lil{GridTester} writing CSV results.



\begin{itemize}
	\item csv exportation, bitfiles
	\item quickly describe the class GridTest
\end{itemize}

\subsection{Randomly generated data}

This paragraph is important for all following experiences.
We would like to put the emphasis on the way we randomly generate sets and 
implications. First, let us focus on set generation. We use \lil{boost::random} 
and \lil{time} libraries for generating pseudo-random numbers. In particular 
for a set $X$, we use discrete uniform random distribution on the interval 
$\lbrack 0 \ ; \ |\Sg| \rbrack$:
\begin{enumerate}
	\item determine the size $|X|$ of $X$ by drawing a number out of our 
	distribution,
	\item draw again $|X|$ numbers. Because of the interval, we are sure to
	obtain valid indices of elements to set in $X$.
\end{enumerate}
\noindent Note that an element can be drawn more than once, resulting in 
effective $|X|$ smaller than the one we got at the beginning. We do not consider
this as an issue. Generating theories then is as follows:
\begin{enumerate}
	\item generate a conclusion randomly,
	\item generate a premise. Because we want implications to be informative,
	we keep as a premise the difference \textit{premise $-$ conclusion}.
	\item because empty premise is likely to occur several times, resulting in
	$\emptyset \imp \Sg$, we allow for no more than one empty premise. 
	Nevertheless, in order not to loop forever with this condition, we fixed
	a maximal amount of re-roll with a variable \lil{MAX_ITER}. Passed this 
	number of failure, we accept an empty premise anyway.
\end{enumerate}
\noindent Note that this method can be discussed and probably improved. For us,
it seems like this method is sufficient to provide theories with informative
implications, and uniformly distributed sizes of premises and conclusions thus
a "good" representation of the theories space. We did not investigate further 
ways of generations since the main task was to test algorithms and not to study
implication structures in depth. This anyway is an interesting question for
further work.

\subsection{Real data}

In this part we will be interested in application to real datasets. The application we used is FCA since the framework we use is dedicated to FCA
testing. We will first present briefly FCA and its correlation with our
minimization issue, before describing some real datasets we have been using
and their characteristics.

\subsubsection{Introduction to FCA}

Formal Concept Analysis is a technique relying on array-like data and lattices
to describe hierarchies in data. It can be used in data mining, text mining or
chemistry for instance. Usually we are given a set $G$ of \belemp{objects} having some \belemp{attributes} of a set $M$. Between $G$ and $M$ we can define
a binary relation $I$. An object $g$ is related to an attribute $m$, $gIm$, if
$g$ has the attribute $m$. The tuple $(G, M, I)$ is called a \belemp{context}.

\vspace{1.2em}

Quite intuitively, we can define an operation $': 2^G \imp 2^M$, associating to a set of objects $A \subseteq G$ the set $B \subseteq M$ of attributes shared by
all elements of $A$. Conversely, we can set $': 2^M \imp 2^G$ yelling the set
$A$ of objects sharing all attributes of some set $B$. Formally:

\[ A' = \{ m \in M \ | \ \forall g \in A,\ gIm \} \quad \forall A \subseteq G \]
\[ B' = \{ g \in G \ | \ \forall m \in B,\ gIm \} \quad \forall B \subseteq M \]


\noindent When combined together those operators define a closure operator $'': 2^M \imp 2^M$. Very surprisingly, this operator is related with attribute implications. Indeed, given a context $(G, M, I)$ we can draw implications between subsets $A \imp B$ of attributes. Intuitively, an implication $A \imp B$ will be valid if every time we have all attributes from $A$, we also have
attributes from $B$: $A' \subseteq B'$ or equivalently, $B \subseteq A''$. A set
$\I$ of implications over $M$ will be complete and sound if the operator $''$ from the context coincide with $\I(\cdot)$ for all $A \subseteq M$. For the recall, $\I$ is \belemp{complete} with respect to a context $\Ens{K} = (G, M, I)$ if all valid implications of $\Ens{K}$ hold in $\I$, in other words $\I$ contains all informations of $\Ens{K}$. $\I$ is \belemp{sound} with respect to
$\Ens{K}$ if all valid implications of $\I$ are also valid in $\Ens{K}$, that is, $\I$ contains only true informations about $\Ens{K}$. 


\paragraph{Example} Pages and chapters ago, we were talking about flowers and
vegetables. Let us imagine we have the following:
\begin{itemize}
	\item[-] plants as $G$, $G = $ \{ \textit{cactus, water lily, apple tree, sea weed, birch} \},
	\item[-] attributes as $M$, $M = $ \{ \textit{aquatic, perennial, flower, seasonal} \}
\end{itemize}
We can represent a context $\Ens{K} = (G, M, I)$ as an array, see table \ref{tab:FCA-context}.

\begin{table}[ht]
	\centering
	\begin{tabular}{| >{\columncolor{clouds}}c | c | c | c | c |}
		\hline \rowcolor{clouds}
		& aquatic & perennial & flower & seasonal \\ \hline
		cactus & & $\times$ & $\times$ & $\times$ \\ \hline
		water lily & $\times$ & & $\times$ & $\times$ \\ \hline
		apple tree & & & $\times$ & $\times$ \\ \hline
		sea weed & $\times$ & & & \\ \hline
		birch & & & & $\times$ \\ \hline
	\end{tabular}
	
	\caption{Example of a small context}
	\label{tab:FCA-context}
\end{table}

\vspace{1.2em}

\noindent In this context we have for instance \{\textit{aquatic}\}$'$ $=$ \{\textit{water lily, sea weed}\} and \{\textit{cactus, apple tree, water lily}\}$' = $ \{\textit{flower, seasonal}\}. \{\textit{perennial}\} $\imp$ \{ \textit{flower, seasonal}\} is an example of valid implication, while 
\{\textit{aquatic}\} $\imp$ \{ \textit{flower, seasonal}\} is not since \textit{sea weed} is \textit{aquatic} but neither has \textit{flower} nor is \textit{seasonal}.

\vspace{1.2em}

As shown, contexts contain binary informations. Unfortunately, we may usually be
confronted to multi-valued data. If we stick to our green example, we could have
an attribute "size" having possibly many values. With such attributes we could get closer from relational databases. Because real datasets we are going to use
contains multi-valued attributes, we will need \belemp{FCA-scaling}. We will not
go into technical details. The idea however is the following:
\begin{itemize}
	\item[-] for continuous attributes (e.g: size of a plant) we can create
	disjoint classes based on intervals allowing for new binary attributes
	\item[-] for discrete attributes (e.g: zone of living) we can create an
	attribute per existing value.
\end{itemize}
\noindent In our case, considering discrete values is sufficient according to 
real datasets we will rely on.

\paragraph{Example} Let us retake our previous example but adding new attribute: \textit{zone of living}. The context may become table \ref{tab:FCA-context-MV}. Of course, remind that we do not assume
any true knowledge about biology, this stands only for appealing examples.

\begin{table}[ht]
	\centering
	\begin{tabular}{| >{\columncolor{clouds}}c | c | c | c | c | c |}
		\hline \rowcolor{clouds}
		& aquatic & perennial & flower & seasonal & zone of living \\ \hline
		cactus & & $\times$ & $\times$ & $\times$ & dry area \\ \hline
		water lily & $\times$ & & $\times$ & $\times$ & water \\ \hline
		apple tree & & & $\times$ & $\times$ & woods \\ \hline 
		sea weed & $\times$ & & & & water \\ \hline 
		birch & & & & $\times$& woods \\ \hline 
	\end{tabular}
	
	\caption{Example of a (discrete) multi-valued attribute}
	\label{tab:FCA-context-MV}
\end{table}

\noindent To be able to work in FCA framework, we may refactor the last attributes into 3 distinct ones, as in table \ref{tab:FCA-context-split}.

\begin{table}[ht]
	\centering
	\begin{tabular}{| >{\columncolor{clouds}}c | c | c | c | c | c | c | c |}
		\hline \rowcolor{clouds}
		& aquatic & perennial & flower & seasonal & dry area & water & woods \\ \hline
		cactus & & $\times$ & $\times$ & $\times$ & $\times$ & & \\ \hline
		water lily & $\times$ & & $\times$ & $\times$ & & $\times$ & \\ \hline
		apple tree & & & $\times$ & $\times$ & & & $\times$ \\ \hline 
		sea weed & $\times$ & & & & & $\times$ & \\ \hline 
		birch & & & & $\times$ & & & $\times$ \\ \hline 
	\end{tabular}
	
	\caption{Example of FCA-scaling for multi-valued attribute}
	\label{tab:FCA-context-split}
\end{table}

\vspace{1.2em}

Eventually, we shall discuss various possible basis one can build out of a context. We will use 4 of them: the canonical basis, the minimal basis resulting from Maier's algorithm, the basis of minimal generators and the
proper basis. We already discussed the two first ones. We derive them from the
non minimal ones:
\begin{itemize}
	\item[(i)] minimal generators: a set $X$ is a minimal generator of its closure $\I(X)$ if it is minimal in $[ X]_{\I}$,
	\item[(ii)] proper implications: implications $A \imp A^{\bullet}$ where 
	$\bullet$ is a saturation operator 
\end{itemize}


\subsubsection{Some real datasets}

For our tests, we took the datasets used in \cite{bazhanov_optimizations_2014} \url{https://archive.ics.uci.edu/ml/datasets.html}. They are the following ones:
\begin{itemize}
	\item[-] \midemp{"Zoo"}: it contains animals described by attributes such as "mammals", their number of legs and so forth. Because all attributes are discrete valued, the dataset is scaled as explained previously. For 101 animals, we go from 17 attributes to 28 by scaling.
	\item[-] \midemp{"Flare"}: describes various solar flares by their position, size, flare classification, activity... With 1389 objects, we ended up on 49 attributes by flattening all multi-valued ones.
	\item[-] \midemp{"Breast cancer"}:
	\item[-] \midemp{"Breast Wisconsin"}:
	\item[-] \midemp{"Post-operative"}: this dataset is dedicated to determine in which service should a patient go after its operation. All parameters are indicators on the metabolism of the patient such as body temperature,
	blood pressure and so forth. For 90 patients and 8 attributes, we scaled
	to 26 attributes.
	\item[-] \midemp{"SPECT"}:
	\item[-] \midemp{"Vote"}:
\end{itemize}

To summarize characteristics of all datasets, we may observe table \ref{tab:real-DS}. For the recall $|\Sg|$ is the number of attributes, and 
$|\B|$ the number of implications for each possible dataset.

\begin{table}[ht]
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline \rowcolor{clouds}
		\multicolumn{2}{|c|}{$\I$} & $|\Sg|$ & $|\B|$ \\ \hline
		\multirow{3}{*}{Zoo} & minimal & \multirow{3}{*}{28} & 141 \\
		& generators & & 874 \\
		& proper & & 554 \\ \hline
		
		\multirow{3}{*}{Flare} & minimal & \multirow{3}{*}{49} & 3382 \\
		& generators & & 39787 \\
		& proper & & 10692 \\ \hline
		
		\multirow{3}{*}{Breast cancer} & minimal & \multirow{3}{*}{43} & 3352 \\
		& generators & & 16137 \\
		& proper & & 11506 \\ \hline
		
		\multirow{3}{*}{Breast Wisconsin} & minimal & \multirow{3}{*}{91} & 10640 \\
		& generators & & 51118 \\
		& proper & & 45748 \\ \hline
		
		\multirow{3}{*}{Post operative} & minimal & \multirow{3}{*}{26} & 625 \\
		& generators & & 3044 \\
		& proper & & 1721 \\ \hline
		
		\multirow{3}{*}{SPECT} & minimal & \multirow{3}{*}{23} & 2169 \\
		& generators & & 44341 \\
		& proper & & 8358 \\ \hline
		
		\multirow{3}{*}{Vote} & minimal & \multirow{3}{*}{18} & 849 \\
		& generators & & 8367 \\
		& proper & & 2410 \\ \hline
	\end{tabular}
\caption{Summary of real datasets characteristics}
\label{tab:real-DS}
\end{table}


In all this section we have been interested in describing the tools we got or developed for implementing the algorithms and data we used. Regarding tools and language, we use C++ and MinGW-64 compiler for implementing and testing the algorithms, based on the code provided in \cite{bazhanov_optimizations_2014}. The CPU has 2.4 GHz frequency. Apart from generating random implications out
of \lil{boost} and uniform distribution, we took various real datasets from the UCI repository. Those real data we used are the same as ones used when testing the code we used on closure algorithms. In the next section we will study implementation of each algorithms, before concluding by a final comparison on 
the real datasets we evoked.

\section{Pruning the algorithms}

Because we can plug-in various closure procedures (\textsc{Closure}, \textsc{LinCLosure}), we are interested in knowing which configuration is
the most efficient for each minimization procedure. However, because we may
have several possible configurations and tests can be highly time consuming,
we will rely on the next assumption based on the result of \cite{bazhanov_optimizations_2014}. Actually, it is exhibited in this paper
that \textsc{LinClosure} performs worst in general than \textsc{Closure}. However, as we will see there are some possible optimizations for \textsc{LinClosure} dealing with initialization steps. Consequently, 
we will first give priority to \textsc{Closure}, and try to replace it 
by \textsc{LinClosure} whenever we find a possible optimized use of 
for this closure method. We will keep as the "best version" the most efficient
in the tests we will run.

\subsection{\textsc{MinCover}}

Recall that \textsc{MinCover} is a two-steps algorithm: right-closure and redundancy elimination. et $\I$ over $\Sg$ be the basis we are trying to minimize. In the first step, we do not remove or add any implications from
$\I$. Furthermore, we do not alter its premises. Therefore, when using
\textsc{LinClosure}, we may need to initialize counters and list only one
time. However, in the second loop, it is question of first removing an implication from $\I$. From our point of view, removing only one implication
$A \imp B$ in counters may be as complex as \textsc{LinClosure} itself:
\begin{itemize}
	\item[-] if the data structure used for \textit{list} in \textsc{LinClosure}
	is a chained list, then removing $A \imp B$ from some \textit{list}[$a$], $a \in A$ is $O(|\B|)$. Because this has to be done for all $a \in A$, the 
	overhaul operation should be $O(|\I|)$, like \textsc{LinClosure} and in particular, the initialization step
	\item[-] if the data structure is an array, there are again two possibilities. Either we store in \textit{list}[$a$] directly implications or indices of implications in $\I$, but then finding and removing an implication will be $O(|\I|)$. Or we can use marking procedure to store a boolean value at some index $i$ representing the $i-th$ implication to know
	whether or not the $i$-th implication should belong to \textit{list}[$a$]. Removing an implication then may be $O(|\Sg|)$. However, we should take care
	of updating all the boolean values  if we remove an implication from $\I$, because the $i$-th index may not correspond to the implication $i$ anymore. To avoid this update, we can in fact do not remove implications in $\I$ at
	all and put all redundant ones in some trash-marked state. This would require some more conditional statements in the nested for loop of the second step of \textsc{LinClosure}, but it may offer indeed a slight optimization.
\end{itemize}
\noindent We did not test the last possible optimizations due to lack of time,
but also because of the results we shall exhibit hereafter about \textsc{MinCover}, stressing on the bad behaviour of \textsc{LinClosure} in practice. Therefore, the two pseudo-codes we compared for \textsc{MinCover} are
algorithms \ref{alg:MinCoverClo}, \ref{alg:MinCoverLin}.

\vspace{1.2em}

\begin{minipage}[t]{0.4\textwidth}
	\begin{algorithm}[H]
		\TitleOfAlgo{\textsc{MinCoverClo}}
		\KwIn{$\I$: an implication base}
		\KwOut{the canonical base of $\I$}
		
		\BlankLine
		\BlankLine
		
		\ForEach{$A \imp B \in \I$}{
			$\I := \I - \{ A \imp B \}$ \;
			$B  := \textsc{Closure}(\I, A \cup B)$ \;
			$\I := \I \cup \{ A \imp B \}$ \;
		}
		
		\BlankLine
		
		\ForEach{$A \imp B \in \I$}{
			$\I := \I - \{ A \imp B \}$ \;
			$A  := \textsc{Closure}(\I, A)$ \;
			\If{$A \neq B$}{
				$\I := \I \cup \{ A \imp B \}$ \;	
			}
		}
	
		
		\label{alg:MinCoverClo}
	\end{algorithm}
\end{minipage}
~
\begin{minipage}[t]{0.4\textwidth}
	\begin{algorithm}[H]
		\TitleOfAlgo{\textsc{MinCoverLin}}
		\KwIn{$\I$: an implication base}
		\KwOut{the canonical base of $\I$}
		
		\BlankLine
		\BlankLine
		
		\textsc{LinClosureInit}$(\I)$ \;
		
		\ForEach{$A \imp B \in \I$}{
			$\I := \I - \{ A \imp B \}$ \;
			$B  := \textsc{LinClosure}(\I, A \cup B)$ \;
			$\I := \I \cup \{ A \imp B \}$ \;
		}
		
		\BlankLine
		
		\ForEach{$A \imp B \in \I$}{
			$\I := \I - \{ A \imp B \}$ \;
			$A  := \textsc{Closure}(\I, A)$ \;
			\If{$A \neq B$}{
				$\I := \I \cup \{ A \imp B \}$ \;	
			}
		}
	
		
		\label{alg:MinCoverLin}
	\end{algorithm}
\end{minipage}

\vspace{1.2em}

In \textsc{MinCoverLin} we used a function called \textsc{LinClosureInit}. In fact, this function is the initialization step of \textsc{LinClosure}. It sets up the containers \textit{list} and \textit{count}. Then, when we call \textsc{LinClosure}, we just consider pure computation of the closure for a given set. In both versions, redundancy elimination is done the same way.

\vspace{1.2em}

The main idea to test efficiency of each version of the algorithm is to perform
two series of tests corresponding to fixing one of the two parameters of its complexity : $|\Sg|$ or $|\B|$. Each series consists in fixing one of those parameters and testing a range of value for the other. We can observe figure
\ref{fig:MinCover-Prune} as results of those experiments.


\begin{figure}[ht]
	\input{Pictures/III/MinCover/ClosureOperators.tex}
\end{figure}

\vspace{1.2em}

The top left figure represents the time spent in \textsc{MinCover} when $|\Sg|$ is fixed to 100, and $|\B|$ goes from 1000 to 20000 by steps of 1000. The top right one, is $|\B| = 100$ and $|\Sg|$ running from 1000 to 100000 by step of 2000. In both cases We associated an array (right under) with some particular values, being useful to see evolution of \textsc{Closure}, flattened by \textsc{LinClosure}. The reason for this limitation is time spent in test relatively to the information it brings. As one can see, there is a huge difference of speed between \textsc{MinCoverLin} and \textsc{MinCoverClo} even if they differ only in the first loop. For each test case, we iterate the algorithm over 100 randomly generated basis so as to explore somehow the space of implication theories. Hence, we see for instance that when the number of implications $|\B|$ reaches 20000, with $|\Sg| = 100$, \textsc{MinCover} with \textsc{LinClosure} needs roughly 225 seconds to run on average for one execution. Assume we would like to go up to $|\B| = 100000$. Then, we would need at least $225 \times 100 \times 80 = 1800000s$ being approximately 3 weeks of test. Of course, we could change the range function to reduce the number of step, but still in our case we want to compare efficiency of \textsc{Closure} against \textsc{LinClosure}. According to the results, we admitted this range was sufficient to see the gap between those two closure operator. Interestingly, \textsc{LinClosure} permits to see practical complexity of \textsc{MinCover}: while the evolution of $|\B|$ draws a somehow quadratic curve of time, $|\Sg|$ matches a linear growth in spite of noisy points. This matches the theoretical complexity $O(|\B||\I|)$ required by \textsc{MinCover}. One could argue that the algorithms we tested both used \textsc{Closure} in the
second loop, altering the overhaul complexity. Indeed, but as we said, \textsc{LinClosure} performs much worse than \textsc{Closure} so that the practical complexity is driven by \textsc{LinClosure}. For \textsc{MinCover}
we can conclude that \textsc{Closure} is a better choice.

\subsection{\textsc{DuquenneMinimization}}

\subsection{\textsc{MaierMinimization}}

\begin{figure}[ht]
	\input{Pictures/III/Maier/Operators_BFixed.tex}
\end{figure}

\subsection{\textsc{BercziMinimization}}

The algorithm issued by Berczi and al in \cite{berczi_directed_2017} (algoritm \ref{alg:Berczi-min}) can be fine-tuned at first sight without considering closure operators. Indeed, recall that we compute only closure of premises under
$\I$ being our input basis, and $\I_c$ our output one. Furthermore, the algorithm suggests to compute the closure of some premises under $\I$ several 
times, which is extensive and redundant. Furthermore, because we build $\I_c$ only by adding implications, all the closure previously computed can only grow.
Therefore we can improve \textsc{BercziMinimization} by the next means:
\begin{itemize}
	\item[-] compute all $\I(A), A \in \B(\I)$, and store them in a list: $C_{\I}$,
	\item[-] keep a list of growing $\I_c(A), A \in \B(\I)$: $C_{\I_c}$.
\end{itemize}
\noindent To illustrate, we can observe the pseudo-code \textsc{BercziImp}. This algorithm does not strictly reflect its implementation of course, but it
presents the two ideas we were talking about previously. Observe that the closures under $\I$ are the most complicated to compute (because $|\B(\I_c)| \leq |\B(\I)|$ even though we repeatedly add implications to $\I_c$) whence
the interest of avoiding useless computations especially for $\I$.

\begin{algorithm}
	\KwIn{$\I$: an implication theory}
	\KwOut{$\I_c$: the DQ-basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	$C_{\I} := \emptyset, \; C_{\I_c} := \emptyset $ \;
	
	\ForEach{$A \imp B \in \I$}{
		$C_{\I}[A] = \I(A)$ \;
		$C_{\I_c}[A] = A$ \;
	}

	\BlankLine
	
	\While{$\exists A \in \B(\I)$ s.t $C_{\I_c}[A] \neq C_{\I}[A]$}{
		\ForEach{$A \imp B \in \I$}{
			\If{$C_{\I}[A] \neq C_{\I_c}[A]$}{
				$C_{\I_c}[A] = \I_c(C_{\I_c}[A])$ \;
			}
		}
	
		\BlankLine
	
		$A_P, P := \min\{A, C_{\I_c}[A]: \;  C_{\I_c}[A] \neq C_{\I}[A] \}$ \;
		$\I_c := \I_c \cup \{P \imp C_{\I}[A_P] \}$ \
		
	}

	\BlankLine
	
	return $\I_c$ \;
	
	\caption{\textsc{BercziImp}}
	\label{alg:Berczi-imp}
\end{algorithm}

When getting the minimum, note that we get both the minimum $\I_c$-closed set not closed in $\I$ and its associated premise (or equivalently, the associated $\I$-closure). Because notations may be a bit heavy, let us illustrate the meaning of $C_{\I}$ and $C_{\I_c}$ in an example.


\paragraph{Example} As usual, let us retake our small example:
\begin{itemize}
	\item $\Sg = \{a, \ b, \ c, \ d, \ e, \ f \}$,
	\item $\I =$ \{\textit{ab $\imp$ cde, cd $\imp$ f, c $\imp$ a, d $\imp$ b, 
		abcd $\imp$ ef} \} 
\end{itemize}
\noindent Table \ref{tab:berczi-tra} is a trace of the algorithm, using $C_{\I}$ and $C_{\I_c}$. The first column contains premises of $\I$, the second one elements of $C_{\I}$ (that is closures of premises of $\I$ under $\I$) and
the third one, $C_{\I_c}$ (closures of $\B(\I)$ under $\I_c$). The last column
says whether $C_{\I}[A] = C_{\I_c}[A]$.

\begin{table}[ht]
\centering
\subfloat[after initialization step and first loop]{
	\begin{tabular}{| c | c | c | c |}
		\hline \rowcolor{clouds}
		$\B(\I)$ & $C_{\I}$ & $C_{\I_c}$ & $=$ \\ \hline
		$ab$     & $abcdef$ & $ab$ & $\times$ \\ \hline
		$cd$     & $abcdef$ & $cd$ & $\times$  \\ \hline \rowcolor{emerald!40!white}
		$c$      & $ca$     & $c$ & $\times$ \\ \hline 
		$d$      & $db$     & $d$ & $ \times$ \\ \hline
		$abcd$   & $abcdef$ & $abcd$ & $\times$ \\ \hline
	\end{tabular}
	\begin{tabular}{c}
		$\I_c$ before \\ 
		$\emptyset$   \\ \\
		$\I_c$ after \\
		$c \imp ca$
	\end{tabular}	
}\quad
\subfloat[second loop]{
	\begin{tabular}{| c | c | c | c |}
		\hline \rowcolor{clouds}
		$\B(\I)$ & $C_{\I}$ & $C_{\I_c}$ & $=$ \\ \hline
		$ab$     & $abcdef$ & $ab$ & $\times$ \\ \hline
		$cd$     & $abcdef$ & $acd$ & $\times$ \\ \hline
		$c$      & $ca$     & $ca$ & $\lor$ \\ \hline \rowcolor{emerald!40!white}
		$d$      & $db$     & $d$ & $\times$ \\ \hline
		$abcd$   & $abcdef$ & $abcd$ & $\times$ \\ \hline
	\end{tabular}
	\begin{tabular}{c}
		$\I_c$ before \\ 
		$c \imp ca$   \\ \\
		$\I_c$ after \\
		$c \imp ca, \ d \imp bd$
	\end{tabular}	
}

\subfloat[third loop]{
	\begin{tabular}{| c | c | c | c |}
		\hline \rowcolor{clouds}
		$\B(\I)$ & $C_{\I}$ & $C_{\I_c}$ & $=$ \\ \hline \rowcolor{emerald!40!white}
		$ab$     & $abcdef$ & $ab$  & $\times$ \\ \hline
		$cd$     & $abcdef$ & $abcd$ & $\times$ \\ \hline
		$c$      & $ca$     & $ca$ & $\lor$ \\ \hline 
		$d$      & $db$     & $db$  & $\lor$ \\ \hline
		$abcd$   & $abcdef$ & $abcd$ & $\times$\\ \hline
	\end{tabular}
	\begin{tabular}{c}
		$\I_c$ before \\ 
		$c \imp ca, \ d \imp bd$   \\ \\
		$\I_c$ after \\
		$c \imp ca, \ d \imp bd, \ ab \imp abcdef$
	\end{tabular}	
}

\subfloat[final loop]{
	\begin{tabular}{| c | c | c | c |}
		\hline \rowcolor{clouds}
		$\B(\I)$ & $C_{\I}$ & $C_{\I_c}$ & $=$ \\ \hline 
		$ab$     & $abcdef$ & $abcdef$ & $\lor$ \\ \hline 
		$cd$     & $abcdef$ & $abcdef$ & $\lor$ \\ \hline 
		$c$      & $ca$     & $ca$ & $\lor$ \\ \hline 
		$d$      & $db$     & $db$ & $\lor$ \\ \hline
		$abcd$   & $abcdef$ & $abcdef$ & $\lor$ \\ \hline
	\end{tabular}
	\begin{tabular}{c}
		Resulting $\I_c$  \\ 
		$c \imp ca, \ d \imp bd, \ ab \imp abcdef$
	\end{tabular}	
}
\caption{Example of execution of \textsc{BercziImp} using $C_{\I}$ and $C_{\I_c}$}
\label{tab:berczi-tra}
\end{table}

At each step, the highlighted row is the set of premises and closure satisfying $\min \{ C_{\I_c}[A]: \ C_{\I_c}[A] \neq C_{\I}[A] \}$, or this row contains the
minimal (inclusion-wise) $\I_c$-closed set not closed in $\I$. As wished, the
only some closures of $\I_c$ are updated at each step, instead of re-computing all of them every time.

\vspace{1.2em}

 More than fine-tuning the principle, we can optimize the use of \textsc{LinClosure} on two aspects:
\begin{itemize}
	\item[(i)] in the loop where we initialize lists, we can use the same
	pruning as in the right-closing step in \textsc{MinCover},
	\item[(ii)] when computing closures of $\I_c$, because $\I_c$ is only
	increasing in implications, updating list and counters can be done in $O(|\Sg|)$,	hence better than the initialization step of $O(|\I|)$.
\end{itemize}
\noindent Consequently, we will again compare for this algorithm two versions, given as \textsc{BercziImpClo} and \textsc{BercziImpLin} (\ref{alg:BercziImpClo}, \ref{alg:BercziImpLin}).

\vspace{1.2em}

\begin{algorithm}[H]
	\TitleOfAlgo{\textsc{BercziImpClo}}
	\KwIn{$\I$: an implication theory}
	\KwOut{$\I_c$: the DQ-basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	$C_{\I} := \emptyset, \; C_{\I_c} := \emptyset $ \;
	
	\ForEach{$A \imp B \in \I$}{
		$C_{\I}[A] = \textsc{Closure}(\I, A)$ \;
		$C_{\I_c}[A] = A$ \;
	}
	
	\BlankLine
	
	\While{$\exists A \in \B(\I)$ s.t $C_{\I_c}[A] \neq C_{\I}[A]$}{
		\ForEach{$A \imp B \in \I$}{
			\If{$C_{\I}[A] \neq C_{\I_c}[A]$}{
				$C_{\I_c}[A] = \textsc{Closure}(\I_c, C_{\I_c}[A])$ \;
			}
		}
		
		\BlankLine
		
		$A_P, P := \min\{A, C_{\I_c}[A]: \;  C_{\I_c}[A] \neq C_{\I}[A] \}$ \;
		$\I_c := \I_c \cup \{P \imp C_{\I}[A_P] \}$ \;
		
	}
	
	\BlankLine
	
	return $\I_c$ \;
	

	\label{alg:BercziImpClo}
\end{algorithm}

\begin{algorithm}[H]
	\TitleOfAlgo{\textsc{BercziImpLin}}
	\KwIn{$\I$: an implication theory}
	\KwOut{$\I_c$: the DQ-basis of $\I$}
	
	\BlankLine
	\BlankLine
	
	$\I_c := \emptyset$ \;
	$C_{\I} := \emptyset, \; C_{\I_c} := \emptyset $ \;
	
	\textsc{LinClosureInit}($\I$) \;
	
	\ForEach{$A \imp B \in \I$}{
		$C_{\I}[A] = \textsc{LinClosure}(\I, A)$ \;
		$C_{\I_c}[A] = A$ \;
	}
	
	\BlankLine
	
	\While{$\exists A \in \B(\I)$ s.t $C_{\I_c}[A] \neq C_{\I}[A]$}{
		\ForEach{$A \imp B \in \I$}{
			\If{$C_{\I}[A] \neq C_{\I_c}[A]$}{
				$C_{\I_c}[A] = \textsc{LinClosure}(\I_c, C_{\I_c}[A])$ \;
			}
		}
		
		\BlankLine
		
		$A_P, P := \min\{A, C_{\I_c}[A]: \; C_{\I_c}[A] \neq C_{\I}[A] \}$ \;
		$\I_c := \I_c \cup \{P \imp C_{\I}[A_P] \}$ \;
		\textsc{LinClosureAddImp}($\I_c, P \imp C_{\I}[A_P]$)\;
		
	}
	
	\BlankLine
	
	return $\I_c$ \;
	
	\label{alg:BercziImpLin}
\end{algorithm}

\vspace{1.2em}

In fact, \textsc{BercziImpClo} does differ from \textsc{BercziImp} only by the closure notations. However, when dealing with \textsc{LinClosure} in \textsc{BercziImpLin}, we use first \textsc{LinClosureInit} to instanciate 
\textit{list} and \textit{count} for \textsc{LinClosure} in $\I$. Hence a call
to this procedure performs only the effective closure computations. Then, 
\textsc{LinClosureAddImp} just updates \textit{list} and \textit{count} for $\I_c$ by adding a new implication in them. Note that before the first call to
this subroutine, \textit{list} and \textit{count} for $I_c$ are empty, hence
\textsc{LinClosure}($\I_c$, $X$) returns $X$.

\vspace{1.2em}

We may observe tests of \textsc{BercziMinimization} (or more precisely, \textsc{BercziImp}) in graphs and tables of figure \ref{fig:Berczi-Prune}. As in previous cases we represent two range-based test on one parameter, when the other is fixed. On the left part we have a graph and a table of tests with fixed $|\Sg| = 100$. On the other side, we fixed $|\B|$ to $100$ also (and not 1000 as in previous cases!). We lowered the boundary $|\B|$ because of the complexity of the algorithm. As one can observe if we recall previous tests, even if $|\B| = 100$ the results are of the same order as previous tests where $|\B| = 1000$. Or course we should consider this quick comparison as no more but a quick glance to previous results since tests are not ran on the same basis (because of randomness). In any event, in spite of the noisy behaviour when $|\Sg|$ tends to $100000$, we can observe a linear growth of the execution time in accordance with the theoretical complexity of \textsc{BercziMinimization}.


\begin{figure}[ht]
	\input{Pictures/III/Berczi/ClosureOperators.tex}
\end{figure}

\vspace{1.2em}

Regarding the evolution of time when $|\Sg|$ is fixed we denote a highly chaotic
behaviour. However, we may perceive a polynomial growth of the time required by
\textsc{BercziMinimization} when used with \textsc{LinClosure} matching theoretical expectations. If we keep on throwing an eye on previous results,
it seems like \textsc{BercziMinimization} performs better than \textsc{MinCover}
for some values, notably in the table, for $|\B| = 10000$ and $15000$. In fact,
those values can somehow be considered as outliers in the graph we gave (they are convexities in the curve). Because the mean measure is sensitive to outlier
basis, we can suppose that in those particular cases we generated some highly redundant basis decreasing the cost of operations. As we will see with real datasets in joint comparison, the difference between \textsc{MinCover} and \textsc{BercziMinimization} is clearly in favour of \textsc{MinCover}. Again, the difference between \textsc{Closure} and \textsc{LinClosure} is clearly visible and we can take \textsc{Closure} as our fastest closure operator.

\subsection{\textsc{AFPMinimization}}


\vspace{1.2em}

During all this section we have been testing our minimization procedures with two closure algorithms: \textsc{Closure} and \textsc{LinCLosure}. From all of
our tests it seemed like \textsc{LinClosure} performs much worse than the 
other operator. This is in accordance with conclusions and hypothesis drawn in
\cite{bazhanov_optimizations_2014} about efficiency of \textsc{LinClosure}. However, those tests allowed us to have a glimpse of the correspondence between
theoretical and practical complexity. Still, we may be able to find various more combinations of closure operations to test to see which one is the best. For us, in the meantime of the master thesis and regarding the performance of \textsc{LinClosure} it seemed that our tests were sufficient. Now that we found out which version of each algorithm was the fastest, we can move forward to 
further tests and joint comparison.


\section{Joint comparison}

In this section we consider the algorithms pruned previously and we compare them
with the real datasets we exposed in the first section of this chapter (see table \ref{tab:real-DS}). For the recall they are datasets taken from the UCI 
repository. For all of those 28 basis, we ran the 5 minimization procedure
we reviewed and implemented. The value recorded is the execution time in seconds
for minimizing the given basis. See the results in \ref{tab:real-exe}.

\vspace{1.2em}

First, one can observe the wide range of performances. On the one hand,
\textsc{MinCover} and \textsc{DuquenneMinimization} do not exceed 100 seconds of execution time, while \textsc{BercziMinimization} and \textsc{AFPMinimization} can require up to several hours. The most striking
case is for Breast Wisconsin case, with proper and generators basis. This is
probably because of the number of implications in those cases. Anyway, while
the two first algorithms require about 30 to 90 seconds for minimization, while
\textsc{BercziMinimization} needs roughly one hour and \textsc{AFP} up to ten or eleven hours. This stresses on the intractability of \textsc{AFP} in practice, and possibly the Angluin algorithm when we are prevented from the speed of an oracle.

\vspace{1.2em}

Regarding the difference between \textsc{MinCover} and \textsc{DuquenneMinimization}, observe that in general, the algorithm by Duquenne is faster on non-minimal basis and slightly slower on already minimal ones. In both cases, one hypothesis to explain those gaps could be the second
loop of \textsc{DuquenneMinimization} allowing for a limitation of closure computations. As we already discussed, the number of closure computations in 
\textsc{MinCover} is likely to be constant whatever the case is, even though
those operations become lighter with the reduction of the input basis. In \textsc{DuquenneMinimization} however, apart from first left-saturation and
redundancy elimination allowing for some savings in closure computations, we stop an iteration of the second loop whenever some quasi-closed premise is not
pseudo-closed. This break point permits to omit closure computations and in practice, thanks to lectic ordering, we may not need to go over all implications of the output basis to check whether an implication is redundant or not. The only case when the second loop should require more computations is when the base is already minimal, because for each step of the second loop, we have to go over all implications of the growing output basis. As remarked in most of execution times, when the basis is already minimal \textsc{DuquenneMinimization} performs somehow slighter worse than 
\textsc{MinCover}, which could be explained by the previous hypothesis.

\begin{table}[ht]
	\centering
\begin{tabular}{| c | c || c | c | c | c | c |}
	\hline \rowcolor{clouds}
	\multicolumn{2}{c}{$\I$} & \textsc{MinCover} & \textsc{Duquenne} & \textsc{Maier} & \textsc{Berczi} & \textsc{AFP} \\ \hline
	
	\multirow{4}{*}{Zoo} & DQ & < 0.001 & < 0.001 & < 0.001 & < 0.001 & 0.016 \\
	& min & < 0.001 & < 0.001 & 0.015 & < 0.001 & 0.016 \\
	& proper & 0.007 & < 0.001 & < 0.001 & 0.016 & 0.063 \\
	& mingen & 0.009 & 0.016 & < 0.001 & 0.047 & 0.094 \\ \hline
	
	\multirow{4}{*}{Flare} & DQ & 0.110 & 0.203 & 0.328 & 27.922 & 115.656 \\
	& min & 0.162 & 0.219 & 0.406 & 27.750 & 116.594 \\
	& proper & 2.166 & 0.953 & 1.250 & 88.375 & 524.031 \\
	& mingen & 19.382 & 9.906 & 11.344 & 160.328 & 2810.620 \\ \hline
	
	\multirow{4}{*}{Breast Cancer} & DQ & 0.120 & 0.140 & 0.234 & 33.047 & 90.031 \\
	& min & 0.147 & 0.156 & 0.297 & 26.578 & 89.516 \\
	& proper & 2.712 & 1.031 & 1.219 & 93.266 & 429.844 \\
	& mingen & 2.850 & 1.578 & 1.703 & 102.562 & 598.172 \\ \hline
	
	\multirow{4}{*}{Breast Wisconsin} & DQ & 1.333 & 1.703 & 2.8125 & 1005.750 & 3109.920 \\
	& min & 1.801 & 2.016 & 3.531 & 949.953 & 3140.940 \\
	& proper & 88.188 & 33.688 & 46.312 & 3675.910 & 40521.0 \\
	& mingen & 51.378 & 32.859 & 33.843 & 2772.980 & 38310.200 \\ \hline
	
	\multirow{4}{*}{Operative} & DQ & 0.004 & < 0.001 & 0.015 & 0.219 & 0.734 \\
	& min & 0.005 & < 0.001 & 0.031 & 0.219 & 0.719 \\
	& proper & 0.048 & 0.016 & 0.016 & 0.594 & 2.422 \\
	& mingen & 0.085 & 0.063 & 0.109 & 0.813 & 4.063 \\ \hline
	
	\multirow{4}{*}{SPECT} & DQ & 0.040 & 0.078 & 0.141 & 10.328 & 23.609 \\
	& min & 0.055 & 0.094 & 0.203 & 8.156 & 22.906 \\
	& proper & 1.270 & 0.484 & 0.531 & 51.063 & 118.531 \\
	& mingen & 32.580 & 13.375 & 13.609 & 194.875 & 930.578 \\ \hline
	
	\multirow{4}{*}{Vote} & DQ & 0.007 & 0.016 & 0.032 & 0.484 & 1.579 \\ 
	& min & 0.009 & 0.016 & 0.047 & 0.469 & 1.516 \\
	& proper & 0.090 & 0.047 & 0.078 & 1.625 & 6.203 \\
	& mingen & 0.682 & 0.313 & 0.485 & 4.109 & 22.875 \\ \hline
	
	
\end{tabular} 
\caption{Comparison of the algorithms on real datasets (execution in $s$)}
\label{tab:real-exe}
\end{table}

\vspace{1.2em}

One can also denote that \textsc{MaierMinimization} seems to be placed in between \textsc{DuquenneMinimization} and \textsc{MinCover} except in the minimal cases. Our hypothesis is the same as for the difference between 
\textsc{DuquenneMinimization} and \textsc{MinCover}: in fact, the first loop of Maier's algorithm is to get rid of redundant implications, therefore when a 
basis is highly redundant, first removing them allow to spare numerous closure
computations and reduces closure cost. This is not the case in \textsc{MinCover}
where we compute right closure of all implications. Still, the rest of the algorithm seems to be slower than the second loop of \textsc{DuquenneMinimization}. This may be because to determine equivalence classes and removing direct determinations we need to use closure operators
twice for each implications, while with the algorithm by Duquenne, we only compute closure when it is needed after an increasing number of set operations though. The interesting point which seems to go in the direction of our explanation is the behaviour under already minimal basis: Maier's algorithm requires an asymptotically higher number of closure computations than both 
\textsc{DuquenneMinimization} and \textsc{MinCover} making it the heaviest 
procedure when it comes at minimizing a basis already minimal.

\vspace{1.2em}

We would like to remind that this explanation holds within the scope of our
tests. We cannot assume it in general and it should be tested in further work on other datasets. Let us say this is a valid hypothesis. In this case, one
should be first worried about removing as much  redundant implications as possible to lighten the burden of subsequent closure computations, whether it is for getting the canonical basis or not.


\section{Perspectives and difficulties}

So far we have developed and explained our work during this master thesis. However, there are two points we should mention at last being in a sense out of 
the scope of the algorithms, namely difficulties we encountered and perspectives
our study leave for future work. 

\vspace{1.2em}

First, let us discuss difficulties. Apart from having to deal with various mathematical frameworks we concentrated to closure systems, some algorithms
or articles appeared to be very demanding. This the case for instance with the work of Ausiello and al. which took us about one month of work before concluding to a potential mistake. Often, those technical difficulties to understand some articles as much as knowing whether to study or not some frameworks found solutions in discussions with the academic supervisor. Tests have also been a problem since most of them are time-consuming (up to several days) relatively to the time we had to work on this thesis. Even though tests were performed in parallel with other work, having only one computer slows down tests because of side tasks. Still, we got some results allowing for hypothesis
on the behaviour of several algorithms leading to hints and tracks to follow for future work.

\vspace{1.2em}

Speaking about future work, let us talk about perspectives. We provided both theoretical and practical review. The two approaches benefit from possible improvement or in-depth look. First on a theoretical point of view, the algorithm \textsc{AFPMinimization} derived from query learning still requires
a proof of correctness, even if we observed a dreadful efficiency in practice. Furthermore, we may still investigate Ausiello to sharpen our insight of redundancy elimination and removing superfluous nodes. An interesting question
drawing a link between theoretical and practical study would be to get more
understanding of redundancy so as to regulate its generation under testing and
improving algorithms by pointing out special kinds of redundancies. Talking about testing, the main perspective on a practical side would be first to run tests on new real datasets to see whether our hypothesis hold or not. One may also be interested in testing more combinations of closure algorithms or finding
more optimizations in each minimization procedure.


\paragraph{Conclusion}

