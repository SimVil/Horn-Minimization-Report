\chapter{Introduction to implications through closure systems}

In this first chapter, we will be involved in presenting our topic of 
minimization. For this ground to be understandable by as much readers as 
possible, we will heavily rely on toy examples to illustrate and provide 
intuition on the various notions we will introduce. To be more precise on the
path we are about to follow here, we are first to expose an informal
small example of the task we want to achieve. Then, we shall investigate the
history of research on our topic, to act as an exposition of the actual 
knowledge on the question and to give a context to our study. For the rest of
this chapter we will get familiar with mathematical objects called 
\belemp{closure operators} and \belemp{closure systems} modelling our problem.
As we shall observe, the topic of minimization can be described in several 
mathematical frameworks. However, even if we describe briefly other objects
in next chapters, we will stick to our closure framework in all the report in
order to have a leading light among various different terminologies.

\section{Implications and minimization: first meeting}

Let us imagine we are some specialist of flowers and plants in general. As such,
we are interested in studying \belemp{correlations} between plant 
characteristics. Some possible traits are: \textit{colourful, bloom, wither, 
aquatic, seasonal, climbing, scented, flower, perennial} and so forth. Having
observed countless plants during our studies, we are able to draw relations
among all those \belemp{attributes}. For instance, we know that a plant having
the attribute \textit{flower} is likely to have traits \textit{scent, bloom, 
wither} while a plant being \textit{perennial} (i.e: does not need a lot of
water to survive, like a cactus) is not likely to be \textit{aquatic}. 

\vspace{1.2em}

Those relations \textit{"if we have some attributes, we get those ones too"}
depict correlation between attributes (not cause/consequence!). It is important
to stress on the knowledge those relations bring. They just indicate that 
whenever we have say \textit{flower}, we have also \textit{colourful}. This is 
very different from saying that \textit{because} some plant is a flower, it 
will be colourful. We call those correlation relations \belemp{implications}
and use \textit{flower $\imp$ colourful} to denote \textit{"if we have the attribute flower, then we have colourful"}. Now let us give some implications:

\begin{center}
	\textit{(colourful, bloom $\imp$ seasonal), (colourful, wither $\imp$ 
		seasonal), (bloom $\imp$ wither)}
\end{center}

\noindent All those implications represent a certain amount of knowledge. While
in our example they are not numerous we could imagine having tons of them. Hence
we would wonder whether there is a way to reduce the number of implications 
while keeping all the knowledge they represent. This question is 
\belemp{minimization}. Actually, in our small example we can reduce
the number of implications. Take \textit{(colourful, bloom $\imp$ seasonal)}. 
We can derive this implication relation only with the two other 
ones. Indeed, because a plant \textit{blooming} is likely to \textit{wither} 
(3rd implication), we have \textit{(colourful, bloom $\imp$ wither)}, but since 
we now have \textit{wither} and \textit{colourful} we also have 
\textit{seasonal} (2nd implication). That is,
the implication \textit{(colourful, bloom $\imp$ seasonal)} is useless (or 
\belemp{redundant}) in our context and can be removed. Our set of implications 
will then be smaller, but pointing out the same relations as before. 

\vspace{1.2em}

To summarize, we have seen that out of a set of \belemp{attributes} we can draw
several relations called \belemp{implications} providing some knowledge. We also
realized that sometimes, some implications are not necessary. Consequently, 
the set of implications we are given can be \belemp{minimized} without 
altering the information it contains. This is the topic we were interested 
during this master thesis. In the next section, we will trace back the overhaul
knowledge on this question.

\section{Research on implications theories minimization}

This section is intended to supply the reader with a general overview of the
minimization topic. After a short contextual information, we focus on some 
relevant results on the question by providing references to algorithms and 
properties dedicated to our problem. Eventually, we situate our work within this
context.

\vspace{1.2em}

The question of minimization has been discussed and developed through various 
frameworks, and several computer scientists communities. Notice that in order 
not to make this synthesis too long, we will stay within the context of 
minimization and will not trace the field of implication theories in general. 
For a survey of this domain anyway, the reader should refer to 
\cite{wild_joy_2017}. Also, note that minimality in general terms is not 
unique. Indeed, one can define several type of minimality among implication 
systems. For instance, not only we can define minimality with respect to the 
number of implication within a system (which is our interest) but also with 
respect to the number of attributes in each implications. The former one is 
called \midemp{canonical} in relational database field, and \midemp{hyperarc 
	minimum} within the graph context. Especially in the graph-theoretic and 
boolean logic settings, one can derive more types of minimality. For general 
introduction to boolean logic notations, we invite the reader to see 
\cite{cori_mathematical_2000}. In terms of propositional logic, implications 
are represented through Horn formulae. Interestingly, the minimization problem 
we are going to consider is the only one being polynomial time solvable. Other 
problems are proved to be NP-Complete or NP-Hard. For more discussion on other 
minimality definitions and their computational complexity, the reader should 
refer to \cite{boros_strong_2017, ausiello_directed_2017, 
	b._ganter_conceptual_2016, ausiello_minimal_1986, wild_joy_2017, 
	boros_horn_1998}. In particular for NP-Completeness in the canonical case, 
	one 
can see \cite{hammer_optimal_1993}. In subsequent explanations, we will refer 
to minimization with respect to the number of implications.

\vspace{1.2em}

To the best of our knowledge, the two first fields in which algorithms and 
properties of minimality arose are Formal Concept Analysis (FCA) (see 
\cite{ganter_formal_1999, 
	ganter_two_2010} for an introduction) and Database Theory (DB) (see 
\cite{maier_theory_1983}). Both sides were developed independently in the early 
80's. For the first domain, characterization of minimality goes to Duquenne and 
Guigues \cite{guigues_j.l_familles_1986}, in which they describe the so-called 
\belemp{canonical basis} (also called \belemp{Duquenne-Guigues basis} after its 
authors, abbreviated DG all along this report) relying on the notion of \belemp{pseudo-closed sets}. For the database part, study of implications is made by Maier through FD's (\cite{maier_theory_1983, david_minimum_1980}). The polynomial time algorithm he gives for minimization heavily relies on a fast subroutine discovered by Beeri and Bernstein in \cite{beeri_computational_1979}, 1979.

\vspace{1.2em}

From then on, knowledge increased over years and spread out over domains. 
Another algorithm based on a minimality theorem is given by Shock in 1986 
(\cite{shock_computing_1986}). Unfortunately, as we shall see and as already 
discussed by Wild in \cite{wild_computations_1995} the algorithm may not be
correct in general, even though the underlying theorem is. During the same 
period, Ausiello and al. brought the problem to graph-theoretic ground, and 
provided new structure known as \textit{FD-Graph} and algorithm to represent 
and work on implication systems in \cite{ausiello_directed_2017, 
	ausiello_graph_1983, ausiello_minimal_1986}. This approach has been seen in 
graph theory as an extension of the transitive closure in graphs 
(\cite{aho_transitive_2006}), but no consideration equivalent to minimization 
task seems to have been taken beforehand, as far as we know. Still in the 1980 
decade, Ganter expressed the canonical basis formalized by Duquenne and Guigues 
in his paper related to algorithms in FCA, \cite{ganter_two_2010} through 
closure systems, pseudo-closed and quasi-closed sets. Next, Wild 
(\cite{wild_implicational_1989, wild_theory_1994, wild_computations_1995}) 
linked within this set-theoretic framework both the relational databases, 
formal concept analysis and lattice-theoretic approach. In relating those 
fields, he describes an algorithm for minimizing a basis, similar to algorithms 
of Day and, somehow, Shock (resp. \cite{day_lattice_1992},  
\cite{shock_computing_1986}). This framework is the one we will use for our 
study, and can be found in more recent work by Ganter \& Obiedkov in 
\cite{b._ganter_conceptual_2016}. Also, the works of Maier and Duquenne-Guigues 
have been used in the lattice-theoretic context by Day in 
\cite{day_lattice_1992} to derive an algorithm based on congruence relations. 
For in-depth knowledge of implication system within lattice terminology, we can 
see \cite{davey_introduction_2002} as an introduction and 
\cite{bertet_lattices_2016} for a survey. Later, Duquenne proposed some 
variations of Day's work with another algorithm in 
\cite{duquenne_variations_2007}. More recently, Bor\`os and al. by 
working in a boolean logic framework, exhibited a theorem on the size of
canonical basis \cite{boros_exclusive_2010, boros_strong_2017}. They also gave
a general theoretic approach that algorithm should do one way or another on
reduction purpose. Out of these papers, Berczi \& al. derived a new 
minimization procedure based on hypergraphs in \cite{berczi_directed_2017}. 
Furthermore, an algorithm for computing the canonical basis starting from any 
system is given in \cite{b._ganter_conceptual_2016}. This last algorithm was our starting point for this study.

\vspace{1.2em}

Even though the work we are going to cite is not designed to answer this 
question of minimization, it must also be exposed as the algorithm is 
intimately related to DG basis and can be used for base reduction. The paper
of Angluin and al. in query learning, see \cite{angluin_learning_1992}, provides
an algorithm for learning a Horn representation of an unknown initial formula. 
It has been shown later by Ariàs and Alcazar (\cite{arias_canonical_2009}) that
the output of Angluin algorithm was always the Duquennes-Guigues basis.

\vspace{1.2em}

Our purpose with this master thesis is to review and implement as much as 
possible the algorithms we exposed to provide a comparison. This comparison 
shall act as both theoretical and experimental statement of algorithm 
efficiency. As we already mentioned we will focus on closure theory framework.
The reason for this choice is our starting point. Because we start from the
algorithms provided by Wild and because the closure framework is the one we 
are the most familiar with, we focus on clearly explain this terminology with
examples. However, once we will be comfortable with those definitions, we will 
relate other frameworks to our main approach in the next chapter, to explain and
draw parallels with other algorithms. In the next section we will focus on 
theoretical definitions we shall need to understand the algorithms we have 
implemented.


% ---------------------------------------------------------------------------- %
% ==== Section: theoretic approach
% ---------------------------------------------------------------------------- %

\section{Implications and minimization: theoretic approach}

Here we will dive into mathematical representation of the task we gave
in the first section of this chapter. For the recall, our aim here is to
get familiar with the representation being closest from closure systems.  Most 
of the notions initially come from \cite{guigues_j.l_familles_1986, 
ganter_two_2010, wild_theory_1994,	ganter_formal_1999} but the reader can 
also find more than sufficient explanations in \cite{b._ganter_conceptual_2016, 
wild_joy_2017}. Readers with knowledge in relational databases will recognize 
most of functional dependency notations. The reason is close vicinity between 
implications and functional dependencies. Talking about our needs, we can 
consider them as equivalent notations. Actually, the real-life application our 
set up will be the closest from is FCA (Formal Concept Analysis, \cite{ganter_formal_1999}) as we shall see in the last chapter.


% ==== Subsection: implications and closure system

\subsection{Implications and closure systems}

The easiest object to project onto mathematical definitions is our attribute
set. For all the report, we fix $\Sg$ to be a set of \belemp{attributes}. 
Usually, we will denote attributes by small letters: \textit{a, b, c, \dots} 
and subsets of $\Sg$ (groups of attributes) will be denoted by capital letters: 
\textit{A, B, C, \dots} We assume the reader to have few background in 
elementary set-theoretic and logical notations. 

\begin{definition}[\midemp{Implication, implication system}] An 
\belemp{implication} over $\Sg$ is a pair $(A, B)$ with $A, B \subseteq \Sg$. 
It is usually denoted by $A \imp B$. A set $\I$ of implications is called an 
\belemp{implication system}, \belemp{implication theory} or 
\belemp{implication(al) base(is)}.
\end{definition}

\noindent Note that given as is, this definition seems to lose the semantic
relation we depicted earlier. But we should keep in mind that in our set up, we
will be given implications more than an attribute set. Hence, implications will
make sense on their own, independently from the attribute set they are drawn 
from. Quickly, remark that implications in logical terms are expressed as
\textit{Horn formulae} giving another of its names to implication theories. 
Also, in $A \imp B$, $A$ is said to be the \belemp{premise} (or \belemp{body}) 
and $B$ the \belemp{conclusion} (\belemp{head}).

\begin{definition}[\midemp{Model}] Let $\I$ be an implication system over 
	$\Sg$, and $M \subseteq \Sg$. Then:
	\begin{itemize}
		\item[(i)] $M$ is a \belemp{model} of an implication $A \imp B$, 
		written 
		$M \models A \imp B$, if $B \subseteq M$ or $A \nsubseteq M$,
		\item[(ii)] $M$ is a \belemp{model} of $\I$ if $M \models A \imp B$ for 
		all
		$A \imp B \in \I$.
	\end{itemize}
	
\end{definition}

\noindent The notion of model may seem disarming at first sight. But $M$ being
a model of $A \imp B$ simply means that, if $A$ is included in $M$, then for
the implication $A \imp B$ to hold in $M$, we must have $B$ in $M$ too. This 
still suits the intuitive notion of premise/conclusion. Placed in the context
of $M$, $A \imp B$ says \textit{"whenever we have A, we must also have B"}.
Reader with some background in mathematical logic should be familiar with the
notation $\models$, denoting semantic entailment, as opposed to $\vdash$ for
syntactic deduction (see \cite{cori_mathematical_2000}). By a fortunate twist of
fate, semantic entailment is our next step:

\begin{definition}[\midemp{Semantic entailment}] We say that an implication 
$A \imp B$ \belemp{semantically follows} from $\I$, denoted $\I \models A 
\imp B$, if all models $M$ of $\I$ are models of $A \imp B$. Given two basis $\I_1$, $\I_2$, $\I_1 \models \I_2$ if $\I_1 \models A \imp B$ for all
implications $A \imp B$ of $\I_2$. Furthermore, $\I_1$ and $\I_2$ are \belemp{equivalent} if $\I_1 \models \I_2$ and $\I_2 \models \I_1$.  
	
\end{definition} 

\noindent In fact, we can also say that $A \imp B$ \belemp{holds} in $\I$. Because future definitions are going to be on a slightly different 
structure, even though closely related to implication systems of course, let us
rest for a while and illustrate our definitions with an example.


\paragraph{Example} Consider again our plant properties. Let $\Sg = $ 
\{\textit{colourful, bloom, wither, seasonal, aquatic, perennial, flower, 
	scented}\}. An implication could be \textit{flower $\imp$ scented}, or even
\textit{(bloom, aquatic) $\imp$ colourful} if we do not care anymore about semantic interpretations. An implication basis $\I$ is for instance:

\begin{center}
	\textit{(colourful, bloom $\imp$ seasonal), (colourful, wither $\imp$ 
		seasonal), (bloom $\imp$ wither)}
\end{center}

\noindent and $M = $\textit{(colourful, bloom, seasonal)} is a model of 
\textit{colourful, bloom $\imp$ seasonal} because both the head and the body
of the implication belong to $M$. Also, $M$ is not a model of $\I$ because it
is not a model of \textit{bloom $\imp$ wither}. A model of $\I$ could be 
\textit{(bloom, wither)} or even the empty set $\emptyset$.

\vspace{1.2em}

Next definitions are about closure operators, and closure systems. We need 
to ground ourselves in those definitions before returning to implications.
$2^{\Sg}$ is the set of all subsets of $\Sg$, also named the \belemp{power set}
of $\Sg$.

\begin{definition}[\midemp{Closure operator}] Let $\Sg$ be a set and $\phi : 
	2^{\Sg} \imp 2^{\Sg}$ an application on the power set of $\Sg$. $\phi$ is
	a \belemp{closure operator} if $\forall X, Y \subseteq \Sg$:
	\begin{itemize}
		\item[(i)] $X \subseteq \phi(X)$ \midemp{(extensive)},
		\item[(ii)] $X \subseteq Y \imp \phi(X) \subseteq \phi(Y)$
		\midemp{(monotone)},
		\item[(iii)] $\phi(X) = \phi(\phi(X))$ \midemp{(idempotent)}.
	\end{itemize}
	$X \subseteq \Sg$ is called \belemp{closed} if $X = \phi(X)$.
\end{definition}

\begin{definition}[\midemp{Closure system}] Let $\Sg$ be a set, and $\Sg^{\phi}
	\subseteq 2^{\Sg}$. $\Sg^{\phi}$ is called a \belemp{closure system} if:
	\begin{itemize}
		\item[(i)] $\Sg \in \Sg^{\phi}$,
		\item[(ii)] if $\cal{S} \subseteq \Sg^{\phi}$, then $\bigcap \cal{S} 
		\in 
		\Sg^{\phi}$ \quad \midemp{(closed under intersection)}.
	\end{itemize}
	
\end{definition}

\noindent In the second definition, it is worth stressing on the fact that
$\Sg^{\phi}$ is a set of sets. Also, the notation $\Sg^{\phi}$ may seem 
surprising, but it has been chosen purposefully. Indeed, to each closure system
$\Sg^{\phi}$ over $\Sg$, we can associate a closure operator $\phi$ and 
vice-versa:
\begin{itemize}
	\item[-] from $\phi$ to $\Sg^{\phi}$: compute all closed sets of $\phi$ to
	obtain $\Sg^{\phi}$,
	\item[-] from $\Sg^{\phi}$ to $\phi$: define $\phi(X)$ as the smallest element of $\Sg^{\phi}$ (inclusion-wise) containing $X$. Observe that such a set always exists in $\Sg^{\phi}$ because $\Sg \in \Sg^{\phi}$.
\end{itemize}
In any event, this notation used for clear exposition of the link
between closure systems and closure operators will be adapted to our context
of implication systems as we shall see later on. Notice that one can encounter 
another object, \belemp{closure space}, being a pair ($\Sg$, $\phi$) where
$\Sg$ is a set and $\phi$ a closure operator over $\Sg$. We are likely to find
this notation notably in \cite{wild_implicational_1989, 
	wild_theory_1994} where a general theory of closure spaces is addressed.


\paragraph{Example} Let us imagine we have four people: \textit{Jezabel, Neige, 
Seraphin} and \textit{Narcisse}. Let us assume they all know each other and then
define a relation \textit{"like"} between them. For instance, say 
\textit{Seraphin likes Jezabel}. this relation is a \belemp{binary relation}: 
it relates pairs of elements. We can represent this relation by a graph where 
nodes are people and edges represent relations:

\begin{figure}[ht]
	\input{Pictures/I/Love.tex}
\end{figure}

The arrow from \textit{Seraphin} to \textit{Jezabel} stands for 
\textit{"Seraphin likes Jezabel"} and the arrow from \textit{Narcisse} to itself
means equivalently \textit{"Narcisse likes Narcisse"}. With this clear, let 
us introduce an operation gathering people. Starting from any group $A$ of 
persons presented here, let's add to $A$ every person liked by at least one 
element of $A$, until adding new people is not possible any more. For example:
\begin{itemize}
	\item[-] if we start from \textit{Neige}, because \textit{Neige likes
	Jezabel} and \textit{Jezabel likes Narcisse} we will add
	both of them to the group of \textit{Neige},
	\item[-] because \textit{Narcisse} is related to himself only, we have no people to add in his group.
\end{itemize}
\noindent Now observe that this operation of gathering people is in fact a
closure operator:
\begin{itemize}
	\item[(i)] it is \midemp{extensive}: starting from any group of people,
	we can only add new ones, hence either the group does not change (e.g: 
	\textit{Narcisse}) or grows,
	\item[(ii)] it is \midemp{monotone}: if we start from a group $A$ containing
	a group $B$, it is clear that we will at least gather in $A$ all the people
	we would add with $B$,
	\item[(iii)] \midemp{idempotency}: once we added all the people we had to
	reach, then trying to find new people is useless by definition. Hence the
	group will remain the same if we apply our operation once more.
\end{itemize}

\vspace{1.2em}

We are going to get back to our main implication purpose to illustrate the 
notion of closure in our context. It turns out that given a basis
$\I$ over some set $\Sg$, the set of models of $\I$, $\Sg^{\I}$, is a closure 
system. Moreover, the operator $\I : 2^{\Sg} \imp 2^{\Sg}$ associating to a 
subset $X$ of $\Sg$ the smallest model (inclusion wise) containing $X$ is 
a closure operator. Furthermore, the closure system it defines is 
$\Sg^{\I}$. An interesting point is the mathematical computation of 
$\I(X)$ given $\I$ as a set of implications. We rely on 
\cite{wild_implicational_1989, b._ganter_conceptual_2016} to this end. Let 
us define a temporary operation $\circ : 2^{\Sg} \imp 2^{\Sg}$ as follows:

\[ X^{\circ} = 
X \cup \bigcup \{ B \; | \; A \imp B \in \I, \; A \subseteq X \} \]

\noindent Applying this operator up to stability provides $\I(X)$. In other 
words $\I(X) 
= X^{\circ \circ \dots}$. It is clear that we have a finite amount of 
iterations since $X$ cannot grow more than $\Sg$. Readers with background in
logic (see \cite{boros_strong_2017}) or graph theory 
(\cite{berczi_directed_2017}) might see this operation as the marking or 
forward chaining procedure.

\vspace{1.2em}

\paragraph{Example} Let's stick to our vegetable example, but reducing $\Sg$ to 
\{\textit{bloom, flower, colourful} \} (abbreviated \textit{b, f, c}) for the 
sake of simplicity. Furthermore, let $\I =$ \{\textit{((colourful, bloom) 
$\imp$ flower), (flower $\imp$ bloom)}\}, abbreviated then $cb \imp f$, $f 
\imp b$. For instance, because $f \imp b \in \I$, the smallest model of $\I$ 
containing $f$ is $bf$, and $bf$ is closed. More precisely, the set of closed
sets is the following:

	\[ \Sg^{\I} = \{ \emptyset, \ b, \ c, \ bf, \ bcf \} \]
	
\vspace{1.2em}

Having presented the main definitions we shall need, we are to investigate 
practical computation of closures and more elaborated structures like the 
\belemp{canonical basis} (or \belemp{Duquenne-Guigues basis}) in the next section.



% ==== Subsection: DG-basis, closure algos

\subsection{Canonical Basis, Closure Algorithm}

Before giving the definition of canonical basis, we should consider special 
kind of sets given $\I$ over $\Sg$. Also, we will need to expose particular
implications. First of all, let us introduce a property through a proposition
on the link between an implication and the closure of its premise (we redirect the reader to \cite{b._ganter_conceptual_2016} for another proof). When not introduced, we consider a system $\I$ of implications, over some attribute set $\Sg$.

\begin{proposition} Let $\I$ be a theory of $\Sg$. Let $A \imp B$ be an implication. $\I \models A \imp B$ if and only if $B \subseteq \I(A)$.	
\end{proposition}

\begin{proof} $(\I \models A \imp B) \imp B \subseteq \I(A)$. Every model
of $\I$ models $A \imp B$, hence for each closed set $X$ of $\I$, either 
$A \subseteq X $ and $B \subseteq X$, or $A \nsubseteq X$. Consider all closed 
$X$ for which $A \subseteq X$. By definition $\I(A) = \bigcap \{X \in 
\Sg^{\I},\  A \subseteq X \}$ and $B \subseteq \I(A)$.

\vspace{1.2em}

$(B \subseteq \I(A)) \imp (\I \models A \imp B)$. By contraposition suppose 
$\I \not\models A \imp B$. Then there must exist at least one model $X$ of $\I$
such that $A \subseteq X$ and $B \nsubseteq X$. Because $A \subseteq X$, 
$\I(A) \subseteq X$ and $B \nsubseteq \I(A)$.
	
\end{proof}

\begin{definition}[\midemp{Redundancy}] An implication $A \imp B$ of $\I$ is 
\belemp{redundant} if $\I - \{A \imp B \} \models A \imp B$. If $\I$ 
contains no redundant implications, it is \belemp{non-redundant}.
	
\end{definition}

\noindent Our definition of redundancy models the notion of \textit{"useless"} 
we were talking about in our toy example: if an implication is true in some 
$\I$ even if we remove it, it brings no knowledge. In practice, redundancy can 
be checked as follows: put $\I^{-}$ as
$\I$ without $A \imp B$ and compute $\I^{-}(A)$. If $\I^{-}(A) = \I(A)$ or 
equivalently, if $B \subseteq \I^{-}(A)$, then $A \imp B$ is redundant. 
Moreover, it is worth commenting that in FCA or DB fields (see 
\cite{ganter_formal_1999, maier_theory_1983}), implications (or 
FD's) are deduced from data presented as contexts or relation schemes. Hence, 
we usually introduce notions of soundness and completeness ensuring that 
implications we are working on are meaningful with respect to the knowledge we 
are dealing with. More precisely, \belemp{soundness} ensures that $\I$ does not 
contain any implication not holding in the dataset. \belemp{Completeness}
says that all true implications in the data context are true in $\I$. Because 
we work directly on implications, $\I$ is by definition sound and complete with 
respect to the models it defines. Next, we set up minimality.

\begin{definition}[Minimality] $\I$ is \belemp{minimal} if removing one of its 
	implication alters $\Sg^{\I}$.
	
\end{definition}

\paragraph{Example} We consider our canonical plant example. Take 

\begin{center}
	$\I = $  \{\textit{((colourful, bloom) $\imp$ seasonal), ((colourful, 
		wither) $\imp$ seasonal), (bloom $\imp$ wither)} \}
\end{center}

\noindent as we explained in first section, the first implication can be 
removed. In particular, it is redundant. Hence $\I$ is not minimal. If 
we get rid of \textit{(colourful, bloom) $\imp$ seasonal)}, $\I$ 
will be minimal.

\vspace{1.2em}

Interestingly, depending on the implications we get, non-redundancy is not a 
sufficient criterion for minimality as we shall see in Maier algorithm. As an 
example for now, consider $\Sg = $\{\textit{a, b, c, d, e, f}\} and $\I = $ 
\{\textit{ab $\imp$ cde, c $\imp$ a, d $\imp$ b, cd $\imp$ f}\}. $\I$ is not 
redundant, but is not minimal either. In fact, $\I_m = $ \{\textit{ab $\imp$ 
cdef, c $\imp$ a, d $\imp$ b}\} is equivalent to $\I$ but with one implication 
less. 

\vspace{1.2em}

For now, we defined what are implication theories, redundancy and minimality. 
One could expect our next step to be the exposition of some minimal basis. 
Unfortunately, we need to make a detour to visit some set and order definitions
before getting back to our main purpose. Those notions not only deserve to
explain minimal basis but also to settle some landmarks for further discussions 
in the next chapter.

\vspace{1.2em}

Recall that in our example of closure operator we briefly approached binary 
relations. To be more formal, let $E, F$ be two sets. A \belemp{binary 
relation} $\rel{R}$ is a set of pairs $(e, f)$ (sometimes denoted 
$e \rel{R} f$) with $e \in E$, $f \in F$, or equivalently $\rel{R} \subseteq E 
\times F$. We will assume $\rel{R} \subseteq E^2$. Actually, $\rel{R}$ can 
present some properties:
\begin{itemize}
	\item[(i)] \belemp{reflexivity}: $\forall x \in E, x\rel{R}x$,
	\item[(ii)] \belemp{irreflexivity}: $\forall x \in E, \lnot(x\rel{R}x)$,
	\item[(iii)] \belemp{symmetry}: $\forall x, y \in E, x \rel{R} y \imp y 
	\rel{R} x$
	\item[(iv)] \belemp{antisymmetry}: $\forall x, y \in E, x \rel{R} y \land 
	y \rel{R} x \imp x = y$,
	\item[(v)] \belemp{asymmetry}: $\forall x, y \in E, x \rel{R} y \imp
	\lnot(y\rel{R}x)$,
	\item[(vi)] \belemp{transitivity}: $\forall x, y, z \in E, x \rel{R} y 
	\land y \rel{R} z \imp x \rel{R} z$
\end{itemize}
As a reminder for some statements $a$ and $b$, $\lnot a$ denotes logical negation ($\lnot a$ is true when $a$ is not), $a \land b$ is the conjunction (the statement $a \land b$ is true if both $a$ and $b$ are) and $a \lor b$ is the disjunction ($a \lor b$ is true if at least one of $a$, $b$ is). $a \imp b$ is a logical implication, if $a$ is true, so is $b$. All possible properties are not given here, see \cite{cori_mathematical_2000} 
for more. With those properties anyway, we can define several types of 
relations:

\begin{definition}[\midemp{Equivalence, order}] Let $E$ be a set and $\rel{R}$ a binary relation on $E$:
\begin{itemize}
	\item[(i)] $\rel{R}$ is an \belemp{equivalence} relation (denoted by $=$) 
	if it is \midemp{reflexive, transitive} and \midemp{symmetric},
	
	\item[(ii)] $\rel{R}$ is an \belemp{(partial) order} ($\leq$) if 
	\midemp{reflexive, transitive} and \midemp{antisymmetric},
	
	\item[(iii)] $\rel{R}$ is a \belemp{strict order} ($<$) if 
	\midemp{irreflexive, transitive} and \midemp{asymmetric}.
\end{itemize}
	
\end{definition}

\paragraph{Example} Time has come for some illustrations. First, let us imagine
we are looking at some tree in a meadow. Because the season is spring, this tree
has branches and leaves. We are interested in the set of all leaves, and we 
would like to relate them by the branch they are on. Hence define $\rel{R}$ as
\textit{"is on the same branch as"}, being a binary relation. It turns out
that $\rel{R}$ is an equivalence relation:
\begin{itemize}
	\item[-] \midemp{reflexivity}: every leaf is on the same branch as itself;
	\item[-] \midemp{transitivity}: if a leaf $l_1$ is on the same branch as
	a leaf $l_2$, and $l_2$ is on the same branch as $l_3$, then it is clear
	that $l_1$ is on the same branch as $l_3$;
	\item[-] \midemp{symmetry}: $l_1$ being on the same branch as $l_2$ clearly
	implies that $l_2$ is on the same branch as $l_1$.
\end{itemize}
\noindent For partial and strict ordering, we will go back to more mathematical
examples, in order to slowly go back to our main purpose. Consider the set 
$\Ens{N}$ ($= \Ens{N}_0 $) of positive integers, including 0. The natural 
relation $\leq$ is an order, and the pair $(\Ens{N}, \leq)$ is an ordered set.
In particular it is a \belemp{totally ordered set} or \belemp{chain} because 
every pair of integers can be compared. $<$ is a strict total ordering on 
$\Ens{N}$. Another example, let $\Sg = \{a, b, c \}$ be a set of attributes and
consider $\subseteq$ as a binary relation on $2^{\Sg}$. Again, $(2^{\Sg}, 
\subseteq)$ is a \belemp{partially ordered set} (or \belemp{poset} under 
abbreviation):
\begin{itemize}
	\item[-] every subset $X$ of $\Sg$ is included in itself, for instance 
	$\{a, b\}$ is a subset or equal to $\{a, b\}$, whence \midemp{reflexivity},
	\item[-] if $X \subseteq Y$ and $Y \subseteq X$ then necessarily, $X = Y$
	(\midemp{antisymmetry}),
	\item[-] if $X \subseteq Y \subseteq Z$, then clearly $X \subseteq Z$
	(\midemp{transitivity})
\end{itemize}
\noindent There is a convenient way to represent posets. At least when they are
not to heavy. It is sometimes called \belemp{Hasse diagram} (see 
\cite{davey_introduction_2002}) and relies on the \belemp{cover} relation of
a partially ordered set. Take any poset $(P, \leq)$ and define the cover 
relation as $x \prec y$ if $x < y$ and $x \leq z < y \imp x = z$. In other 
word, $x \prec y$ says \textit{"there is no element between x and y"}. The
example of $\Ens{N}$ is appealing. For instance, $4 \prec 5$ because there is
no integer between 4 and 5, but $4 \nprec 7$ since we can find 5 and 6 as 
intermediary elements. Now the Hasse diagram of $(P, \leq)$ is a graph drawn as 
follows:
\begin{enumerate}
	\item there is a point for each $x \in P$,
	\item if $x \leq y$, then $y$ is placed above $x$,
	\item we draw an arc between $x$ and $y$ if and only if $x \prec y$ in $P$.
\end{enumerate}
As examples, one can observe the diagrams of $(\Ens{N}, \leq)$ and $(2^{\Sg}, 
\subseteq)$ described previously in figure \ref{fig:I-posets}. On the right-hand
side we wrote a subset of $\Sg$ by a concatenation of its element for 
readability purpose.


\begin{figure}[ht]
	\input{Pictures/I/Posets.tex}
\end{figure}

\vspace{1.2em}

Now equipped with orders and equivalence relation, we can go a bit further in
the study of implications and sets. For instance, as exposed in the previous 
example, we can consider our attribute set $\Sg$ (more precisely its power set)
equipped with $\subseteq$ as an ordering. Furthermore, recall that $\I$ is
a set of implications and hence provide a closure operators. Because every 
subset of $\Sg$ has only one closure in $\I$, we can define an equivalence 
relation $\equl$ on $2^{\Sg}$ as follows:

	\[ \forall X, Y \subseteq \Sg, X \equl Y \textit{ if and only if }
	\I(X) = \I(Y) \]

\noindent Let us go one step beyond. With $\equl$, we can set up 
\belemp{equivalence classes} on $(2^{\Sg}, \equl)$. An equivalence class can be 
defined with respect to an element $X$ as follows:

	\[ \left[ X \right]_{\I} = \{ Y \subseteq \Sg \ | \ X \equl Y \}  \]
	
\noindent Those equivalence classes are a partition of $2^{\Sg}$ with respect
to the closed sets of $\I$: every model of $\Sg^{\I}$ defines an 
equivalence class. 

\paragraph{Example} Because all this discussion on equivalence class may have
been a bit troubling, let us rest for a while before eventually reaching 
minimal basis definitions. Remind our plant example:
\begin{itemize}
	\item[-] $\Sg = $ \{\textit{bloom, flower, colourful}\} (abbreviated 
	\{\textit{b, f, c}\}),
	\item[-] $\I = $ \{\textit{((colourful, bloom) $\imp$ flower), (flower 
	$\imp$ bloom)}\} (abbreviated $cb \imp f$, $f \imp b$)
	\item[-] the models (closed sets of $\I$) are: $\Sg^{\I} = \{ \emptyset, \ 
	b, \ c, \ bf, \ bcf \}$
\end{itemize}
\noindent In details, because of the implication $f \imp b$, we can observe 
that $f$ and $bf$ belong to the equivalence class defined by $bf$. The same
goes for $bc$, $cf$ and $bcf$, describing the class given associated to $bcf$.
In order to make it clear, we give a graphical representation of those classes
using orders in figure \ref{fig:I-def-CS}.

\begin{figure}[ht]
	\input{Pictures/I/CS.tex}
\end{figure}

\vspace{1.2em}

On the left side of the picture, we drew $(2^{\Sg}, \subseteq)$. On the
right-hand side: $(\Sg^{\I}, \subseteq)$. Clusters on the left diagram are
the equivalence classes, associated (dotted arrows) to their
closed representative. If a cluster contains only one element, this element is 
closed. This drawing shows the relation between a closure operator and its 
associated system, in particular in implication basis context, where the 
closure describes models. Finally, one can graphically note that the set of 
models is indeed closed under intersection. While this representation is 
graphically appealing, it is clearly not tractable for larger attribute set: we 
have to draw a diagram with an exponential number of elements (one for all $X 
\in 2^{\Sg}$). Thus, all Hasse diagrams we are going to draw only aim at 
providing some intuition of the various notions and not as an efficient 
representation.

\vspace{1.2em}

With this detour in order theory made, even though closely related to our topic
as we have seen, we can now go back to our main goal: the canonical basis. It
relies on some particular sets in the closure systems.

\begin{definition}[\midemp{Pseudo-closed set}] Given $\I$ over $\Sg$, we say
	that $P \subseteq \Sg$ is \belemp{pseudo-closed} if:
	\begin{itemize}
		\item[(i)] $P \neq \I(P)$,
		\item[(ii)] $Q \subset P$ and $Q$ pseudo-closed implies $\I(Q) 
		\subseteq P$.
	\end{itemize}
\end{definition}

\noindent The idea of pseudo-closed sets goes back to Guigues and Duquenne in
\cite{guigues_j.l_familles_1986}, but the name comes from Ganter in 
\cite{ganter_two_2010}. We can also find explanations in following 
research \cite{ganter_two_2010, day_lattice_1992} and
in \cite{b._ganter_conceptual_2016}. It turns out that we can explain 
pseudo-closure by using so called \textit{quasi-closed sets} (see 
\cite{wild_implicational_1989, ganter_two_2010, guigues_j.l_familles_1986}). 

\begin{definition}[\midemp{Quasi-closed set}] a set $Q \subseteq \Sg$ is 
	\belemp{quasi-closed} with respect to $\I$ if:
	\begin{itemize}
		\item[(i)] $Q \neq \I(Q)$,
		\item[(ii)] $\forall A \subseteq Q$, $\I(A) \subseteq Q$ or $\I(A) = 
		\I(Q)$.
		
	\end{itemize}
	
\end{definition}

\noindent The recursive definition of pseudo-closed sets may seem complicated,
and it is somehow since the problem of determining whether a set is 
pseudo-closed or not has been proven to be NP-Hard (see 
\cite{b._ganter_conceptual_2016}). Fortunately, quasi-closed sets and 
equivalence classes give another definition to pseudo-closeness: a set is 
pseudo-closed if it is quasi-closed and minimal (inclusion-wise) among quasi-closed sets in its equivalence class. Let us illustrate those notions with some diagrams.


\paragraph{Example} Let us consider the following case:
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c \}$,
	\item[-] $\I = $ \{\textit{a $\imp$ b, b $\imp$ a, c $\imp$ ab}\}.
\end{itemize}
We could replace $a \imp b$ and $b \imp a$ by $ a \longleftrightarrow b$ since it means that $a$ and $b$ are equivalent attributes. As in figure \ref{fig:I-def-CS}, we will represent the power set of $\Sg$ and 
equivalence classes of $\I$. Two subsets of $\Sg$ are in the same class if they
have the same closure in $\I$. First, one can observe the effective class 
representation in figure \ref{fig:I-def-EQC}. Models of $\I$ are indeed 
$\emptyset$, $ab$ and $abc$. For instance $\I(ac) = abc$.

\vspace{1.2em}

\begin{figure}[ht]
	\input{Pictures/I/EquivalenceClass.tex}
\end{figure}

Next, we can observe figure \ref{fig:I-def-QCS} in which we somehow represented 
the definition of quasi-closure. We still represent equivalence classes. On
the left-hand side figure, we consider the subset $c$. For $c$ to be 
quasi-closed, we must look at all of its subsets, and see whether the closure
of each subset is either smaller than $c$ or in the same equivalence class
as $c$. The dashed line shows which elements of the diagram we have to 
consider. In fact it represents what we call in lattice and order theories the 
\belemp{ideal} or \belemp{down-set} generated by $c$:

	\[ \downarrow c = \{ X \subseteq \Sg | X \subseteq c \} \] 

It appears that the only distinct subset of $c$ is $\emptyset$, which is 
closed. $c$ itself is also not closed. Hence $c$ is indeed quasi-closed. 

\vspace{1.2em}

\begin{figure}[ht]
	\input{Pictures/I/QuasiClosed.tex}
\end{figure}

On the right-hand side we consider the subset $bc$. As shown in the picture 
(under the dashed line), there are 3 elements to consider: $\emptyset$, $c$, 
$b$. For the same reason as for $c$, $\emptyset$ is not a problem. the closure
of $c$ is not included in $bc$, but equals the closure of $bc$. Hence $c$ is
not an issue either. However, $b$ is included in $bc$, but its closure is $ab$,
neither subset of $bc$ nor equal to $abc$. Therefore, $bc$ is not quasi-closed.
Actually, one could informally say that a set $Q$ is quasi-closed if the 
following is true:

\[ (\forall P \in 2^{\Sg})\left[( P \in \; \downarrow Q) \imp (\I(P) \in \; 
\downarrow Q \cup \{\I(Q)\})\right] \]

\noindent where $\downarrow Q$ is the ideal generated by $Q$ (see dashed line
in figure \ref{fig:I-def-QCS}). 

\paragraph{Example} Now let us take 
\begin{itemize}
	\item[-] $\Sg = \{a, \ b, \ c \}$,
	\item[-] $\I = $ \{\textit{c $\imp$ ab, b $\imp$ ab}\}.
\end{itemize}
Again, we will use equivalence classes and Hasse diagram to represent the 
closure system of $\I$, see figure \ref{fig:I-def-PCS}. In this representation
we coloured all quasi-closed sets at least in grey. Red (or lighter) nodes are
precisely pseudo-closed sets: they are the minimal quasi-closed sets among the
equivalence class defined by $abc$. 

\newpage

\begin{figure}[ht]
	\input{Pictures/I/PseudoClosed.tex}
\end{figure}

Note that in particular, minimal premises of $\I$ inclusion wise are 
pseudo-closed. Furthermore, we should be aware that an equivalence class may
not contain pseudo-closed set, or more generally, quasi-closed sets. As such,
we cannot consider that minimal elements of equivalence classes are 
quasi-closed. Take for example $\I =$ \{\textit{$\emptyset \imp $ a, b $\imp$ a 
}\}. $b$ and $ab$ define a class, but $b$ is not even quasi-closed. With these
notions, we can move on and define the canonical basis.

\begin{definition}[\midemp{Duquenne-Guigues basis}] The basis $\I$ defined by
	
	\[ \I = \{ P \imp \I(P) \; | \; P \text{ is pseudo-closed in $\I$ } \} \]
	
	\noindent is called the \belemp{Duquenne-Guigues} or \belemp{canonical} 
	basis. 
	It is \belemp{minimal}.
	
\end{definition}

\noindent This definition does not say that the canonical basis is the only one
being minimal. Actually, it says that every minimal basis should have the same
number of implications than this one. We can find a deeper argument in 
\cite{b._ganter_conceptual_2016} (chapter III, proposition 17) on links between any minimal basis and the canonical one.

\vspace{1.2em}

So far we discussed several notions: implications, pseudo-closed set, 
quasi-closed set, canonical basis and so forth. Most of them relies heavily on
computing the closure of sets with respect to $\I$. Hence, to have practical 
efficiency, we must be able to compute closures as fast as possible. 
Fortunately, several algorithms can be found. Among them, there is a naïve
procedure based on the operation $\circ$ we described earlier. Furthermore the 
algorithm by Beeri and Bernstein in \cite{beeri_computational_1979} called 
\textsc{LinClosure} addresses this question. \textsc{LinClosure} as previously 
mentioned has been widely used, notably in \cite{maier_theory_1983, 
david_minimum_1980, b._ganter_conceptual_2016, shock_computing_1986, 
day_lattice_1992}. Before describing those procedures, let us introduce our 
complexity notations:
\begin{itemize}
	\item[-] $|\Sg|$ will denote the size of the attribute set $\Sg$,
	\item[-] $|\B|$ will be the number of implications in $\I$ ($\B$ stands
	for body),
	\item[-] $|\I|$ is the number of symbols used to represent $\I$.
\end{itemize}
We consider $|\I|$ to be in reduced form for complexity results. By \textit{"reduced"} we mean that we do not have distinct implications with same bodies. Indeed, if say, $a \imp b$ and $a \imp c$ holds in some $\Sg^{\I}$ then we can replace those two implications by $a \imp bc$. Moreover, we shall not explain in details $O$ notation for complexity since we do not need in-depth knowledge within this field. For us, it is enough to say that $O$ is the 
asymptotically worst case complexity (in time or space). For instance, in the 
worst case, $|\I| = |\B|\times|\Sg|$, thus $|\I| = O(|\B|\times|\Sg|)$. 
\textsc{Closure} and \textsc{LinClosure} are algorithms \ref{alg:closure}, 
\ref{alg:linclosure} (resp.). 

\vspace{1.2em}

\begin{algorithm}[ht]
\KwIn{A base $\I$, $X \subseteq \Sg$}
\KwOut{The closure $\I(X)$ of $X$ under $\I$}

\BlankLine
\BlankLine

$closed := \bot$ \;
$\I(X) := X$ \;
\While{$\lnot closed$}{
	$closed := \top$ \;
	\ForEach{$A \imp B \in \I$}{
		\If{$A \subseteq \I(X)$}{
			$\I(X) := \I(X) \cup B$ \;
			$\I := \I - \{A \imp B \}$ \;
			$closed := \bot$ \;
		}
		
	}
}

\BlankLine

return $\I(X)$\;

\caption{\textsc{Closure}}
\label{alg:closure}
\end{algorithm}


As we already mentioned, the algorithm \textsc{Closure} relies on the $\circ$ 
operation. The principle is to re-roll over the set of implications $\I$ to see 
whether there exists an implication $A \imp B$ in $\I$ such that $\I(X) 
\not\models A \imp B$ up to stability. Asymptotically, we will need $O(|\B|^2 
\times |\Sg|)$ if we remove only one implication per loop. the $|\Sg|$ cost
comes from the set union. $\bot$ stands for \textit{false} and $\top$ for \textit{true}.

\vspace{1.2em}

\begin{algorithm}
\KwIn{A base $\I$, $X \subseteq \Sg$}
\KwOut{The closure $\I(X)$ of $X$ under $\I$}

\BlankLine
\BlankLine

\ForEach{$A \imp B \in \I$}{
	$count[A \imp B] := |A|$ \;
	
	\If{$|A| = 0$}{
		$X := X \cup B$ \;
	}
	
	\ForEach{$a \in A$}{
		$list[a] = list[a] \cup  \{ A \imp B \}$ \;
	}
}

\BlankLine

$update := X$ \;

\BlankLine

\While{$update \neq \emptyset$}{
	choose $m \in update$ \;
	$update := update - \{m\}$ \;
	
	\ForEach{$A \imp B \in list[m]$}{
		$count[A \imp B] := count[A \imp B] - 1$ \;
		\If{$count[A \imp B] = 0$}{
			$add := B - X$ \;
			$X := X \cup add$ \;
			$update := update \cup add$ \;
		}
		
	}
}

return $X$ \;

\caption{\textsc{LinClosure}}
\label{alg:linclosure}
\end{algorithm}

\textsc{LinClosure} has $O(|\I|)$ time complexity. The main idea is to use
counters. Starting from $X$, if we reach for a given $A \imp B$ as many elements
as $|A|$, then $A \subseteq \I(X)$ and we must also add $B$. Because the closure
in itself is not the main point of our topic, we will not study 
\textsc{LinClosure} in depth. Furthermore, there exists other linear time 
algorithm for computing closure. For more complete theoretical and practical 
comparisons of closure algorithms, we redirect the reader to 
\cite{bazhanov_optimizations_2014}. In this paper, \textsc{LinClosure} is 
shown maybe not to be the most efficient algorithm in practice when used in 
other algorithms, especially when compared with \textsc{Closure}. Anyway, 
because of its theoretical complexity and use in all algorithms we will review, 
we will still consider \textsc{LinClosure}, notably because we can separate the 
initialization step from the computation one in some cases on optimization 
purpose.

\vspace{1.2em}


In this last section, we got a step further in building ground for understanding
the implication theory structure. We gave definitions of minimality and visual
examples of particular sets called pseudo-closed. With the support of those
sets, we defined the canonical basis known to be minimal. Finally, algorithms
for efficiently computing closures have been presented. 


\paragraph{Conclusion} In this chapter we first gave a soft introduction to our 
task with a somehow \textit{"physical"} example. Then we described briefly advances starting from the first properties found independently in Concept Analysis and Relational Databases fields. We have seen that the question of Horn minimization has been studied in various fields such as graphs, closure spaces, logic (where the name Horn comes from), functional dependencies, lattices. Then, we placed our study within this context. The aim of this study has been exposed as providing a review of some algorithms we talked about, and comparing them under implementation. The last part of the chapter was dedicated to a more formal and theoretical ground necessary for a good understanding of subsequent parts. In the next chapter, we will theoretically discuss in details several algorithms.

